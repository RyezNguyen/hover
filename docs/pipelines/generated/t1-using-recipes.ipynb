{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15728522",
   "metadata": {},
   "source": [
    "> The most common usage of `hover` is through built-in `recipe`s like in the quickstart.\n",
    ">\n",
    "> :ferris_wheel: Let's explore another `recipe` -- an active learning example.\n",
    "\n",
    "-   <details open><summary>Dependencies for {== local environments ==}</summary>\n",
    "    When you run the code locally, you may need to install additional packages.\n",
    "\n",
    "    To run the text embedding code on this page, you need:\n",
    "```shell\n",
    "    pip install spacy\n",
    "    python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "    To render `bokeh` plots in Jupyter, you need:\n",
    "```shell\n",
    "    pip install jupyter_bokeh\n",
    "```\n",
    "\n",
    "    If you are using JupyterLab older than 3.0, use this instead ([reference](https://pypi.org/project/jupyter-bokeh/)):\n",
    "```shell\n",
    "    jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "    jupyter labextension install @bokeh/jupyter_bokeh\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "## **Fundamentals**\n",
    "\n",
    "Hover `recipe`s are functions that take a `SupervisableDataset` and return an annotation interface.\n",
    "\n",
    "The `SupervisableDataset` is assumed to have some data and embeddings.\n",
    "\n",
    "## **Recap: Data & Embeddings**\n",
    "\n",
    "Let's preprare a dataset with embeddings. This is almost the same as in the [quickstart](../t0-quickstart/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e14e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-13T01:20:18.692176Z",
     "iopub.status.busy": "2024-02-13T01:20:18.691981Z",
     "iopub.status.idle": "2024-02-13T01:20:22.468090Z",
     "shell.execute_reply": "2024-02-13T01:20:22.467444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: Initializing</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: Initializing\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: Deduplicating</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: Deduplicating\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: --subset raw rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">374</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: --subset raw rows: \u001b[0m\u001b[1;36m400\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m374\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: --subset train rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">382</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: --subset train rows: \u001b[0m\u001b[1;36m400\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m382\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: --subset dev rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: --subset dev rows: \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m97\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: --subset test rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: --subset test rows: \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m98\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableTextDataset: Set up label encoder/decoder with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #008000; text-decoration-color: #008000\"> classes.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableTextDataset: Set up label encoder/decoder with \u001b[0m\u001b[1;36m20\u001b[0m\u001b[32m classes.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableTextDataset: Population updater: latest population with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #008000; text-decoration-color: #008000\"> classes.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableTextDataset: Population updater: latest population with \u001b[0m\u001b[1;36m20\u001b[0m\u001b[32m classes.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: finished setting up bokeh elements.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: finished setting up bokeh elements.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableTextDataset: finished initialization.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableTextDataset: finished initialization.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hover.core.dataset import SupervisableTextDataset\n",
    "import pandas as pd\n",
    "\n",
    "raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\"\n",
    "train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\"\n",
    "\n",
    "# for fast, low-memory demonstration purpose, sample the data\n",
    "df_raw = pd.read_csv(raw_csv_path).sample(400)\n",
    "df_raw[\"SUBSET\"] = \"raw\"\n",
    "df_train = pd.read_csv(train_csv_path).sample(400)\n",
    "df_train[\"SUBSET\"] = \"train\"\n",
    "df_dev = pd.read_csv(train_csv_path).sample(100)\n",
    "df_dev[\"SUBSET\"] = \"dev\"\n",
    "df_test = pd.read_csv(train_csv_path).sample(100)\n",
    "df_test[\"SUBSET\"] = \"test\"\n",
    "\n",
    "# build overall dataframe and ensure feature type\n",
    "df = pd.concat([df_raw, df_train, df_dev, df_test])\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "# this class stores the dataset throught the labeling process\n",
    "dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267f493",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0606e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-13T01:20:22.471357Z",
     "iopub.status.busy": "2024-02-13T01:20:22.470754Z",
     "iopub.status.idle": "2024-02-13T01:20:25.184032Z",
     "shell.execute_reply": "2024-02-13T01:20:25.183336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Moscow Scientific Inductrial Association \"Spectrum\" offer        VIDEOSCAN vision system for PC/AT,wich include software and set of        controllers.                       SOFTWARE              For support VIDEOSCAN family program kit was developed. Kit        includes more then 200 different functions for image processing.             Kit works in the interactive regime, and has include Help for        non professional users.             There are next possibility:        - input frame by any board of VIDEOSCAN family;        - read - white image to - from disk;        - print image on the printer;        - makes arithmetic with 2 frames;        - filter image;        - work with gistogramme;        - edit image.        - include users exe modules.                       CONTROLLER VS9         The function of VS-9 controller is to load TV-images into  PC/AT.        VS-9 controller allows one to load a fragment of the TV-frame from        a field of 724x600 pixels.        The clock rate is 14,7 MHz when loading an image with 512 pixel in        the line and 7,4 MHz when loading a 256 pixels image. This        provides the equal pixel size of input image in both horizontal        and vertical directions.        The number of gray levels in any input modes is 256.        Video signal capture time - 2.5s.                       CONTROLLER VS52         The purpose of the controller is to enter the TV images into a IBM        PC AT or any other machine of that type. The controller was        created on the base of modern elements, including user        programmable gate arrays.        The controller allows to digitize a input signal with different        resolutions. Its flexible architecture makes possible to change        technical parameters. Instead of TV signal one can process any        other analog signal (including signals from slow-speed scanning        devices).        The controller has the following technical characteristics:        - memory volume - from 256 K to 2 Mb ;        - resolution when working with standard video signal - from 64x64        to 1024x512 pixels ;        - resolution when working in slow input regime - up to 2048x1024        pixels;        - video signal capture time - 40 ms.        - maximum size of a screen when memory volume is 2Mb - 2048x1024        pixels ;        - number of gray level - 256 ;        - clock rate for input - up to 30 MHz ;        - 4 input video multiplexer ;        - input/output lookup table (LUT);        - possibility to realize \"scroll\" and \"zoom\";                     - 8 lines for external synchronization (an input using external        controlling signal) ;        - electronic adjustment of black and white reference for analog -        digital converter;        - possibility output image to the color RGB monitor.        One can change all listed above functions and parameters of the        controller by reprogramming it.                 IMAGE PROCESSOR VS100 \n",
      "Vector shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "# use your preferred embedding for the task\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# raw data (str in this case) -> np.array\n",
    "@lru_cache(maxsize=int(1e+4))\n",
    "def vectorizer(text):\n",
    "    clean_text = re.sub(r\"[\\s]+\", r\" \", str(text))\n",
    "    return nlp(clean_text, disable=nlp.pipe_names).vector\n",
    "\n",
    "text = dataset.dfs[\"raw\"]().loc[0, \"text\"]\n",
    "vec = vectorizer(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Vector shape: {vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f8590",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed07c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-13T01:20:25.187001Z",
     "iopub.status.busy": "2024-02-13T01:20:25.186416Z",
     "iopub.status.idle": "2024-02-13T01:20:51.936836Z",
     "shell.execute_reply": "2024-02-13T01:20:51.936018Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:   0%|          | 0/951 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:   5%|▌         | 52/951 [00:00<00:01, 519.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  12%|█▏        | 113/951 [00:00<00:01, 572.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  18%|█▊        | 171/951 [00:00<00:02, 378.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  23%|██▎       | 215/951 [00:00<00:02, 350.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  29%|██▉       | 274/951 [00:00<00:01, 410.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  36%|███▌      | 339/951 [00:00<00:01, 475.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  42%|████▏     | 404/951 [00:00<00:01, 523.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  49%|████▉     | 468/951 [00:01<00:01, 476.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  56%|█████▋    | 536/951 [00:01<00:00, 426.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  63%|██████▎   | 595/951 [00:01<00:00, 463.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  68%|██████▊   | 651/951 [00:01<00:00, 487.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  74%|███████▍  | 703/951 [00:01<00:00, 369.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  78%|███████▊  | 746/951 [00:01<00:00, 366.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  86%|████████▌ | 818/951 [00:01<00:00, 447.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing:  91%|█████████▏| 869/951 [00:02<00:00, 380.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing: 100%|█████████▉| 950/951 [00:02<00:00, 475.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Vectorizing: 100%|██████████| 951/951 [00:02<00:00, 440.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: Fit-transforming UMAP on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">853</span><span style=\"color: #000080; text-decoration-color: #000080\"> samples</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: Fit-transforming UMAP on \u001b[0m\u001b[1;36m853\u001b[0m\u001b[34m samples\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableTextDataset: Transforming UMAP on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span><span style=\"color: #000080; text-decoration-color: #000080\"> samples</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableTextDataset: Transforming UMAP on \u001b[0m\u001b[1;36m98\u001b[0m\u001b[34m samples\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableTextDataset: Computed </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">-d embedding in columns </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'embed_2d_0'</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'embed_2d_1'</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableTextDataset: Computed \u001b[0m\u001b[1;36m2\u001b[0m\u001b[32m-d embedding in columns \u001b[0m\u001b[1;32m[\u001b[0m\u001b[32m'embed_2d_0'\u001b[0m\u001b[32m, \u001b[0m\u001b[32m'embed_2d_1'\u001b[0m\u001b[1;32m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# any kwargs will be passed onto the corresponding reduction\n",
    "# for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "# for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html\n",
    "reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25fbb1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **Recipe-Specific Ingredient**\n",
    "\n",
    "Each recipe has different functionalities and potentially different signature.\n",
    "\n",
    "To utilize active learning, we need to specify how to get a model in the loop.\n",
    "\n",
    "`hover` considers the `vectorizer` as a \"frozen\" embedding and follows up with a neural network, which infers its own dimensionality from the vectorizer and the output classes.\n",
    "\n",
    "-   This architecture named [`VectorNet`](../../reference/core-neural/#hover.core.neural.VectorNet) is the (default) basis of active learning in `hover`.\n",
    "\n",
    "-   <details open><summary>Custom models</summary>\n",
    "    It is possible to use a model other than `VectorNet` or its subclass.\n",
    "\n",
    "    You will need to implement the following methods with the same signatures as `VectorNet`:\n",
    "\n",
    "    -   [`train`](../../reference/core-neural/#hover.core.neural.VectorNet.train)\n",
    "    -   [`save`](../../reference/core-neural/#hover.core.neural.VectorNet.save)\n",
    "    -   [`predict_proba`](../../reference/core-neural/#hover.core.neural.VectorNet.predict_proba)\n",
    "    -   [`prepare_loader`](../../reference/core-neural/#hover.core.neural.VectorNet.prepare_loader)\n",
    "    -   [`manifold_trajectory`](../../reference/core-neural/#hover.core.neural.VectorNet.manifold_trajectory)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9d35a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-13T01:20:51.940300Z",
     "iopub.status.busy": "2024-02-13T01:20:51.939760Z",
     "iopub.status.idle": "2024-02-13T01:20:52.675698Z",
     "shell.execute_reply": "2024-02-13T01:20:52.675004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 VectorNet: reset neural net: in </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"color: #008000; text-decoration-color: #008000\"> out </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 VectorNet: reset neural net: in \u001b[0m\u001b[1;36m300\u001b[0m\u001b[32m out \u001b[0m\u001b[1;36m20\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 VectorNet: finished setting up bokeh elements.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 VectorNet: finished setting up bokeh elements.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00534117 0.000671   0.00054741 0.01415109 0.01943414 0.01070061\n",
      " 0.27901444 0.00098913 0.00224342 0.03010877 0.04971968 0.1839772\n",
      " 0.00672028 0.00208733 0.0136717  0.00075994 0.1868347  0.1347613\n",
      " 0.04709165 0.01117511]\n",
      "[[0.00534117 0.000671   0.00054741 0.01415109 0.01943414 0.01070061\n",
      "  0.27901444 0.00098913 0.00224342 0.03010877 0.04971968 0.1839772\n",
      "  0.00672028 0.00208733 0.0136717  0.00075994 0.1868347  0.1347613\n",
      "  0.04709165 0.01117511]]\n"
     ]
    }
   ],
   "source": [
    "from hover.core.neural import VectorNet\n",
    "from hover.utils.common_nn import LogisticRegression\n",
    "\n",
    "# Create a model with vectorizer-NN architecture.\n",
    "# model.pt will point to a PyTorch state dict (to be created)\n",
    "# the label classes in the dataset can change, and vecnet can adjust to that\n",
    "vecnet = VectorNet(vectorizer, LogisticRegression, \"model.pt\", dataset.classes)\n",
    "\n",
    "# predict_proba accepts individual strings or list\n",
    "# text -> vector -> class probabilities\n",
    "# if no classes right now, will see an empty list\n",
    "print(vecnet.predict_proba(text))\n",
    "print(vecnet.predict_proba([text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2ebac",
   "metadata": {},
   "source": [
    "Note how the callback dynamically takes `dataset.classes`, which means the model architecture will adapt when we add classes during annotation.\n",
    "\n",
    "## :sparkles: **Apply Labels**\n",
    "\n",
    "Now we invoke the `active_learning` recipe.\n",
    "\n",
    "-   <details open><summary>Tips: how recipes work programmatically</summary>\n",
    "    In general, a `recipe` is a function taking a `SupervisableDataset` and other arguments based on its functionality.\n",
    "\n",
    "    Here are a few common recipes:\n",
    "\n",
    "    === \"active_learning\"\n",
    "\n",
    "        ::: hover.recipes.experimental.active_learning\n",
    "            rendering:\n",
    "              show_root_heading: false\n",
    "              show_root_toc_entry: false\n",
    "\n",
    "    === \"simple_annotator\"\n",
    "\n",
    "        ::: hover.recipes.stable.simple_annotator\n",
    "            rendering:\n",
    "              show_root_heading: false\n",
    "              show_root_toc_entry: false\n",
    "\n",
    "    === \"linked_annotator\"\n",
    "\n",
    "        ::: hover.recipes.stable.linked_annotator\n",
    "            rendering:\n",
    "              show_root_heading: false\n",
    "              show_root_toc_entry: false\n",
    "\n",
    "    The recipe returns a `handle` function which `bokeh` can use to visualize an annotation interface in multiple settings.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261d1b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-13T01:20:52.678537Z",
     "iopub.status.busy": "2024-02-13T01:20:52.677969Z",
     "iopub.status.idle": "2024-02-13T01:20:52.750897Z",
     "shell.execute_reply": "2024-02-13T01:20:52.750183Z"
    }
   },
   "outputs": [],
   "source": [
    "from hover.recipes.experimental import active_learning\n",
    "\n",
    "interactive_plot = active_learning(dataset, vecnet)\n",
    "\n",
    "# ---------- NOTEBOOK MODE: for your actual Jupyter environment ---------\n",
    "# this code will render the entire plot in Jupyter\n",
    "# from bokeh.io import show, output_notebook\n",
    "# output_notebook()\n",
    "# show(interactive_plot, notebook_url='https://localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae7e9a",
   "metadata": {},
   "source": [
    "-   <details open><summary>Tips: annotation interface with multiple plots</summary>\n",
    "    <details open><summary>Video guide: leveraging linked selection</summary>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TIwBlCH9YHw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "    </details>\n",
    "\n",
    "    <details open><summary>Video guide: active learning</summary>\n",
    "        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hRIn3r7ovQ8\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "    </details>\n",
    "\n",
    "    <details open><summary>Text guide: active learning</summary>\n",
    "        Inspecting model predictions allows us to\n",
    "\n",
    "        -   get an idea of how the current set of annotations will likely teach the model.\n",
    "        -   locate the most valuable samples for further annotation.\n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
