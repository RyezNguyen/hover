{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b459a33",
   "metadata": {},
   "source": [
    "> `hover` supports bulk-labeling audios through their URLs (which can be local).\n",
    ">\n",
    "> :bulb: Let's do a quickstart for audios and note what's different from texts.\n",
    "\n",
    "-   <details open><summary>This page assumes that you have know the basics</summary>\n",
    "    i.e. simple usage of `dataset` and `annotator`. Please visit the [quickstart tutorial](/hover/pages/tutorial/t0-quickstart) if you haven't done so.\n",
    "\n",
    "</details>\n",
    "\n",
    "## **Dataset for audios**\n",
    "\n",
    "`hover` handles audios through their URL addresses. URLs are strings which can be easily stored, hashed, and looked up against. They are also convenient for rendering tooltips in the annotation interface.\n",
    "\n",
    "Similarly to `SupervisableTextDataset`, we can build one for audios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca48938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:39:55.333521Z",
     "iopub.status.busy": "2023-08-08T01:39:55.333109Z",
     "iopub.status.idle": "2023-08-08T01:39:56.346006Z",
     "shell.execute_reply": "2023-08-08T01:39:56.345314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: Initializing</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: Initializing\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: Deduplicating</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: Deduplicating\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: --subset raw rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: --subset raw rows: \u001b[0m\u001b[1;36m500\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m500\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: --subset train rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: --subset train rows: \u001b[0m\u001b[1;36m300\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m300\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: --subset dev rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: --subset dev rows: \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: --subset test rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> -&gt; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: --subset test rows: \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m -> \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableAudioDataset: Set up label encoder/decoder with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008000; text-decoration-color: #008000\"> classes.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableAudioDataset: Set up label encoder/decoder with \u001b[0m\u001b[1;36m10\u001b[0m\u001b[32m classes.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableAudioDataset: Population updater: latest population with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #008000; text-decoration-color: #008000\"> classes.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableAudioDataset: Population updater: latest population with \u001b[0m\u001b[1;36m10\u001b[0m\u001b[32m classes.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: finished setting up bokeh elements.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: finished setting up bokeh elements.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableAudioDataset: finished initialization.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableAudioDataset: finished initialization.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>label</th>\n",
       "      <th>SUBSET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/Aud...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/Aud...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/Aud...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/Aud...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://raw.githubusercontent.com/phurwicz/Aud...</td>\n",
       "      <td>ABSTAIN</td>\n",
       "      <td>raw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio    label SUBSET\n",
       "0  https://raw.githubusercontent.com/phurwicz/Aud...  ABSTAIN    raw\n",
       "1  https://raw.githubusercontent.com/phurwicz/Aud...  ABSTAIN    raw\n",
       "2  https://raw.githubusercontent.com/phurwicz/Aud...  ABSTAIN    raw\n",
       "3  https://raw.githubusercontent.com/phurwicz/Aud...  ABSTAIN    raw\n",
       "4  https://raw.githubusercontent.com/phurwicz/Aud...  ABSTAIN    raw"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hover.core.dataset import SupervisableAudioDataset\n",
    "import pandas as pd\n",
    "\n",
    "# this is a table of audio-MNIST (pronounced digit 0-9) urls, 100 audios per digit\n",
    "example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.7.0/audio_mnist.csv\"\n",
    "df = pd.read_csv(example_csv_path).sample(frac=1).reset_index(drop=True)\n",
    "df[\"SUBSET\"] = \"raw\"\n",
    "df.loc[500:800, 'SUBSET'] = 'train'\n",
    "df.loc[800:900, 'SUBSET'] = 'dev'\n",
    "df.loc[900:, 'SUBSET'] = 'test'\n",
    "\n",
    "dataset = SupervisableAudioDataset.from_pandas(df, feature_key=\"audio\", label_key=\"label\")\n",
    "\n",
    "# each subset can be accessed as its own DataFrame\n",
    "dataset.dfs[\"raw\"]().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb4b77",
   "metadata": {},
   "source": [
    "## **Vectorizer for audios**\n",
    "\n",
    "We can follow a `URL -> content -> audio array -> vector` path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cb84c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:39:56.349011Z",
     "iopub.status.busy": "2023-08-08T01:39:56.348742Z",
     "iopub.status.idle": "2023-08-08T01:39:56.392524Z",
     "shell.execute_reply": "2023-08-08T01:39:56.391936Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def url_to_content(url):\n",
    "    \"\"\"\n",
    "    Turn a URL to response content.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e107ed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:39:56.395805Z",
     "iopub.status.busy": "2023-08-08T01:39:56.395190Z",
     "iopub.status.idle": "2023-08-08T01:39:56.400963Z",
     "shell.execute_reply": "2023-08-08T01:39:56.400388Z"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from io import BytesIO\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def url_to_audio(url):\n",
    "    \"\"\"\n",
    "    Turn a URL to audio data.\n",
    "    \"\"\"\n",
    "    data, sampling_rate = librosa.load(BytesIO(url_to_content(url)))\n",
    "    return data, sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd1059",
   "metadata": {},
   "source": [
    "-   <details open><summary>Caching and reading from disk</summary>\n",
    "    This guide uses [`@wrappy.memoize`](https://erniethornhill.github.io/wrappy/) in place of `@functools.lru_cache` for caching.\n",
    "\n",
    "    -   The benefit is that `wrappy.memoize` can persist the cache to disk, speeding up code across sessions.\n",
    "\n",
    "    Cached values for this guide have been pre-computed, making it much master to run the guide.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdb61a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:39:56.403886Z",
     "iopub.status.busy": "2023-08-08T01:39:56.403384Z",
     "iopub.status.idle": "2023-08-08T01:39:56.419408Z",
     "shell.execute_reply": "2023-08-08T01:39:56.418740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Persisting __main__.vectorizer() output to\n",
      "custom_cache/audio_url_to_vector.pkl.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import wrappy\n",
    "\n",
    "@wrappy.memoize(cache_limit=10000, persist_path='custom_cache/audio_url_to_vector.pkl')\n",
    "def vectorizer(url):\n",
    "    \"\"\"\n",
    "    Averaged MFCC over time.\n",
    "    Resembles word-embedding-average-as-doc-embedding for texts.\n",
    "    \"\"\"\n",
    "    y, sr = url_to_audio(url)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=32)\n",
    "    return mfcc.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02798ced",
   "metadata": {},
   "source": [
    "## **Embedding and Plot**\n",
    "\n",
    "This is exactly the same as in the quickstart, just switching to audio data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1637f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:39:56.422270Z",
     "iopub.status.busy": "2023-08-08T01:39:56.421864Z",
     "iopub.status.idle": "2023-08-08T01:46:28.828781Z",
     "shell.execute_reply": "2023-08-08T01:46:28.828126Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████| 1000/1000 [06:12<00:00,  2.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: Fit-transforming UMAP on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">900</span><span style=\"color: #000080; text-decoration-color: #000080\"> samples</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: Fit-transforming UMAP on \u001b[0m\u001b[1;36m900\u001b[0m\u001b[34m samples\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runner/work/hover/hover/.tox/test_notebook_generation/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/runner/work/hover/hover/.tox/test_notebook_generation/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/runner/work/hover/hover/.tox/test_notebook_generation/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/runner/work/hover/hover/.tox/test_notebook_generation/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">🔵 SupervisableAudioDataset: Transforming UMAP on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000080; text-decoration-color: #000080\"> samples</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m🔵 SupervisableAudioDataset: Transforming UMAP on \u001b[0m\u001b[1;36m100\u001b[0m\u001b[34m samples\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">🟢 SupervisableAudioDataset: Computed </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">-d embedding in columns </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'embed_2d_0'</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'embed_2d_1'</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m🟢 SupervisableAudioDataset: Computed \u001b[0m\u001b[1;36m2\u001b[0m\u001b[32m-d embedding in columns \u001b[0m\u001b[1;32m[\u001b[0m\u001b[32m'embed_2d_0'\u001b[0m\u001b[32m, \u001b[0m\u001b[32m'embed_2d_1'\u001b[0m\u001b[1;32m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# any kwargs will be passed onto the corresponding reduction\n",
    "# for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "# for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html\n",
    "reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9fad98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:46:28.831990Z",
     "iopub.status.busy": "2023-08-08T01:46:28.831653Z",
     "iopub.status.idle": "2023-08-08T01:46:28.910629Z",
     "shell.execute_reply": "2023-08-08T01:46:28.910001Z"
    }
   },
   "outputs": [],
   "source": [
    "from hover.recipes.stable import simple_annotator\n",
    "\n",
    "interactive_plot = simple_annotator(dataset)\n",
    "\n",
    "# ---------- NOTEBOOK MODE: for your actual Jupyter environment ---------\n",
    "# this code will render the entire plot in Jupyter\n",
    "# from bokeh.io import show, output_notebook\n",
    "# output_notebook()\n",
    "# show(interactive_plot, notebook_url='https://localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce086e31",
   "metadata": {},
   "source": [
    "-   <details open><summary>What's special for audios?</summary>\n",
    "    **Tooltips**\n",
    "\n",
    "    For text, the tooltip shows the original value.\n",
    "\n",
    "    For audios, the tooltip embeds the audio based on URL.\n",
    "\n",
    "    -   audios in the local file system shall be served through [`python -m http.server`](https://docs.python.org/3/library/http.server.html).\n",
    "    -   they can then be accessed through `https://localhost:<port>/relative/path/to/file`.\n",
    "\n",
    "    **Search**\n",
    "\n",
    "    For text, the search widget is based on regular expressions.\n",
    "\n",
    "    For audios, the search widget is based on vector cosine similarity.\n",
    "\n",
    "    -   the `dataset` has remembered the `vectorizer` under the hood and passed it to the `annotator`.\n",
    "    -   {== please [**let us know**](https://github.com/phurwicz/hover/issues/new) if you think there's a better way to search audios in this case. ==}\n",
    "        -   dynamic time warping, due to its running time (> 10ms per pair for small 100x10 MFCC arrays), is too slow for search.\n",
    "            -   we are experimenting with subsampled signals and pre-selected data points (by vector similarity, for example).\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
