{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hover - 0.7.0 Documentation Explore and label on a map of raw data. Get enough to feed your model in no time. hover speeds up data labeling through embedding + visualization + callbacks . You just need raw data and an embedding to get a map. Explore the map to find \"zones\" that are easy and those that are tricky. Start the conquest of your data by coloring the zones through wisdom! Features It's fast because it labels data in bulk. A semantic scatter plot of your data for labeling, equipped with Tooltip for each point on mouse hover Table view for inspecting all selected points Toggle buttons that clearly distinguish data subsets Search widgets for ad-hoc data highlight It's accurate because multiple angles work together. Supplementary views to use in conjunction with the annotator, including Finder: filter data by search criteria Softlabel: active learning by in-the-loop model prediction score Snorkel: custom functions for labeling and filtering It's flexible (and fun!) because the process never gets old. :toolbox: Additional tools and options that allow you to Go to higher dimensions (3D? 4D?) and choose your xy-axes Consecutively select across areas, dimensions, and views Kick outliers and fix mistakes Check out @phurwicz/hover-binder for a list of demo apps. Quickstart Code + Walkthrough -> Labeling App edit & run code right in your browser, with guides along the way. Jump to Labeling App interactive plot for labeling data, pre-built and hosted on Binder. Install Python: 3.7+ OS: Linux & Mac & Windows PyPI (for all releases): pip install hover Conda-forge (for 0.6.0 and above): conda install -c conda-forge hover For Windows users, we recommend Windows Subsystem for Linux . On Windows itself you will need C++ build tools for dependencies. Resources Binder repo Changelog Documentation Tutorials Project News Feb 25, 2022 version 0.7.0 is now available. Check out the changelog for details :partying_face:. Some tl-dr for the impatient: audio and image support supply audio/image files through URLs to label with hover ! any type supported by HTML (and your browser) will be supported here. high-dimensional support you can now use higher-than-2D embeddings. hover still plots in 2D, but you can dynamically choose which two dimension to use. Remarks Shoutouts Thanks to Bokeh because hover would not exist without linked plots and callbacks, or be nearly as good without embeddable server apps. Thanks to Philip Vollet for sharing hover with the community even when it was really green. Contributing All feedbacks are welcome, especially what you find lacking and want it fixed! ./requirements-dev.txt lists required packages for development. Pull requests are advised to use a superset of the pre-commit hooks listed in .pre-commit-config.yaml . Citation If you have found hover useful to your work, please let us know :hugs: @misc { hover, title= {{ hover } : label data at scale } , url= { https://github.com/phurwicz/hover } , note= { Open software from https://github.com/phurwicz/hover } , author= { Pavel Hurwicz and Haochuan Wei } , year= { 2021 } , }","title":"Home"},{"location":"#hover-070-documentation","text":"Explore and label on a map of raw data. Get enough to feed your model in no time. hover speeds up data labeling through embedding + visualization + callbacks . You just need raw data and an embedding to get a map. Explore the map to find \"zones\" that are easy and those that are tricky. Start the conquest of your data by coloring the zones through wisdom!","title":"Hover - 0.7.0 Documentation"},{"location":"#features","text":"It's fast because it labels data in bulk. A semantic scatter plot of your data for labeling, equipped with Tooltip for each point on mouse hover Table view for inspecting all selected points Toggle buttons that clearly distinguish data subsets Search widgets for ad-hoc data highlight It's accurate because multiple angles work together. Supplementary views to use in conjunction with the annotator, including Finder: filter data by search criteria Softlabel: active learning by in-the-loop model prediction score Snorkel: custom functions for labeling and filtering It's flexible (and fun!) because the process never gets old. :toolbox: Additional tools and options that allow you to Go to higher dimensions (3D? 4D?) and choose your xy-axes Consecutively select across areas, dimensions, and views Kick outliers and fix mistakes Check out @phurwicz/hover-binder for a list of demo apps.","title":"Features"},{"location":"#quickstart","text":"","title":"Quickstart"},{"location":"#code-walkthrough-labeling-app","text":"edit & run code right in your browser, with guides along the way.","title":"Code + Walkthrough -&gt; Labeling App"},{"location":"#jump-to-labeling-app","text":"interactive plot for labeling data, pre-built and hosted on Binder.","title":"Jump to Labeling App"},{"location":"#install","text":"Python: 3.7+ OS: Linux & Mac & Windows PyPI (for all releases): pip install hover Conda-forge (for 0.6.0 and above): conda install -c conda-forge hover For Windows users, we recommend Windows Subsystem for Linux . On Windows itself you will need C++ build tools for dependencies.","title":"Install"},{"location":"#resources","text":"Binder repo Changelog Documentation Tutorials","title":"Resources"},{"location":"#project-news","text":"Feb 25, 2022 version 0.7.0 is now available. Check out the changelog for details :partying_face:. Some tl-dr for the impatient: audio and image support supply audio/image files through URLs to label with hover ! any type supported by HTML (and your browser) will be supported here. high-dimensional support you can now use higher-than-2D embeddings. hover still plots in 2D, but you can dynamically choose which two dimension to use.","title":"Project News"},{"location":"#remarks","text":"","title":"Remarks"},{"location":"#shoutouts","text":"Thanks to Bokeh because hover would not exist without linked plots and callbacks, or be nearly as good without embeddable server apps. Thanks to Philip Vollet for sharing hover with the community even when it was really green.","title":"Shoutouts"},{"location":"#contributing","text":"All feedbacks are welcome, especially what you find lacking and want it fixed! ./requirements-dev.txt lists required packages for development. Pull requests are advised to use a superset of the pre-commit hooks listed in .pre-commit-config.yaml .","title":"Contributing"},{"location":"#citation","text":"If you have found hover useful to your work, please let us know :hugs: @misc { hover, title= {{ hover } : label data at scale } , url= { https://github.com/phurwicz/hover } , note= { Open software from https://github.com/phurwicz/hover } , author= { Pavel Hurwicz and Haochuan Wei } , year= { 2021 } , }","title":"Citation"},{"location":"pages/guides/datatype-multimodal/","text":"","title":"Datatype multimodal"},{"location":"pages/guides/g0-datatype-image/","text":"hover supports bulk-labeling images through their URLs. Let's do a quickstart for images and note what's different from texts. This page assumes that you have know the basics i.e. simple usage of dataset and annotator . Please visit the quickstart tutorial if you haven't done so. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: Dataset for Images hover handles images through their URL addresses. URLs are strings which can be easily stored, hashed, and looked up against. They are also convenient for rendering tooltips in the annotation interface. Similarly to SupervisableTextDataset , we can build one for images: from hover.core.dataset import SupervisableImageDataset import pandas as pd # this is a 1000-image-url set of ImageNet data # with custom labels: animal, object, food example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.7.0/imagenet_custom.csv\" df = pd.read_csv(example_csv_path).sample(frac=1).reset_index(drop=True) df[\"SUBSET\"] = \"raw\" df.loc[500:800, 'SUBSET'] = 'train' df.loc[800:900, 'SUBSET'] = 'dev' df.loc[900:, 'SUBSET'] = 'test' dataset = SupervisableImageDataset.from_pandas(df, feature_key=\"image\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) Vectorizer for Images We can follow a URL -> content -> image object -> vector path. import requests from functools import lru_cache @lru_cache(maxsize=10000) def url_to_content(url): \"\"\" Turn a URL to response content. \"\"\" response = requests.get(url) return response.content from PIL import Image from io import BytesIO @lru_cache(maxsize=10000) def url_to_image(url): \"\"\" Turn a URL to a PIL Image. \"\"\" img = Image.open(BytesIO(url_to_content(url))).convert(\"RGB\") return img Caching and reading from disk This guide uses @wrappy.memoize in place of @functools.lru_cache for caching. The benefit is that wrappy.memoize can persist the cache to disk, speeding up code across sessions. Cached values for this guide have been pre-computed, making it much master to run the guide. import torch import wrappy from efficientnet_pytorch import EfficientNet from torchvision import transforms # EfficientNet is a series of pre-trained models # https://github.com/lukemelas/EfficientNet-PyTorch effnet = EfficientNet.from_pretrained(\"efficientnet-b0\") effnet.eval() # standard transformations for ImageNet-trained models tfms = transforms.Compose( [ transforms.Resize(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), ] ) @wrappy.memoize(cache_limit=10000, persist_path='custom_cache/image_url_to_vector.pkl') def vectorizer(url): \"\"\" Using logits on ImageNet-1000 classes. \"\"\" img = tfms(url_to_image(url)).unsqueeze(0) with torch.no_grad(): outputs = effnet(img) return outputs.detach().numpy().flatten() Embedding and Plot This is exactly the same as in the quickstart, just switching to image data: # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) from hover.recipes.stable import simple_annotator interactive_plot = simple_annotator(dataset) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') What's special for images? Tooltips For text, the tooltip shows the original value. For images, the tooltip embeds the image based on URL. images in the local file system shall be served through python -m http.server . they can then be accessed through https://localhost:<port>/relative/path/to/file . Search For text, the search widget is based on regular expressions. For images, the search widget is based on vector cosine similarity. the dataset has remembered the vectorizer under the hood and passed it to the annotator . please let us know if you think there's a better way to search images in this case. @import url(\"../../../styles/monokai.css\");","title":"Image Data"},{"location":"pages/guides/g0-datatype-image/#dataset-for-images","text":"hover handles images through their URL addresses. URLs are strings which can be easily stored, hashed, and looked up against. They are also convenient for rendering tooltips in the annotation interface. Similarly to SupervisableTextDataset , we can build one for images: from hover.core.dataset import SupervisableImageDataset import pandas as pd # this is a 1000-image-url set of ImageNet data # with custom labels: animal, object, food example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.7.0/imagenet_custom.csv\" df = pd.read_csv(example_csv_path).sample(frac=1).reset_index(drop=True) df[\"SUBSET\"] = \"raw\" df.loc[500:800, 'SUBSET'] = 'train' df.loc[800:900, 'SUBSET'] = 'dev' df.loc[900:, 'SUBSET'] = 'test' dataset = SupervisableImageDataset.from_pandas(df, feature_key=\"image\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5)","title":"Dataset for Images"},{"location":"pages/guides/g0-datatype-image/#vectorizer-for-images","text":"We can follow a URL -> content -> image object -> vector path. import requests from functools import lru_cache @lru_cache(maxsize=10000) def url_to_content(url): \"\"\" Turn a URL to response content. \"\"\" response = requests.get(url) return response.content from PIL import Image from io import BytesIO @lru_cache(maxsize=10000) def url_to_image(url): \"\"\" Turn a URL to a PIL Image. \"\"\" img = Image.open(BytesIO(url_to_content(url))).convert(\"RGB\") return img Caching and reading from disk This guide uses @wrappy.memoize in place of @functools.lru_cache for caching. The benefit is that wrappy.memoize can persist the cache to disk, speeding up code across sessions. Cached values for this guide have been pre-computed, making it much master to run the guide. import torch import wrappy from efficientnet_pytorch import EfficientNet from torchvision import transforms # EfficientNet is a series of pre-trained models # https://github.com/lukemelas/EfficientNet-PyTorch effnet = EfficientNet.from_pretrained(\"efficientnet-b0\") effnet.eval() # standard transformations for ImageNet-trained models tfms = transforms.Compose( [ transforms.Resize(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), ] ) @wrappy.memoize(cache_limit=10000, persist_path='custom_cache/image_url_to_vector.pkl') def vectorizer(url): \"\"\" Using logits on ImageNet-1000 classes. \"\"\" img = tfms(url_to_image(url)).unsqueeze(0) with torch.no_grad(): outputs = effnet(img) return outputs.detach().numpy().flatten()","title":"Vectorizer for Images"},{"location":"pages/guides/g0-datatype-image/#embedding-and-plot","text":"This is exactly the same as in the quickstart, just switching to image data: # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) from hover.recipes.stable import simple_annotator interactive_plot = simple_annotator(dataset) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') What's special for images? Tooltips For text, the tooltip shows the original value. For images, the tooltip embeds the image based on URL. images in the local file system shall be served through python -m http.server . they can then be accessed through https://localhost:<port>/relative/path/to/file . Search For text, the search widget is based on regular expressions. For images, the search widget is based on vector cosine similarity. the dataset has remembered the vectorizer under the hood and passed it to the annotator . please let us know if you think there's a better way to search images in this case. @import url(\"../../../styles/monokai.css\");","title":"Embedding and Plot"},{"location":"pages/guides/g1-datatype-audio/","text":"hover supports bulk-labeling audios through their URLs. Let's do a quickstart for audios and note what's different from texts. This page assumes that you have know the basics i.e. simple usage of dataset and annotator . Please visit the quickstart tutorial if you haven't done so. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: Dataset for audios hover handles audios through their URL addresses. URLs are strings which can be easily stored, hashed, and looked up against. They are also convenient for rendering tooltips in the annotation interface. Similarly to SupervisableTextDataset , we can build one for audios: from hover.core.dataset import SupervisableAudioDataset import pandas as pd # this is a table of audio-MNIST (pronounced digit 0-9) urls, 100 audios per digit example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.7.0/audio_mnist.csv\" df = pd.read_csv(example_csv_path).sample(frac=1).reset_index(drop=True) df[\"SUBSET\"] = \"raw\" df.loc[500:800, 'SUBSET'] = 'train' df.loc[800:900, 'SUBSET'] = 'dev' df.loc[900:, 'SUBSET'] = 'test' dataset = SupervisableAudioDataset.from_pandas(df, feature_key=\"audio\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) Vectorizer for audios We can follow a URL -> content -> audio array -> vector path. import requests from functools import lru_cache @lru_cache(maxsize=10000) def url_to_content(url): \"\"\" Turn a URL to response content. \"\"\" response = requests.get(url) return response.content import librosa from io import BytesIO @lru_cache(maxsize=10000) def url_to_audio(url): \"\"\" Turn a URL to audio data. \"\"\" data, sampling_rate = librosa.load(BytesIO(url_to_content(url))) return data, sampling_rate Caching and reading from disk This guide uses @wrappy.memoize in place of @functools.lru_cache for caching. The benefit is that wrappy.memoize can persist the cache to disk, speeding up code across sessions. Cached values for this guide have been pre-computed, making it much master to run the guide. import wrappy @wrappy.memoize(cache_limit=10000, persist_path='custom_cache/audio_url_to_vector.pkl') def vectorizer(url): \"\"\" Averaged MFCC over time. Resembles word-embedding-average-as-doc-embedding for texts. \"\"\" y, sr = url_to_audio(url) mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=32) return mfcc.mean(axis=1) Embedding and Plot This is exactly the same as in the quickstart, just switching to audio data: # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) from hover.recipes.stable import simple_annotator interactive_plot = simple_annotator(dataset) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') What's special for audios? Tooltips For text, the tooltip shows the original value. For audios, the tooltip embeds the audio based on URL. audios in the local file system shall be served through python -m http.server . they can then be accessed through https://localhost:<port>/relative/path/to/file . Search For text, the search widget is based on regular expressions. For audios, the search widget is based on vector cosine similarity. the dataset has remembered the vectorizer under the hood and passed it to the annotator . please let us know if you think there's a better way to search audios in this case. dynamic time warping, due to its running time (> 10ms per pair for small 100x10 MFCC arrays), is too slow for search. we are experimenting with subsampled signals and pre-selected data points (by vector similarity, for example). @import url(\"../../../styles/monokai.css\");","title":"Audio Data"},{"location":"pages/guides/g1-datatype-audio/#dataset-for-audios","text":"hover handles audios through their URL addresses. URLs are strings which can be easily stored, hashed, and looked up against. They are also convenient for rendering tooltips in the annotation interface. Similarly to SupervisableTextDataset , we can build one for audios: from hover.core.dataset import SupervisableAudioDataset import pandas as pd # this is a table of audio-MNIST (pronounced digit 0-9) urls, 100 audios per digit example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.7.0/audio_mnist.csv\" df = pd.read_csv(example_csv_path).sample(frac=1).reset_index(drop=True) df[\"SUBSET\"] = \"raw\" df.loc[500:800, 'SUBSET'] = 'train' df.loc[800:900, 'SUBSET'] = 'dev' df.loc[900:, 'SUBSET'] = 'test' dataset = SupervisableAudioDataset.from_pandas(df, feature_key=\"audio\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5)","title":"Dataset for audios"},{"location":"pages/guides/g1-datatype-audio/#vectorizer-for-audios","text":"We can follow a URL -> content -> audio array -> vector path. import requests from functools import lru_cache @lru_cache(maxsize=10000) def url_to_content(url): \"\"\" Turn a URL to response content. \"\"\" response = requests.get(url) return response.content import librosa from io import BytesIO @lru_cache(maxsize=10000) def url_to_audio(url): \"\"\" Turn a URL to audio data. \"\"\" data, sampling_rate = librosa.load(BytesIO(url_to_content(url))) return data, sampling_rate Caching and reading from disk This guide uses @wrappy.memoize in place of @functools.lru_cache for caching. The benefit is that wrappy.memoize can persist the cache to disk, speeding up code across sessions. Cached values for this guide have been pre-computed, making it much master to run the guide. import wrappy @wrappy.memoize(cache_limit=10000, persist_path='custom_cache/audio_url_to_vector.pkl') def vectorizer(url): \"\"\" Averaged MFCC over time. Resembles word-embedding-average-as-doc-embedding for texts. \"\"\" y, sr = url_to_audio(url) mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=32) return mfcc.mean(axis=1)","title":"Vectorizer for audios"},{"location":"pages/guides/g1-datatype-audio/#embedding-and-plot","text":"This is exactly the same as in the quickstart, just switching to audio data: # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) from hover.recipes.stable import simple_annotator interactive_plot = simple_annotator(dataset) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') What's special for audios? Tooltips For text, the tooltip shows the original value. For audios, the tooltip embeds the audio based on URL. audios in the local file system shall be served through python -m http.server . they can then be accessed through https://localhost:<port>/relative/path/to/file . Search For text, the search widget is based on regular expressions. For audios, the search widget is based on vector cosine similarity. the dataset has remembered the vectorizer under the hood and passed it to the annotator . please let us know if you think there's a better way to search audios in this case. dynamic time warping, due to its running time (> 10ms per pair for small 100x10 MFCC arrays), is too slow for search. we are experimenting with subsampled signals and pre-selected data points (by vector similarity, for example). @import url(\"../../../styles/monokai.css\");","title":"Embedding and Plot"},{"location":"pages/reference/core-dataset/","text":"Dataset classes which extend beyond DataFrames. When we supervise a collection of data, these operations need to be simple: managing raw / train / dev / test subsets transferring data points between subsets pulling updates from annotation interfaces pushing updates to annotation interfaces getting a 2D embedding loading data for training models SupervisableAudioDataset ( SupervisableDataset ) SupervisableDataset whose primary feature is audio . Source code in hover/core/dataset.py class SupervisableAudioDataset ( SupervisableDataset ): \"\"\" ???+ note \"`SupervisableDataset` whose primary feature is `audio`.\" \"\"\" FEATURE_KEY = \"audio\" SupervisableDataset ( Loggable ) Feature-agnostic class for a dataset open to supervision. Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that the DataFrame form supports most kinds of operations; the list-of-dicts form could be useful for manipulations outside the scope of pandas; synchronization between the two forms should be called sparingly. Source code in hover/core/dataset.py class SupervisableDataset ( Loggable ): \"\"\" ???+ note \"Feature-agnostic class for a dataset open to supervision.\" Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that - the DataFrame form supports most kinds of operations; - the list-of-dicts form could be useful for manipulations outside the scope of pandas; - synchronization between the two forms should be called sparingly. \"\"\" # 'scratch': intended to be directly editable by other objects, i.e. Explorers # labels will be stored but not used for information in hover itself SCRATCH_SUBSETS = tuple ([ \"raw\" ]) # non-'scratch': intended to be read-only outside of the class # 'public': labels will be considered as part of the classification task and will be used for built-in supervision PUBLIC_SUBSETS = tuple ([ \"train\" , \"dev\" ]) # 'private': labels will be considered as part of the classification task and will NOT be used for supervision PRIVATE_SUBSETS = tuple ([ \"test\" ]) FEATURE_KEY = \"feature\" def __init__ ( self , * args , ** kwargs ): \"\"\" ???+ note \"Set up data subsets, widgets, and supplementary data structures.\" See `self.setup_dfs` for parameter details. \"\"\" self . _info ( \"Initializing...\" ) self . setup_dfs ( * args , ** kwargs ) self . df_deduplicate () self . compute_feature_index () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_file_export () self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . setup_sel_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _vectorizer_lookup = OrderedDict () self . _good ( f \" { self . __class__ . __name__ } : finished initialization.\" ) def setup_dfs ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ note \"Subroutine of the constructor that creates standard-format DataFrames.\" | Param | Type | Description | | :------------ | :----- | :----------------------------------- | | `raw_dictl` | `list` | list of dicts holding the **to-be-supervised** raw data | | `train_dictl` | `list` | list of dicts holding any **supervised** train data | | `dev_dictl` | `list` | list of dicts holding any **supervised** dev data | | `test_dictl` | `list` | list of dicts holding any **supervised** test data | | `feature_key` | `str` | the key for the feature in each piece of data | | `label_key` | `str` | the key for the `**str**` label in supervised data | \"\"\" def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] # standardize records dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } # initialize dataframes self . dfs = dict () for _key , _dictl in dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df def copy ( self ): \"\"\" ???+ note \"Create another instance, copying over the data entries.\" Also copy data structures that don't get created in the new instance. \"\"\" dataset = self . __class__ . from_pandas ( self . to_pandas ()) dataset . _vectorizer_lookup . update ( self . _vectorizer_lookup ) return dataset def compute_feature_index ( self ): \"\"\" ???+ note \"Allow lookup by feature value without setting it as the index.\" Assumes that feature values are unique. The reason not to just set the feature as the index is because integer indices work smoothly with Bokeh `DataSource`s, NumPy `array`s, and Torch `Tensor`s. \"\"\" feature_to_subset_idx = {} for _subset , _df in self . dfs . items (): _values = _df [ self . __class__ . FEATURE_KEY ] . values for i , _val in enumerate ( _values ): if _val in feature_to_subset_idx : raise ValueError ( f \"Expected unique feature values, found duplicate { _val } \" ) feature_to_subset_idx [ _val ] = ( _subset , i ) self . feature_to_subset_idx = feature_to_subset_idx def locate_by_feature_value ( self , value , auto_recompute = True ): \"\"\" ???+ note \"Find the subset and index given a feature value.\" Assumes that the value is present and detects if the subset and index found is consistent with the value. \"\"\" subset , index = self . feature_to_subset_idx [ value ] current_value = self . dfs [ subset ] . at [ index , self . __class__ . FEATURE_KEY ] if current_value != value : if auto_recompute : self . _warn ( \"locate_by_feature_value mismatch. Recomputing index.\" ) self . compute_feature_index () # if ever need to recompute twice, there must be a bug return self . locate_by_feature_value ( value , auto_recompute = False ) else : raise ValueError ( \"locate_by_feature_value mismatch.\" ) return subset , index def to_pandas ( self ): \"\"\" ???+ note \"Export to a pandas DataFrame.\" \"\"\" dfs = [] for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _df = self . dfs [ _subset ] . copy () _df [ DATASET_SUBSET_FIELD ] = _subset dfs . append ( _df ) return pd . concat ( dfs , axis = 0 ) @classmethod def from_pandas ( cls , df , ** kwargs ): \"\"\" ???+ note \"Import from a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `df` | `DataFrame` | with a \"SUBSET\" field dividing subsets | \"\"\" SUBSETS = cls . SCRATCH_SUBSETS + cls . PUBLIC_SUBSETS + cls . PRIVATE_SUBSETS if DATASET_SUBSET_FIELD not in df . columns : raise ValueError ( f \"Expecting column ' { DATASET_SUBSET_FIELD } ' in the DataFrame which takes values from { SUBSETS } \" ) dictls = {} for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _sub_df = df [ df [ DATASET_SUBSET_FIELD ] == _subset ] dictls [ _subset ] = _sub_df . to_dict ( orient = \"records\" ) return cls ( raw_dictl = dictls [ \"raw\" ], train_dictl = dictls [ \"train\" ], dev_dictl = dictls [ \"dev\" ], test_dictl = dictls [ \"test\" ], ** kwargs , ) def setup_widgets ( self ): \"\"\" ???+ note \"Create `bokeh` widgets for interactive data management.\" Operations: - PUSH: push updated dataframes to linked `explorer`s. - COMMIT: added selected points to a specific subset `dataframe`. - DEDUP: cross-deduplicate across all subset `dataframe`s. - VIEW: view selected points of linked `explorer`s. - the link can be different from that for PUSH. Typically all the `explorer`s sync their selections, and only an `annotator` is linked to the `dataset`. - PATCH: update a few edited rows from VIEW result to the dataset. - EVICT: remove a few rows from both VIEW result and linked `explorer` selection. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_viewer = Button ( label = \"View Selected\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_patcher = Button ( label = \"Update Row Values\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_evictor = Button ( label = \"Evict Rows from Selection\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. Changes dataset rows. No change to explorers. - PUSH shall be blocked until DEDUP is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. Changes dataset rows. No change to explorers. - COMMIT shall be blocked until PUSH is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. No change to dataset rows. Changes explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True # empty the selection table, then allow PATCH and EVICT self . sel_table . source . data = dict () self . sel_table . source . selected . indices = [] self . selection_patcher . disabled = False self . selection_evictor . disabled = False self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget () def view ( self ): \"\"\" ???+ note \"Defines the layout of `bokeh` objects when visualized.\" \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , # population table and directly associated widgets row ( self . update_pusher , self . data_committer , self . dedup_trigger , self . file_exporter , ), self . pop_table , # selection table and directly associated widgets row ( self . selection_viewer , self . selection_patcher , self . selection_evictor , ), self . sel_table , ) def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable pushing updated DataFrames to explorers that depend on them.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" explorer . link_dataset ( self ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" ) def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | \"\"\" explorer . link_dataset ( self ) def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) # chain another callback self . _callback_update_population () self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" ) def subscribe_selection_view ( self , explorer , subsets ): \"\"\" ???+ note \"Enable viewing groups of data entries, specified by a selection in an explorer.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subsets` | `list` | subset selections to consider | \"\"\" assert ( isinstance ( subsets , list ) and len ( subsets ) > 0 ), \"Expected a non-empty list of subsets\" explorer . link_dataset ( self ) def callback_view (): sel_slices = [] for subset in subsets : selected_idx = sorted ( explorer . sources [ subset ] . selected . indices ) sub_slice = explorer . dfs [ subset ] . iloc [ selected_idx ] sel_slices . append ( sub_slice ) selected = pd . concat ( sel_slices , axis = 0 ) self . _callback_update_selection ( selected ) def callback_evict (): # create sets for fast index discarding subset_to_indicies = {} for subset in subsets : indicies = set ( explorer . sources [ subset ] . selected . indices ) subset_to_indicies [ subset ] = indicies # from datatable index, get feature values to look up dataframe index sel_source = self . sel_table . source raw_indicies = sel_source . selected . indices for i in raw_indicies : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) subset_to_indicies [ subset ] . discard ( idx ) # assign indices back to change actual selection for subset in subsets : indicies = sorted ( list ( subset_to_indicies [ subset ])) explorer . sources [ subset ] . selected . indices = indicies self . _good ( f \"Selection table: evicted { len ( raw_indicies ) } points from selection.\" ) # refresh the selection table callback_view () self . selection_viewer . on_click ( callback_view ) self . selection_evictor . on_click ( callback_evict ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to selection table: { subsets } \" ) def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ note \"Auto-determine labels in the dataset, then create encoder/decoder in lexical order.\" Add `\"ABSTAIN\"` as a no-label placeholder which gets ignored categorically. | Param | Type | Description | | :-------- | :----- | :--------------------------------- | | `verbose` | `bool` | whether to log verbosely | | `debug` | `bool` | whether to enable label validation | \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels () def validate_labels ( self , raise_exception = True ): \"\"\" ???+ note \"Assert that every label is in the encoder.\" | Param | Type | Description | | :---------------- | :----- | :---------------------------------- | | `raise_exception` | `bool` | whether to raise errors when failed | \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : int ( x in self . label_encoder ) ) # DO NOT change the \"==\" to \"is\"; False in pandas is not False below _invalid_indices = np . where ( _mask == 0 )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ( self . dfs [ _key ] . loc [ _invalid_indices ]) if raise_exception : raise ValueError ( \"invalid labels\" ) def setup_file_export ( self ): self . file_exporter = Dropdown ( label = \"Export\" , button_type = \"warning\" , menu = [ \"Excel\" , \"CSV\" , \"JSON\" , \"pickle\" ], height_policy = \"fit\" , width_policy = \"min\" , ) def callback_export ( event , path_root = None ): \"\"\" A callback on clicking the 'self.annotator_export' button. Saves the dataframe to a pickle. \"\"\" export_format = event . item # auto-determine the export path root if path_root is None : timestamp = current_time ( \"%Y%m %d %H%M%S\" ) path_root = f \"hover-dataset-export- { timestamp } \" export_df = self . to_pandas () if export_format == \"Excel\" : export_path = f \" { path_root } .xlsx\" export_df . to_excel ( export_path , index = False ) elif export_format == \"CSV\" : export_path = f \" { path_root } .csv\" export_df . to_csv ( export_path , index = False ) elif export_format == \"JSON\" : export_path = f \" { path_root } .json\" export_df . to_json ( export_path , orient = \"records\" ) elif export_format == \"pickle\" : export_path = f \" { path_root } .pkl\" export_df . to_pickle ( export_path ) else : raise ValueError ( f \"Unexpected export format { export_format } \" ) self . _good ( f \"saved Pandas DataFrame version to { export_path } \" ) # assign the callback, keeping its reference self . _callback_export = callback_export self . file_exporter . on_click ( self . _callback_export ) def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for monitoring subset data populations.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], TableColumn ( field = \"color\" , title = \"color\" , formatter = HTMLTemplateFormatter ( template = COLOR_GLYPH_TEMPLATE ), ), ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] color_dict = auto_label_color ( self . classes ) eff_colors = [ color_dict [ _label ] for _label in eff_labels ] pop_data = dict ( color = eff_colors , label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Population updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . dedup_trigger . on_click ( update_population ) # store the callback so that it can be referenced by other methods self . _callback_update_population = update_population def setup_sel_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for viewing selected data points.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" sel_source = ColumnDataSource ( dict ()) sel_columns = dataset_default_sel_table_columns ( self . __class__ . FEATURE_KEY ) table_kwargs = dataset_default_sel_table_kwargs ( self . __class__ . FEATURE_KEY ) table_kwargs . update ( kwargs ) self . sel_table = DataTable ( source = sel_source , columns = sel_columns , ** table_kwargs ) def update_selection ( selected_df ): \"\"\" To be triggered as a subroutine of `self.selection_viewer`. \"\"\" sel_source . data = selected_df . to_dict ( orient = \"list\" ) # now that selection table has changed, clear sub-selection sel_source . selected . indices = [] self . _good ( f \"Selection table: latest selection with { selected_df . shape [ 0 ] } entries.\" ) self . _callback_update_selection = update_selection def patch_edited_selection (): sel_source = self . sel_table . source raw_indices = sel_source . selected . indices for i in raw_indices : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) for key in sel_source . data . keys (): self . dfs [ subset ] . at [ idx , key ] = sel_source . data [ key ][ i ] self . _good ( f \"Selection table: edited { len ( raw_indices ) } dataset rows.\" ) # if edited labels (which is common), then population has changed self . _callback_update_population () self . selection_patcher . on_click ( patch_edited_selection ) def df_deduplicate ( self ): \"\"\" ???+ note \"Cross-deduplicate data entries by feature between subsets.\" \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" ) self . compute_feature_index () @property def vectorizer_lookup ( self ): return self . _vectorizer_lookup @vectorizer_lookup . setter def vectorizer_lookup ( self , * args , ** kwargs ): self . _fail ( \"assigning vectorizer lookup by reference is forbidden.\" ) def compute_nd_embedding ( self , vectorizer , method , dimension = 2 , ** kwargs ): \"\"\" ???+ note \"Get embeddings in n-dimensional space and return the dimensionality reducer.\" Reference: [`DimensionalityReducer`](https://github.com/phurwicz/hover/blob/main/hover/core/representation/reduction.py) | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `dimension` | `int` | dimension of output embedding | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" from hover.core.representation.reduction import DimensionalityReducer # register the vectorizer for scenarios that may need it self . vectorizer_lookup [ dimension ] = vectorizer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" assert isinstance ( dimension , int ) and dimension >= 2 embedding_cols = [ embedding_field ( dimension , i ) for i in range ( dimension )] # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) trans_arr = np . array ( [ vectorizer ( _inp ) for _inp in tqdm ( feature_inp , desc = \"Vectorizing\" )] ) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , dimension = dimension , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] for _i in range ( dimension ): _col = embedding_cols [ _i ] self . dfs [ _key ][ _col ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), _i ] ) start_idx += _length self . _good ( f \"Computed { dimension } -d embedding in columns { embedding_cols } \" ) return reducer def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ note \"Get embeddings in the xy-plane and return the dimensionality reducer.\" A special case of `compute_nd_embedding`. | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" reducer = self . compute_nd_embedding ( vectorizer , method , dimension = 2 , ** kwargs ) return reducer def loader ( self , key , * vectorizers , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ note \"Prepare a torch `Dataloader` for training or evaluation.\" | Param | Type | Description | | :------------ | :------------ | :--------------------------------- | | `key` | `str` | subset of data, e.g. `\"train\"` | | `vectorizers` | `callable`(s) | the feature -> vector function(s) | | `batch_size` | `int` | size per batch | | `smoothing_coeff` | `float` | portion of probability to equally split between classes | \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import ( VectorDataset , MultiVectorDataset , one_hot , label_smoothing , ) # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) # prepare output vectors labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) # prepare input vectors assert len ( vectorizers ) > 0 , \"Expected at least one vectorizer\" multi_flag = len ( vectorizers ) > 1 features = df [ self . __class__ . FEATURE_KEY ] . tolist () input_vector_lists = [] for _vec_func in vectorizers : self . _info ( f \"Preparing { key } input vectors...\" ) _input_vecs = [ _vec_func ( _f ) for _f in tqdm ( features , desc = \"Vectorizing\" )] input_vector_lists . append ( _input_vecs ) self . _info ( f \"Preparing { key } data loader...\" ) if multi_flag : assert len ( input_vector_lists ) > 1 , \"Expected multiple lists of vectors\" loader = MultiVectorDataset ( input_vector_lists , output_vectors ) . loader ( batch_size = batch_size ) else : assert len ( input_vector_lists ) == 1 , \"Expected only one list of vectors\" input_vectors = input_vector_lists [ 0 ] loader = VectorDataset ( input_vectors , output_vectors ) . loader ( batch_size = batch_size ) self . _good ( f \"Prepared { key } loader with { len ( features ) } examples; { len ( vectorizers ) } vectors per feature, batch size { batch_size } \" ) return loader __init__ ( self , * args , ** kwargs ) special Set up data subsets, widgets, and supplementary data structures. See self.setup_dfs for parameter details. Source code in hover/core/dataset.py def __init__ ( self , * args , ** kwargs ): \"\"\" ???+ note \"Set up data subsets, widgets, and supplementary data structures.\" See `self.setup_dfs` for parameter details. \"\"\" self . _info ( \"Initializing...\" ) self . setup_dfs ( * args , ** kwargs ) self . df_deduplicate () self . compute_feature_index () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_file_export () self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . setup_sel_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _vectorizer_lookup = OrderedDict () self . _good ( f \" { self . __class__ . __name__ } : finished initialization.\" ) compute_2d_embedding ( self , vectorizer , method , ** kwargs ) Get embeddings in the xy-plane and return the dimensionality reducer. A special case of compute_nd_embedding . Param Type Description vectorizer callable the feature -> vector function method str arg for DimensionalityReducer **kwargs kwargs for DimensionalityReducer Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ note \"Get embeddings in the xy-plane and return the dimensionality reducer.\" A special case of `compute_nd_embedding`. | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" reducer = self . compute_nd_embedding ( vectorizer , method , dimension = 2 , ** kwargs ) return reducer compute_feature_index ( self ) Allow lookup by feature value without setting it as the index. Assumes that feature values are unique. The reason not to just set the feature as the index is because integer indices work smoothly with Bokeh DataSource s, NumPy array s, and Torch Tensor s. Source code in hover/core/dataset.py def compute_feature_index ( self ): \"\"\" ???+ note \"Allow lookup by feature value without setting it as the index.\" Assumes that feature values are unique. The reason not to just set the feature as the index is because integer indices work smoothly with Bokeh `DataSource`s, NumPy `array`s, and Torch `Tensor`s. \"\"\" feature_to_subset_idx = {} for _subset , _df in self . dfs . items (): _values = _df [ self . __class__ . FEATURE_KEY ] . values for i , _val in enumerate ( _values ): if _val in feature_to_subset_idx : raise ValueError ( f \"Expected unique feature values, found duplicate { _val } \" ) feature_to_subset_idx [ _val ] = ( _subset , i ) self . feature_to_subset_idx = feature_to_subset_idx compute_nd_embedding ( self , vectorizer , method , dimension = 2 , ** kwargs ) Get embeddings in n-dimensional space and return the dimensionality reducer. Reference: DimensionalityReducer Param Type Description vectorizer callable the feature -> vector function method str arg for DimensionalityReducer dimension int dimension of output embedding **kwargs kwargs for DimensionalityReducer Source code in hover/core/dataset.py def compute_nd_embedding ( self , vectorizer , method , dimension = 2 , ** kwargs ): \"\"\" ???+ note \"Get embeddings in n-dimensional space and return the dimensionality reducer.\" Reference: [`DimensionalityReducer`](https://github.com/phurwicz/hover/blob/main/hover/core/representation/reduction.py) | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `dimension` | `int` | dimension of output embedding | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" from hover.core.representation.reduction import DimensionalityReducer # register the vectorizer for scenarios that may need it self . vectorizer_lookup [ dimension ] = vectorizer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" assert isinstance ( dimension , int ) and dimension >= 2 embedding_cols = [ embedding_field ( dimension , i ) for i in range ( dimension )] # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) trans_arr = np . array ( [ vectorizer ( _inp ) for _inp in tqdm ( feature_inp , desc = \"Vectorizing\" )] ) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , dimension = dimension , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] for _i in range ( dimension ): _col = embedding_cols [ _i ] self . dfs [ _key ][ _col ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), _i ] ) start_idx += _length self . _good ( f \"Computed { dimension } -d embedding in columns { embedding_cols } \" ) return reducer copy ( self ) Create another instance, copying over the data entries. Also copy data structures that don't get created in the new instance. Source code in hover/core/dataset.py def copy ( self ): \"\"\" ???+ note \"Create another instance, copying over the data entries.\" Also copy data structures that don't get created in the new instance. \"\"\" dataset = self . __class__ . from_pandas ( self . to_pandas ()) dataset . _vectorizer_lookup . update ( self . _vectorizer_lookup ) return dataset df_deduplicate ( self ) Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" ???+ note \"Cross-deduplicate data entries by feature between subsets.\" \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" ) self . compute_feature_index () from_pandas ( df , ** kwargs ) classmethod Import from a pandas DataFrame. Param Type Description df DataFrame with a \"SUBSET\" field dividing subsets Source code in hover/core/dataset.py @classmethod def from_pandas ( cls , df , ** kwargs ): \"\"\" ???+ note \"Import from a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `df` | `DataFrame` | with a \"SUBSET\" field dividing subsets | \"\"\" SUBSETS = cls . SCRATCH_SUBSETS + cls . PUBLIC_SUBSETS + cls . PRIVATE_SUBSETS if DATASET_SUBSET_FIELD not in df . columns : raise ValueError ( f \"Expecting column ' { DATASET_SUBSET_FIELD } ' in the DataFrame which takes values from { SUBSETS } \" ) dictls = {} for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _sub_df = df [ df [ DATASET_SUBSET_FIELD ] == _subset ] dictls [ _subset ] = _sub_df . to_dict ( orient = \"records\" ) return cls ( raw_dictl = dictls [ \"raw\" ], train_dictl = dictls [ \"train\" ], dev_dictl = dictls [ \"dev\" ], test_dictl = dictls [ \"test\" ], ** kwargs , ) loader ( self , key , * vectorizers , * , batch_size = 64 , smoothing_coeff = 0.0 ) Prepare a torch Dataloader for training or evaluation. Param Type Description key str subset of data, e.g. \"train\" vectorizers callable (s) the feature -> vector function(s) batch_size int size per batch smoothing_coeff float portion of probability to equally split between classes Source code in hover/core/dataset.py def loader ( self , key , * vectorizers , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ note \"Prepare a torch `Dataloader` for training or evaluation.\" | Param | Type | Description | | :------------ | :------------ | :--------------------------------- | | `key` | `str` | subset of data, e.g. `\"train\"` | | `vectorizers` | `callable`(s) | the feature -> vector function(s) | | `batch_size` | `int` | size per batch | | `smoothing_coeff` | `float` | portion of probability to equally split between classes | \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import ( VectorDataset , MultiVectorDataset , one_hot , label_smoothing , ) # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) # prepare output vectors labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) # prepare input vectors assert len ( vectorizers ) > 0 , \"Expected at least one vectorizer\" multi_flag = len ( vectorizers ) > 1 features = df [ self . __class__ . FEATURE_KEY ] . tolist () input_vector_lists = [] for _vec_func in vectorizers : self . _info ( f \"Preparing { key } input vectors...\" ) _input_vecs = [ _vec_func ( _f ) for _f in tqdm ( features , desc = \"Vectorizing\" )] input_vector_lists . append ( _input_vecs ) self . _info ( f \"Preparing { key } data loader...\" ) if multi_flag : assert len ( input_vector_lists ) > 1 , \"Expected multiple lists of vectors\" loader = MultiVectorDataset ( input_vector_lists , output_vectors ) . loader ( batch_size = batch_size ) else : assert len ( input_vector_lists ) == 1 , \"Expected only one list of vectors\" input_vectors = input_vector_lists [ 0 ] loader = VectorDataset ( input_vectors , output_vectors ) . loader ( batch_size = batch_size ) self . _good ( f \"Prepared { key } loader with { len ( features ) } examples; { len ( vectorizers ) } vectors per feature, batch size { batch_size } \" ) return loader locate_by_feature_value ( self , value , auto_recompute = True ) Find the subset and index given a feature value. Assumes that the value is present and detects if the subset and index found is consistent with the value. Source code in hover/core/dataset.py def locate_by_feature_value ( self , value , auto_recompute = True ): \"\"\" ???+ note \"Find the subset and index given a feature value.\" Assumes that the value is present and detects if the subset and index found is consistent with the value. \"\"\" subset , index = self . feature_to_subset_idx [ value ] current_value = self . dfs [ subset ] . at [ index , self . __class__ . FEATURE_KEY ] if current_value != value : if auto_recompute : self . _warn ( \"locate_by_feature_value mismatch. Recomputing index.\" ) self . compute_feature_index () # if ever need to recompute twice, there must be a bug return self . locate_by_feature_value ( value , auto_recompute = False ) else : raise ValueError ( \"locate_by_feature_value mismatch.\" ) return subset , index setup_dfs ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = 'feature' , label_key = 'label' ) Subroutine of the constructor that creates standard-format DataFrames. Param Type Description raw_dictl list list of dicts holding the to-be-supervised raw data train_dictl list list of dicts holding any supervised train data dev_dictl list list of dicts holding any supervised dev data test_dictl list list of dicts holding any supervised test data feature_key str the key for the feature in each piece of data label_key str the key for the **str** label in supervised data Source code in hover/core/dataset.py def setup_dfs ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ note \"Subroutine of the constructor that creates standard-format DataFrames.\" | Param | Type | Description | | :------------ | :----- | :----------------------------------- | | `raw_dictl` | `list` | list of dicts holding the **to-be-supervised** raw data | | `train_dictl` | `list` | list of dicts holding any **supervised** train data | | `dev_dictl` | `list` | list of dicts holding any **supervised** dev data | | `test_dictl` | `list` | list of dicts holding any **supervised** test data | | `feature_key` | `str` | the key for the feature in each piece of data | | `label_key` | `str` | the key for the `**str**` label in supervised data | \"\"\" def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] # standardize records dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } # initialize dataframes self . dfs = dict () for _key , _dictl in dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df setup_label_coding ( self , verbose = True , debug = False ) Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add \"ABSTAIN\" as a no-label placeholder which gets ignored categorically. Param Type Description verbose bool whether to log verbosely debug bool whether to enable label validation Source code in hover/core/dataset.py def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ note \"Auto-determine labels in the dataset, then create encoder/decoder in lexical order.\" Add `\"ABSTAIN\"` as a no-label placeholder which gets ignored categorically. | Param | Type | Description | | :-------- | :----- | :--------------------------------- | | `verbose` | `bool` | whether to log verbosely | | `debug` | `bool` | whether to enable label validation | \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels () setup_pop_table ( self , ** kwargs ) Set up a bokeh DataTable widget for monitoring subset data populations. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for monitoring subset data populations.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], TableColumn ( field = \"color\" , title = \"color\" , formatter = HTMLTemplateFormatter ( template = COLOR_GLYPH_TEMPLATE ), ), ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] color_dict = auto_label_color ( self . classes ) eff_colors = [ color_dict [ _label ] for _label in eff_labels ] pop_data = dict ( color = eff_colors , label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Population updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . dedup_trigger . on_click ( update_population ) # store the callback so that it can be referenced by other methods self . _callback_update_population = update_population setup_sel_table ( self , ** kwargs ) Set up a bokeh DataTable widget for viewing selected data points. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_sel_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for viewing selected data points.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" sel_source = ColumnDataSource ( dict ()) sel_columns = dataset_default_sel_table_columns ( self . __class__ . FEATURE_KEY ) table_kwargs = dataset_default_sel_table_kwargs ( self . __class__ . FEATURE_KEY ) table_kwargs . update ( kwargs ) self . sel_table = DataTable ( source = sel_source , columns = sel_columns , ** table_kwargs ) def update_selection ( selected_df ): \"\"\" To be triggered as a subroutine of `self.selection_viewer`. \"\"\" sel_source . data = selected_df . to_dict ( orient = \"list\" ) # now that selection table has changed, clear sub-selection sel_source . selected . indices = [] self . _good ( f \"Selection table: latest selection with { selected_df . shape [ 0 ] } entries.\" ) self . _callback_update_selection = update_selection def patch_edited_selection (): sel_source = self . sel_table . source raw_indices = sel_source . selected . indices for i in raw_indices : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) for key in sel_source . data . keys (): self . dfs [ subset ] . at [ idx , key ] = sel_source . data [ key ][ i ] self . _good ( f \"Selection table: edited { len ( raw_indices ) } dataset rows.\" ) # if edited labels (which is common), then population has changed self . _callback_update_population () self . selection_patcher . on_click ( patch_edited_selection ) setup_widgets ( self ) Create bokeh widgets for interactive data management. Operations: - PUSH: push updated dataframes to linked explorer s. - COMMIT: added selected points to a specific subset dataframe . - DEDUP: cross-deduplicate across all subset dataframe s. - VIEW: view selected points of linked explorer s. - the link can be different from that for PUSH. Typically all the explorer s sync their selections, and only an annotator is linked to the dataset . - PATCH: update a few edited rows from VIEW result to the dataset. - EVICT: remove a few rows from both VIEW result and linked explorer selection. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" ???+ note \"Create `bokeh` widgets for interactive data management.\" Operations: - PUSH: push updated dataframes to linked `explorer`s. - COMMIT: added selected points to a specific subset `dataframe`. - DEDUP: cross-deduplicate across all subset `dataframe`s. - VIEW: view selected points of linked `explorer`s. - the link can be different from that for PUSH. Typically all the `explorer`s sync their selections, and only an `annotator` is linked to the `dataset`. - PATCH: update a few edited rows from VIEW result to the dataset. - EVICT: remove a few rows from both VIEW result and linked `explorer` selection. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_viewer = Button ( label = \"View Selected\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_patcher = Button ( label = \"Update Row Values\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_evictor = Button ( label = \"Evict Rows from Selection\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. Changes dataset rows. No change to explorers. - PUSH shall be blocked until DEDUP is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. Changes dataset rows. No change to explorers. - COMMIT shall be blocked until PUSH is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. No change to dataset rows. Changes explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True # empty the selection table, then allow PATCH and EVICT self . sel_table . source . data = dict () self . sel_table . source . selected . indices = [] self . selection_patcher . disabled = False self . selection_evictor . disabled = False self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget () subscribe_data_commit ( self , explorer , subset_mapping ) Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | \"\"\" explorer . link_dataset ( self ) def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) # chain another callback self . _callback_update_population () self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" ) subscribe_selection_view ( self , explorer , subsets ) Enable viewing groups of data entries, specified by a selection in an explorer. Param Type Description explorer BokehBaseExplorer the explorer to register subsets list subset selections to consider Source code in hover/core/dataset.py def subscribe_selection_view ( self , explorer , subsets ): \"\"\" ???+ note \"Enable viewing groups of data entries, specified by a selection in an explorer.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subsets` | `list` | subset selections to consider | \"\"\" assert ( isinstance ( subsets , list ) and len ( subsets ) > 0 ), \"Expected a non-empty list of subsets\" explorer . link_dataset ( self ) def callback_view (): sel_slices = [] for subset in subsets : selected_idx = sorted ( explorer . sources [ subset ] . selected . indices ) sub_slice = explorer . dfs [ subset ] . iloc [ selected_idx ] sel_slices . append ( sub_slice ) selected = pd . concat ( sel_slices , axis = 0 ) self . _callback_update_selection ( selected ) def callback_evict (): # create sets for fast index discarding subset_to_indicies = {} for subset in subsets : indicies = set ( explorer . sources [ subset ] . selected . indices ) subset_to_indicies [ subset ] = indicies # from datatable index, get feature values to look up dataframe index sel_source = self . sel_table . source raw_indicies = sel_source . selected . indices for i in raw_indicies : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) subset_to_indicies [ subset ] . discard ( idx ) # assign indices back to change actual selection for subset in subsets : indicies = sorted ( list ( subset_to_indicies [ subset ])) explorer . sources [ subset ] . selected . indices = indicies self . _good ( f \"Selection table: evicted { len ( raw_indicies ) } points from selection.\" ) # refresh the selection table callback_view () self . selection_viewer . on_click ( callback_view ) self . selection_evictor . on_click ( callback_evict ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to selection table: { subsets } \" ) subscribe_update_push ( self , explorer , subset_mapping ) Enable pushing updated DataFrames to explorers that depend on them. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable pushing updated DataFrames to explorers that depend on them.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" explorer . link_dataset ( self ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" ) to_pandas ( self ) Export to a pandas DataFrame. Source code in hover/core/dataset.py def to_pandas ( self ): \"\"\" ???+ note \"Export to a pandas DataFrame.\" \"\"\" dfs = [] for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _df = self . dfs [ _subset ] . copy () _df [ DATASET_SUBSET_FIELD ] = _subset dfs . append ( _df ) return pd . concat ( dfs , axis = 0 ) validate_labels ( self , raise_exception = True ) Assert that every label is in the encoder. Param Type Description raise_exception bool whether to raise errors when failed Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" ???+ note \"Assert that every label is in the encoder.\" | Param | Type | Description | | :---------------- | :----- | :---------------------------------- | | `raise_exception` | `bool` | whether to raise errors when failed | \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : int ( x in self . label_encoder ) ) # DO NOT change the \"==\" to \"is\"; False in pandas is not False below _invalid_indices = np . where ( _mask == 0 )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ( self . dfs [ _key ] . loc [ _invalid_indices ]) if raise_exception : raise ValueError ( \"invalid labels\" ) view ( self ) Defines the layout of bokeh objects when visualized. Source code in hover/core/dataset.py def view ( self ): \"\"\" ???+ note \"Defines the layout of `bokeh` objects when visualized.\" \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , # population table and directly associated widgets row ( self . update_pusher , self . data_committer , self . dedup_trigger , self . file_exporter , ), self . pop_table , # selection table and directly associated widgets row ( self . selection_viewer , self . selection_patcher , self . selection_evictor , ), self . sel_table , ) SupervisableImageDataset ( SupervisableDataset ) SupervisableDataset whose primary feature is image . Source code in hover/core/dataset.py class SupervisableImageDataset ( SupervisableDataset ): \"\"\" ???+ note \"`SupervisableDataset` whose primary feature is `image`.\" \"\"\" FEATURE_KEY = \"image\" SupervisableTextDataset ( SupervisableDataset ) SupervisableDataset whose primary feature is text . Source code in hover/core/dataset.py class SupervisableTextDataset ( SupervisableDataset ): \"\"\" ???+ note \"`SupervisableDataset` whose primary feature is `text`.\" \"\"\" FEATURE_KEY = \"text\"","title":"hover.core.dataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableAudioDataset","text":"SupervisableDataset whose primary feature is audio . Source code in hover/core/dataset.py class SupervisableAudioDataset ( SupervisableDataset ): \"\"\" ???+ note \"`SupervisableDataset` whose primary feature is `audio`.\" \"\"\" FEATURE_KEY = \"audio\"","title":"SupervisableAudioDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset","text":"Feature-agnostic class for a dataset open to supervision. Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that the DataFrame form supports most kinds of operations; the list-of-dicts form could be useful for manipulations outside the scope of pandas; synchronization between the two forms should be called sparingly. Source code in hover/core/dataset.py class SupervisableDataset ( Loggable ): \"\"\" ???+ note \"Feature-agnostic class for a dataset open to supervision.\" Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that - the DataFrame form supports most kinds of operations; - the list-of-dicts form could be useful for manipulations outside the scope of pandas; - synchronization between the two forms should be called sparingly. \"\"\" # 'scratch': intended to be directly editable by other objects, i.e. Explorers # labels will be stored but not used for information in hover itself SCRATCH_SUBSETS = tuple ([ \"raw\" ]) # non-'scratch': intended to be read-only outside of the class # 'public': labels will be considered as part of the classification task and will be used for built-in supervision PUBLIC_SUBSETS = tuple ([ \"train\" , \"dev\" ]) # 'private': labels will be considered as part of the classification task and will NOT be used for supervision PRIVATE_SUBSETS = tuple ([ \"test\" ]) FEATURE_KEY = \"feature\" def __init__ ( self , * args , ** kwargs ): \"\"\" ???+ note \"Set up data subsets, widgets, and supplementary data structures.\" See `self.setup_dfs` for parameter details. \"\"\" self . _info ( \"Initializing...\" ) self . setup_dfs ( * args , ** kwargs ) self . df_deduplicate () self . compute_feature_index () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_file_export () self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . setup_sel_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _vectorizer_lookup = OrderedDict () self . _good ( f \" { self . __class__ . __name__ } : finished initialization.\" ) def setup_dfs ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ note \"Subroutine of the constructor that creates standard-format DataFrames.\" | Param | Type | Description | | :------------ | :----- | :----------------------------------- | | `raw_dictl` | `list` | list of dicts holding the **to-be-supervised** raw data | | `train_dictl` | `list` | list of dicts holding any **supervised** train data | | `dev_dictl` | `list` | list of dicts holding any **supervised** dev data | | `test_dictl` | `list` | list of dicts holding any **supervised** test data | | `feature_key` | `str` | the key for the feature in each piece of data | | `label_key` | `str` | the key for the `**str**` label in supervised data | \"\"\" def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] # standardize records dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } # initialize dataframes self . dfs = dict () for _key , _dictl in dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df def copy ( self ): \"\"\" ???+ note \"Create another instance, copying over the data entries.\" Also copy data structures that don't get created in the new instance. \"\"\" dataset = self . __class__ . from_pandas ( self . to_pandas ()) dataset . _vectorizer_lookup . update ( self . _vectorizer_lookup ) return dataset def compute_feature_index ( self ): \"\"\" ???+ note \"Allow lookup by feature value without setting it as the index.\" Assumes that feature values are unique. The reason not to just set the feature as the index is because integer indices work smoothly with Bokeh `DataSource`s, NumPy `array`s, and Torch `Tensor`s. \"\"\" feature_to_subset_idx = {} for _subset , _df in self . dfs . items (): _values = _df [ self . __class__ . FEATURE_KEY ] . values for i , _val in enumerate ( _values ): if _val in feature_to_subset_idx : raise ValueError ( f \"Expected unique feature values, found duplicate { _val } \" ) feature_to_subset_idx [ _val ] = ( _subset , i ) self . feature_to_subset_idx = feature_to_subset_idx def locate_by_feature_value ( self , value , auto_recompute = True ): \"\"\" ???+ note \"Find the subset and index given a feature value.\" Assumes that the value is present and detects if the subset and index found is consistent with the value. \"\"\" subset , index = self . feature_to_subset_idx [ value ] current_value = self . dfs [ subset ] . at [ index , self . __class__ . FEATURE_KEY ] if current_value != value : if auto_recompute : self . _warn ( \"locate_by_feature_value mismatch. Recomputing index.\" ) self . compute_feature_index () # if ever need to recompute twice, there must be a bug return self . locate_by_feature_value ( value , auto_recompute = False ) else : raise ValueError ( \"locate_by_feature_value mismatch.\" ) return subset , index def to_pandas ( self ): \"\"\" ???+ note \"Export to a pandas DataFrame.\" \"\"\" dfs = [] for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _df = self . dfs [ _subset ] . copy () _df [ DATASET_SUBSET_FIELD ] = _subset dfs . append ( _df ) return pd . concat ( dfs , axis = 0 ) @classmethod def from_pandas ( cls , df , ** kwargs ): \"\"\" ???+ note \"Import from a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `df` | `DataFrame` | with a \"SUBSET\" field dividing subsets | \"\"\" SUBSETS = cls . SCRATCH_SUBSETS + cls . PUBLIC_SUBSETS + cls . PRIVATE_SUBSETS if DATASET_SUBSET_FIELD not in df . columns : raise ValueError ( f \"Expecting column ' { DATASET_SUBSET_FIELD } ' in the DataFrame which takes values from { SUBSETS } \" ) dictls = {} for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _sub_df = df [ df [ DATASET_SUBSET_FIELD ] == _subset ] dictls [ _subset ] = _sub_df . to_dict ( orient = \"records\" ) return cls ( raw_dictl = dictls [ \"raw\" ], train_dictl = dictls [ \"train\" ], dev_dictl = dictls [ \"dev\" ], test_dictl = dictls [ \"test\" ], ** kwargs , ) def setup_widgets ( self ): \"\"\" ???+ note \"Create `bokeh` widgets for interactive data management.\" Operations: - PUSH: push updated dataframes to linked `explorer`s. - COMMIT: added selected points to a specific subset `dataframe`. - DEDUP: cross-deduplicate across all subset `dataframe`s. - VIEW: view selected points of linked `explorer`s. - the link can be different from that for PUSH. Typically all the `explorer`s sync their selections, and only an `annotator` is linked to the `dataset`. - PATCH: update a few edited rows from VIEW result to the dataset. - EVICT: remove a few rows from both VIEW result and linked `explorer` selection. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_viewer = Button ( label = \"View Selected\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_patcher = Button ( label = \"Update Row Values\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_evictor = Button ( label = \"Evict Rows from Selection\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. Changes dataset rows. No change to explorers. - PUSH shall be blocked until DEDUP is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. Changes dataset rows. No change to explorers. - COMMIT shall be blocked until PUSH is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. No change to dataset rows. Changes explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True # empty the selection table, then allow PATCH and EVICT self . sel_table . source . data = dict () self . sel_table . source . selected . indices = [] self . selection_patcher . disabled = False self . selection_evictor . disabled = False self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget () def view ( self ): \"\"\" ???+ note \"Defines the layout of `bokeh` objects when visualized.\" \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , # population table and directly associated widgets row ( self . update_pusher , self . data_committer , self . dedup_trigger , self . file_exporter , ), self . pop_table , # selection table and directly associated widgets row ( self . selection_viewer , self . selection_patcher , self . selection_evictor , ), self . sel_table , ) def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable pushing updated DataFrames to explorers that depend on them.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" explorer . link_dataset ( self ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" ) def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | \"\"\" explorer . link_dataset ( self ) def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) # chain another callback self . _callback_update_population () self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" ) def subscribe_selection_view ( self , explorer , subsets ): \"\"\" ???+ note \"Enable viewing groups of data entries, specified by a selection in an explorer.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subsets` | `list` | subset selections to consider | \"\"\" assert ( isinstance ( subsets , list ) and len ( subsets ) > 0 ), \"Expected a non-empty list of subsets\" explorer . link_dataset ( self ) def callback_view (): sel_slices = [] for subset in subsets : selected_idx = sorted ( explorer . sources [ subset ] . selected . indices ) sub_slice = explorer . dfs [ subset ] . iloc [ selected_idx ] sel_slices . append ( sub_slice ) selected = pd . concat ( sel_slices , axis = 0 ) self . _callback_update_selection ( selected ) def callback_evict (): # create sets for fast index discarding subset_to_indicies = {} for subset in subsets : indicies = set ( explorer . sources [ subset ] . selected . indices ) subset_to_indicies [ subset ] = indicies # from datatable index, get feature values to look up dataframe index sel_source = self . sel_table . source raw_indicies = sel_source . selected . indices for i in raw_indicies : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) subset_to_indicies [ subset ] . discard ( idx ) # assign indices back to change actual selection for subset in subsets : indicies = sorted ( list ( subset_to_indicies [ subset ])) explorer . sources [ subset ] . selected . indices = indicies self . _good ( f \"Selection table: evicted { len ( raw_indicies ) } points from selection.\" ) # refresh the selection table callback_view () self . selection_viewer . on_click ( callback_view ) self . selection_evictor . on_click ( callback_evict ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to selection table: { subsets } \" ) def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ note \"Auto-determine labels in the dataset, then create encoder/decoder in lexical order.\" Add `\"ABSTAIN\"` as a no-label placeholder which gets ignored categorically. | Param | Type | Description | | :-------- | :----- | :--------------------------------- | | `verbose` | `bool` | whether to log verbosely | | `debug` | `bool` | whether to enable label validation | \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels () def validate_labels ( self , raise_exception = True ): \"\"\" ???+ note \"Assert that every label is in the encoder.\" | Param | Type | Description | | :---------------- | :----- | :---------------------------------- | | `raise_exception` | `bool` | whether to raise errors when failed | \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : int ( x in self . label_encoder ) ) # DO NOT change the \"==\" to \"is\"; False in pandas is not False below _invalid_indices = np . where ( _mask == 0 )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ( self . dfs [ _key ] . loc [ _invalid_indices ]) if raise_exception : raise ValueError ( \"invalid labels\" ) def setup_file_export ( self ): self . file_exporter = Dropdown ( label = \"Export\" , button_type = \"warning\" , menu = [ \"Excel\" , \"CSV\" , \"JSON\" , \"pickle\" ], height_policy = \"fit\" , width_policy = \"min\" , ) def callback_export ( event , path_root = None ): \"\"\" A callback on clicking the 'self.annotator_export' button. Saves the dataframe to a pickle. \"\"\" export_format = event . item # auto-determine the export path root if path_root is None : timestamp = current_time ( \"%Y%m %d %H%M%S\" ) path_root = f \"hover-dataset-export- { timestamp } \" export_df = self . to_pandas () if export_format == \"Excel\" : export_path = f \" { path_root } .xlsx\" export_df . to_excel ( export_path , index = False ) elif export_format == \"CSV\" : export_path = f \" { path_root } .csv\" export_df . to_csv ( export_path , index = False ) elif export_format == \"JSON\" : export_path = f \" { path_root } .json\" export_df . to_json ( export_path , orient = \"records\" ) elif export_format == \"pickle\" : export_path = f \" { path_root } .pkl\" export_df . to_pickle ( export_path ) else : raise ValueError ( f \"Unexpected export format { export_format } \" ) self . _good ( f \"saved Pandas DataFrame version to { export_path } \" ) # assign the callback, keeping its reference self . _callback_export = callback_export self . file_exporter . on_click ( self . _callback_export ) def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for monitoring subset data populations.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], TableColumn ( field = \"color\" , title = \"color\" , formatter = HTMLTemplateFormatter ( template = COLOR_GLYPH_TEMPLATE ), ), ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] color_dict = auto_label_color ( self . classes ) eff_colors = [ color_dict [ _label ] for _label in eff_labels ] pop_data = dict ( color = eff_colors , label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Population updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . dedup_trigger . on_click ( update_population ) # store the callback so that it can be referenced by other methods self . _callback_update_population = update_population def setup_sel_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for viewing selected data points.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" sel_source = ColumnDataSource ( dict ()) sel_columns = dataset_default_sel_table_columns ( self . __class__ . FEATURE_KEY ) table_kwargs = dataset_default_sel_table_kwargs ( self . __class__ . FEATURE_KEY ) table_kwargs . update ( kwargs ) self . sel_table = DataTable ( source = sel_source , columns = sel_columns , ** table_kwargs ) def update_selection ( selected_df ): \"\"\" To be triggered as a subroutine of `self.selection_viewer`. \"\"\" sel_source . data = selected_df . to_dict ( orient = \"list\" ) # now that selection table has changed, clear sub-selection sel_source . selected . indices = [] self . _good ( f \"Selection table: latest selection with { selected_df . shape [ 0 ] } entries.\" ) self . _callback_update_selection = update_selection def patch_edited_selection (): sel_source = self . sel_table . source raw_indices = sel_source . selected . indices for i in raw_indices : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) for key in sel_source . data . keys (): self . dfs [ subset ] . at [ idx , key ] = sel_source . data [ key ][ i ] self . _good ( f \"Selection table: edited { len ( raw_indices ) } dataset rows.\" ) # if edited labels (which is common), then population has changed self . _callback_update_population () self . selection_patcher . on_click ( patch_edited_selection ) def df_deduplicate ( self ): \"\"\" ???+ note \"Cross-deduplicate data entries by feature between subsets.\" \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" ) self . compute_feature_index () @property def vectorizer_lookup ( self ): return self . _vectorizer_lookup @vectorizer_lookup . setter def vectorizer_lookup ( self , * args , ** kwargs ): self . _fail ( \"assigning vectorizer lookup by reference is forbidden.\" ) def compute_nd_embedding ( self , vectorizer , method , dimension = 2 , ** kwargs ): \"\"\" ???+ note \"Get embeddings in n-dimensional space and return the dimensionality reducer.\" Reference: [`DimensionalityReducer`](https://github.com/phurwicz/hover/blob/main/hover/core/representation/reduction.py) | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `dimension` | `int` | dimension of output embedding | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" from hover.core.representation.reduction import DimensionalityReducer # register the vectorizer for scenarios that may need it self . vectorizer_lookup [ dimension ] = vectorizer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" assert isinstance ( dimension , int ) and dimension >= 2 embedding_cols = [ embedding_field ( dimension , i ) for i in range ( dimension )] # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) trans_arr = np . array ( [ vectorizer ( _inp ) for _inp in tqdm ( feature_inp , desc = \"Vectorizing\" )] ) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , dimension = dimension , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] for _i in range ( dimension ): _col = embedding_cols [ _i ] self . dfs [ _key ][ _col ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), _i ] ) start_idx += _length self . _good ( f \"Computed { dimension } -d embedding in columns { embedding_cols } \" ) return reducer def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ note \"Get embeddings in the xy-plane and return the dimensionality reducer.\" A special case of `compute_nd_embedding`. | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" reducer = self . compute_nd_embedding ( vectorizer , method , dimension = 2 , ** kwargs ) return reducer def loader ( self , key , * vectorizers , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ note \"Prepare a torch `Dataloader` for training or evaluation.\" | Param | Type | Description | | :------------ | :------------ | :--------------------------------- | | `key` | `str` | subset of data, e.g. `\"train\"` | | `vectorizers` | `callable`(s) | the feature -> vector function(s) | | `batch_size` | `int` | size per batch | | `smoothing_coeff` | `float` | portion of probability to equally split between classes | \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import ( VectorDataset , MultiVectorDataset , one_hot , label_smoothing , ) # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) # prepare output vectors labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) # prepare input vectors assert len ( vectorizers ) > 0 , \"Expected at least one vectorizer\" multi_flag = len ( vectorizers ) > 1 features = df [ self . __class__ . FEATURE_KEY ] . tolist () input_vector_lists = [] for _vec_func in vectorizers : self . _info ( f \"Preparing { key } input vectors...\" ) _input_vecs = [ _vec_func ( _f ) for _f in tqdm ( features , desc = \"Vectorizing\" )] input_vector_lists . append ( _input_vecs ) self . _info ( f \"Preparing { key } data loader...\" ) if multi_flag : assert len ( input_vector_lists ) > 1 , \"Expected multiple lists of vectors\" loader = MultiVectorDataset ( input_vector_lists , output_vectors ) . loader ( batch_size = batch_size ) else : assert len ( input_vector_lists ) == 1 , \"Expected only one list of vectors\" input_vectors = input_vector_lists [ 0 ] loader = VectorDataset ( input_vectors , output_vectors ) . loader ( batch_size = batch_size ) self . _good ( f \"Prepared { key } loader with { len ( features ) } examples; { len ( vectorizers ) } vectors per feature, batch size { batch_size } \" ) return loader","title":"SupervisableDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.__init__","text":"Set up data subsets, widgets, and supplementary data structures. See self.setup_dfs for parameter details. Source code in hover/core/dataset.py def __init__ ( self , * args , ** kwargs ): \"\"\" ???+ note \"Set up data subsets, widgets, and supplementary data structures.\" See `self.setup_dfs` for parameter details. \"\"\" self . _info ( \"Initializing...\" ) self . setup_dfs ( * args , ** kwargs ) self . df_deduplicate () self . compute_feature_index () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_file_export () self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . setup_sel_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _vectorizer_lookup = OrderedDict () self . _good ( f \" { self . __class__ . __name__ } : finished initialization.\" )","title":"__init__()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_2d_embedding","text":"Get embeddings in the xy-plane and return the dimensionality reducer. A special case of compute_nd_embedding . Param Type Description vectorizer callable the feature -> vector function method str arg for DimensionalityReducer **kwargs kwargs for DimensionalityReducer Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ note \"Get embeddings in the xy-plane and return the dimensionality reducer.\" A special case of `compute_nd_embedding`. | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" reducer = self . compute_nd_embedding ( vectorizer , method , dimension = 2 , ** kwargs ) return reducer","title":"compute_2d_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_feature_index","text":"Allow lookup by feature value without setting it as the index. Assumes that feature values are unique. The reason not to just set the feature as the index is because integer indices work smoothly with Bokeh DataSource s, NumPy array s, and Torch Tensor s. Source code in hover/core/dataset.py def compute_feature_index ( self ): \"\"\" ???+ note \"Allow lookup by feature value without setting it as the index.\" Assumes that feature values are unique. The reason not to just set the feature as the index is because integer indices work smoothly with Bokeh `DataSource`s, NumPy `array`s, and Torch `Tensor`s. \"\"\" feature_to_subset_idx = {} for _subset , _df in self . dfs . items (): _values = _df [ self . __class__ . FEATURE_KEY ] . values for i , _val in enumerate ( _values ): if _val in feature_to_subset_idx : raise ValueError ( f \"Expected unique feature values, found duplicate { _val } \" ) feature_to_subset_idx [ _val ] = ( _subset , i ) self . feature_to_subset_idx = feature_to_subset_idx","title":"compute_feature_index()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_nd_embedding","text":"Get embeddings in n-dimensional space and return the dimensionality reducer. Reference: DimensionalityReducer Param Type Description vectorizer callable the feature -> vector function method str arg for DimensionalityReducer dimension int dimension of output embedding **kwargs kwargs for DimensionalityReducer Source code in hover/core/dataset.py def compute_nd_embedding ( self , vectorizer , method , dimension = 2 , ** kwargs ): \"\"\" ???+ note \"Get embeddings in n-dimensional space and return the dimensionality reducer.\" Reference: [`DimensionalityReducer`](https://github.com/phurwicz/hover/blob/main/hover/core/representation/reduction.py) | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `dimension` | `int` | dimension of output embedding | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" from hover.core.representation.reduction import DimensionalityReducer # register the vectorizer for scenarios that may need it self . vectorizer_lookup [ dimension ] = vectorizer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" assert isinstance ( dimension , int ) and dimension >= 2 embedding_cols = [ embedding_field ( dimension , i ) for i in range ( dimension )] # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp . extend ( self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist ()) trans_arr = np . array ( [ vectorizer ( _inp ) for _inp in tqdm ( feature_inp , desc = \"Vectorizing\" )] ) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , dimension = dimension , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] for _i in range ( dimension ): _col = embedding_cols [ _i ] self . dfs [ _key ][ _col ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), _i ] ) start_idx += _length self . _good ( f \"Computed { dimension } -d embedding in columns { embedding_cols } \" ) return reducer","title":"compute_nd_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.copy","text":"Create another instance, copying over the data entries. Also copy data structures that don't get created in the new instance. Source code in hover/core/dataset.py def copy ( self ): \"\"\" ???+ note \"Create another instance, copying over the data entries.\" Also copy data structures that don't get created in the new instance. \"\"\" dataset = self . __class__ . from_pandas ( self . to_pandas ()) dataset . _vectorizer_lookup . update ( self . _vectorizer_lookup ) return dataset","title":"copy()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.df_deduplicate","text":"Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" ???+ note \"Cross-deduplicate data entries by feature between subsets.\" \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" ) self . compute_feature_index ()","title":"df_deduplicate()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.from_pandas","text":"Import from a pandas DataFrame. Param Type Description df DataFrame with a \"SUBSET\" field dividing subsets Source code in hover/core/dataset.py @classmethod def from_pandas ( cls , df , ** kwargs ): \"\"\" ???+ note \"Import from a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `df` | `DataFrame` | with a \"SUBSET\" field dividing subsets | \"\"\" SUBSETS = cls . SCRATCH_SUBSETS + cls . PUBLIC_SUBSETS + cls . PRIVATE_SUBSETS if DATASET_SUBSET_FIELD not in df . columns : raise ValueError ( f \"Expecting column ' { DATASET_SUBSET_FIELD } ' in the DataFrame which takes values from { SUBSETS } \" ) dictls = {} for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _sub_df = df [ df [ DATASET_SUBSET_FIELD ] == _subset ] dictls [ _subset ] = _sub_df . to_dict ( orient = \"records\" ) return cls ( raw_dictl = dictls [ \"raw\" ], train_dictl = dictls [ \"train\" ], dev_dictl = dictls [ \"dev\" ], test_dictl = dictls [ \"test\" ], ** kwargs , )","title":"from_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.loader","text":"Prepare a torch Dataloader for training or evaluation. Param Type Description key str subset of data, e.g. \"train\" vectorizers callable (s) the feature -> vector function(s) batch_size int size per batch smoothing_coeff float portion of probability to equally split between classes Source code in hover/core/dataset.py def loader ( self , key , * vectorizers , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ note \"Prepare a torch `Dataloader` for training or evaluation.\" | Param | Type | Description | | :------------ | :------------ | :--------------------------------- | | `key` | `str` | subset of data, e.g. `\"train\"` | | `vectorizers` | `callable`(s) | the feature -> vector function(s) | | `batch_size` | `int` | size per batch | | `smoothing_coeff` | `float` | portion of probability to equally split between classes | \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import ( VectorDataset , MultiVectorDataset , one_hot , label_smoothing , ) # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) # prepare output vectors labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) # prepare input vectors assert len ( vectorizers ) > 0 , \"Expected at least one vectorizer\" multi_flag = len ( vectorizers ) > 1 features = df [ self . __class__ . FEATURE_KEY ] . tolist () input_vector_lists = [] for _vec_func in vectorizers : self . _info ( f \"Preparing { key } input vectors...\" ) _input_vecs = [ _vec_func ( _f ) for _f in tqdm ( features , desc = \"Vectorizing\" )] input_vector_lists . append ( _input_vecs ) self . _info ( f \"Preparing { key } data loader...\" ) if multi_flag : assert len ( input_vector_lists ) > 1 , \"Expected multiple lists of vectors\" loader = MultiVectorDataset ( input_vector_lists , output_vectors ) . loader ( batch_size = batch_size ) else : assert len ( input_vector_lists ) == 1 , \"Expected only one list of vectors\" input_vectors = input_vector_lists [ 0 ] loader = VectorDataset ( input_vectors , output_vectors ) . loader ( batch_size = batch_size ) self . _good ( f \"Prepared { key } loader with { len ( features ) } examples; { len ( vectorizers ) } vectors per feature, batch size { batch_size } \" ) return loader","title":"loader()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.locate_by_feature_value","text":"Find the subset and index given a feature value. Assumes that the value is present and detects if the subset and index found is consistent with the value. Source code in hover/core/dataset.py def locate_by_feature_value ( self , value , auto_recompute = True ): \"\"\" ???+ note \"Find the subset and index given a feature value.\" Assumes that the value is present and detects if the subset and index found is consistent with the value. \"\"\" subset , index = self . feature_to_subset_idx [ value ] current_value = self . dfs [ subset ] . at [ index , self . __class__ . FEATURE_KEY ] if current_value != value : if auto_recompute : self . _warn ( \"locate_by_feature_value mismatch. Recomputing index.\" ) self . compute_feature_index () # if ever need to recompute twice, there must be a bug return self . locate_by_feature_value ( value , auto_recompute = False ) else : raise ValueError ( \"locate_by_feature_value mismatch.\" ) return subset , index","title":"locate_by_feature_value()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_dfs","text":"Subroutine of the constructor that creates standard-format DataFrames. Param Type Description raw_dictl list list of dicts holding the to-be-supervised raw data train_dictl list list of dicts holding any supervised train data dev_dictl list list of dicts holding any supervised dev data test_dictl list list of dicts holding any supervised test data feature_key str the key for the feature in each piece of data label_key str the key for the **str** label in supervised data Source code in hover/core/dataset.py def setup_dfs ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ note \"Subroutine of the constructor that creates standard-format DataFrames.\" | Param | Type | Description | | :------------ | :----- | :----------------------------------- | | `raw_dictl` | `list` | list of dicts holding the **to-be-supervised** raw data | | `train_dictl` | `list` | list of dicts holding any **supervised** train data | | `dev_dictl` | `list` | list of dicts holding any **supervised** dev data | | `test_dictl` | `list` | list of dicts holding any **supervised** test data | | `feature_key` | `str` | the key for the feature in each piece of data | | `label_key` | `str` | the key for the `**str**` label in supervised data | \"\"\" def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] # standardize records dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } # initialize dataframes self . dfs = dict () for _key , _dictl in dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df","title":"setup_dfs()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_label_coding","text":"Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add \"ABSTAIN\" as a no-label placeholder which gets ignored categorically. Param Type Description verbose bool whether to log verbosely debug bool whether to enable label validation Source code in hover/core/dataset.py def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ note \"Auto-determine labels in the dataset, then create encoder/decoder in lexical order.\" Add `\"ABSTAIN\"` as a no-label placeholder which gets ignored categorically. | Param | Type | Description | | :-------- | :----- | :--------------------------------- | | `verbose` | `bool` | whether to log verbosely | | `debug` | `bool` | whether to enable label validation | \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels ()","title":"setup_label_coding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_pop_table","text":"Set up a bokeh DataTable widget for monitoring subset data populations. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for monitoring subset data populations.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], TableColumn ( field = \"color\" , title = \"color\" , formatter = HTMLTemplateFormatter ( template = COLOR_GLYPH_TEMPLATE ), ), ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] color_dict = auto_label_color ( self . classes ) eff_colors = [ color_dict [ _label ] for _label in eff_labels ] pop_data = dict ( color = eff_colors , label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Population updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . dedup_trigger . on_click ( update_population ) # store the callback so that it can be referenced by other methods self . _callback_update_population = update_population","title":"setup_pop_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_sel_table","text":"Set up a bokeh DataTable widget for viewing selected data points. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_sel_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for viewing selected data points.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" sel_source = ColumnDataSource ( dict ()) sel_columns = dataset_default_sel_table_columns ( self . __class__ . FEATURE_KEY ) table_kwargs = dataset_default_sel_table_kwargs ( self . __class__ . FEATURE_KEY ) table_kwargs . update ( kwargs ) self . sel_table = DataTable ( source = sel_source , columns = sel_columns , ** table_kwargs ) def update_selection ( selected_df ): \"\"\" To be triggered as a subroutine of `self.selection_viewer`. \"\"\" sel_source . data = selected_df . to_dict ( orient = \"list\" ) # now that selection table has changed, clear sub-selection sel_source . selected . indices = [] self . _good ( f \"Selection table: latest selection with { selected_df . shape [ 0 ] } entries.\" ) self . _callback_update_selection = update_selection def patch_edited_selection (): sel_source = self . sel_table . source raw_indices = sel_source . selected . indices for i in raw_indices : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) for key in sel_source . data . keys (): self . dfs [ subset ] . at [ idx , key ] = sel_source . data [ key ][ i ] self . _good ( f \"Selection table: edited { len ( raw_indices ) } dataset rows.\" ) # if edited labels (which is common), then population has changed self . _callback_update_population () self . selection_patcher . on_click ( patch_edited_selection )","title":"setup_sel_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_widgets","text":"Create bokeh widgets for interactive data management. Operations: - PUSH: push updated dataframes to linked explorer s. - COMMIT: added selected points to a specific subset dataframe . - DEDUP: cross-deduplicate across all subset dataframe s. - VIEW: view selected points of linked explorer s. - the link can be different from that for PUSH. Typically all the explorer s sync their selections, and only an annotator is linked to the dataset . - PATCH: update a few edited rows from VIEW result to the dataset. - EVICT: remove a few rows from both VIEW result and linked explorer selection. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" ???+ note \"Create `bokeh` widgets for interactive data management.\" Operations: - PUSH: push updated dataframes to linked `explorer`s. - COMMIT: added selected points to a specific subset `dataframe`. - DEDUP: cross-deduplicate across all subset `dataframe`s. - VIEW: view selected points of linked `explorer`s. - the link can be different from that for PUSH. Typically all the `explorer`s sync their selections, and only an `annotator` is linked to the `dataset`. - PATCH: update a few edited rows from VIEW result to the dataset. - EVICT: remove a few rows from both VIEW result and linked `explorer` selection. \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_viewer = Button ( label = \"View Selected\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_patcher = Button ( label = \"Update Row Values\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_evictor = Button ( label = \"Evict Rows from Selection\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. Changes dataset rows. No change to explorers. - PUSH shall be blocked until DEDUP is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. Changes dataset rows. No change to explorers. - COMMIT shall be blocked until PUSH is executed. - PATCH shall be blocked until PUSH is executed. - EVICT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . selection_patcher . disabled = True self . selection_evictor . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. No change to dataset rows. Changes explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True # empty the selection table, then allow PATCH and EVICT self . sel_table . source . data = dict () self . sel_table . source . selected . indices = [] self . selection_patcher . disabled = False self . selection_evictor . disabled = False self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget ()","title":"setup_widgets()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_data_commit","text":"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | \"\"\" explorer . link_dataset ( self ) def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) # chain another callback self . _callback_update_population () self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" )","title":"subscribe_data_commit()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_selection_view","text":"Enable viewing groups of data entries, specified by a selection in an explorer. Param Type Description explorer BokehBaseExplorer the explorer to register subsets list subset selections to consider Source code in hover/core/dataset.py def subscribe_selection_view ( self , explorer , subsets ): \"\"\" ???+ note \"Enable viewing groups of data entries, specified by a selection in an explorer.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subsets` | `list` | subset selections to consider | \"\"\" assert ( isinstance ( subsets , list ) and len ( subsets ) > 0 ), \"Expected a non-empty list of subsets\" explorer . link_dataset ( self ) def callback_view (): sel_slices = [] for subset in subsets : selected_idx = sorted ( explorer . sources [ subset ] . selected . indices ) sub_slice = explorer . dfs [ subset ] . iloc [ selected_idx ] sel_slices . append ( sub_slice ) selected = pd . concat ( sel_slices , axis = 0 ) self . _callback_update_selection ( selected ) def callback_evict (): # create sets for fast index discarding subset_to_indicies = {} for subset in subsets : indicies = set ( explorer . sources [ subset ] . selected . indices ) subset_to_indicies [ subset ] = indicies # from datatable index, get feature values to look up dataframe index sel_source = self . sel_table . source raw_indicies = sel_source . selected . indices for i in raw_indicies : feature_value = sel_source . data [ self . __class__ . FEATURE_KEY ][ i ] subset , idx = self . locate_by_feature_value ( feature_value ) subset_to_indicies [ subset ] . discard ( idx ) # assign indices back to change actual selection for subset in subsets : indicies = sorted ( list ( subset_to_indicies [ subset ])) explorer . sources [ subset ] . selected . indices = indicies self . _good ( f \"Selection table: evicted { len ( raw_indicies ) } points from selection.\" ) # refresh the selection table callback_view () self . selection_viewer . on_click ( callback_view ) self . selection_evictor . on_click ( callback_evict ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to selection table: { subsets } \" )","title":"subscribe_selection_view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_update_push","text":"Enable pushing updated DataFrames to explorers that depend on them. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable pushing updated DataFrames to explorers that depend on them.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" explorer . link_dataset ( self ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" )","title":"subscribe_update_push()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.to_pandas","text":"Export to a pandas DataFrame. Source code in hover/core/dataset.py def to_pandas ( self ): \"\"\" ???+ note \"Export to a pandas DataFrame.\" \"\"\" dfs = [] for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _df = self . dfs [ _subset ] . copy () _df [ DATASET_SUBSET_FIELD ] = _subset dfs . append ( _df ) return pd . concat ( dfs , axis = 0 )","title":"to_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.validate_labels","text":"Assert that every label is in the encoder. Param Type Description raise_exception bool whether to raise errors when failed Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" ???+ note \"Assert that every label is in the encoder.\" | Param | Type | Description | | :---------------- | :----- | :---------------------------------- | | `raise_exception` | `bool` | whether to raise errors when failed | \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : int ( x in self . label_encoder ) ) # DO NOT change the \"==\" to \"is\"; False in pandas is not False below _invalid_indices = np . where ( _mask == 0 )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ( self . dfs [ _key ] . loc [ _invalid_indices ]) if raise_exception : raise ValueError ( \"invalid labels\" )","title":"validate_labels()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.view","text":"Defines the layout of bokeh objects when visualized. Source code in hover/core/dataset.py def view ( self ): \"\"\" ???+ note \"Defines the layout of `bokeh` objects when visualized.\" \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , # population table and directly associated widgets row ( self . update_pusher , self . data_committer , self . dedup_trigger , self . file_exporter , ), self . pop_table , # selection table and directly associated widgets row ( self . selection_viewer , self . selection_patcher , self . selection_evictor , ), self . sel_table , )","title":"view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableImageDataset","text":"SupervisableDataset whose primary feature is image . Source code in hover/core/dataset.py class SupervisableImageDataset ( SupervisableDataset ): \"\"\" ???+ note \"`SupervisableDataset` whose primary feature is `image`.\" \"\"\" FEATURE_KEY = \"image\"","title":"SupervisableImageDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableTextDataset","text":"SupervisableDataset whose primary feature is text . Source code in hover/core/dataset.py class SupervisableTextDataset ( SupervisableDataset ): \"\"\" ???+ note \"`SupervisableDataset` whose primary feature is `text`.\" \"\"\" FEATURE_KEY = \"text\"","title":"SupervisableTextDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset","text":"","title":"hover.core.dataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableAudioDataset","text":"","title":"SupervisableAudioDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset","text":"","title":"SupervisableDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_2d_embedding","text":"","title":"compute_2d_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_feature_index","text":"","title":"compute_feature_index()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_nd_embedding","text":"","title":"compute_nd_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.copy","text":"","title":"copy()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.df_deduplicate","text":"","title":"df_deduplicate()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.from_pandas","text":"","title":"from_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.loader","text":"","title":"loader()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.locate_by_feature_value","text":"","title":"locate_by_feature_value()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_dfs","text":"","title":"setup_dfs()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_label_coding","text":"","title":"setup_label_coding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_pop_table","text":"","title":"setup_pop_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_sel_table","text":"","title":"setup_sel_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_widgets","text":"","title":"setup_widgets()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_data_commit","text":"","title":"subscribe_data_commit()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_selection_view","text":"","title":"subscribe_selection_view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_update_push","text":"","title":"subscribe_update_push()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.to_pandas","text":"","title":"to_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.validate_labels","text":"","title":"validate_labels()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.view","text":"","title":"view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableImageDataset","text":"","title":"SupervisableImageDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableTextDataset","text":"","title":"SupervisableTextDataset"},{"location":"pages/reference/core-explorer-base/","text":"Base class(es) for ALL explorer implementations. BokehBaseExplorer ( Loggable , ABC ) Base class for visually exploring data with Bokeh . Assumes: in supplied dataframes (always) xy coordinates in x and y columns; (always) an index for the rows; (always) classification label (or ABSTAIN) in a label column. Does not assume: a specific form of data; what the map serves to do. Source code in hover/core/explorer/base.py class BokehBaseExplorer ( Loggable , ABC , metaclass = RichTracebackABCMeta ): \"\"\" ???+ note \"Base class for visually exploring data with `Bokeh`.\" Assumes: - in supplied dataframes - (always) xy coordinates in `x` and `y` columns; - (always) an index for the rows; - (always) classification label (or ABSTAIN) in a `label` column. Does not assume: - a specific form of data; - what the map serves to do. \"\"\" SUBSET_GLYPH_KWARGS = {} DEFAULT_SUBSET_MAPPING = { _k : _k for _k in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]} PRIMARY_FEATURE = None MANDATORY_COLUMNS = [ \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"coords\" : True , \"index\" : True } def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Constructor shared by all child classes.\" | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | 1. settle the figure settings by using child class defaults & kwargs overrides 2. settle the glyph settings by using child class defaults 3. set up dataframes to sync with 4. create widgets that child classes can override 5. create data sources the correspond to class-specific data subsets. 6. initialize a figure under the settings above \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip ( kwargs . pop ( \"tooltips\" , \"\" )), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . figure = figure ( ** self . figure_kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_dfs ( df_dict ) self . _setup_sources () self . _setup_widgets () @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" ???+ note \"Alternative constructor from a `SupervisableDataset`.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `dataset` | `SupervisableDataset` | dataset with `DataFrame`s | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | | `*args` | | forwarded to the constructor | | `**kwargs` | | forwarded to the constructor | \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer = cls ( df_dict , * args , ** kwargs ) explorer . link_dataset ( dataset ) return explorer def link_dataset ( self , dataset ): \"\"\" ???+ note \"Get tied to a dataset, which is common for explorers.\" \"\"\" if not hasattr ( self , \"linked_dataset\" ): self . linked_dataset = dataset else : assert self . linked_dataset is dataset , \"cannot link to two datasets\" def view ( self ): \"\"\" ???+ note \"Define the high-level visual layout of the whole explorer.\" \"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure ) def _build_tooltip ( self , extra ): \"\"\" ???+ note \"Define a windowed tooltip which shows inspection details.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `extra` | `str` | user-supplied extra HTML | Note that this is a method rather than a class attribute because child classes may involve instance attributes in the tooltip. \"\"\" standard = bokeh_hover_tooltip ( ** self . __class__ . TOOLTIP_KWARGS ) return f \" { standard } \\n { extra } \" def _setup_widgets ( self ): \"\"\" ???+ note \"High-level function creating widgets for interactive functionality.\" \"\"\" self . _info ( \"Setting up widgets\" ) self . _dynamic_widgets = OrderedDict () self . _dynamic_callbacks = OrderedDict () self . _dynamic_resources = OrderedDict () self . _setup_search_widgets () self . _setup_selection_option () self . _setup_subset_toggle () self . _setup_axes_dropdown () @abstractmethod def _layout_widgets ( self ): \"\"\" ???+ note \"Define the low-level layout of widgets.\" \"\"\" pass @abstractmethod def _setup_search_widgets ( self ): \"\"\" ???+ note \"Define how to search data points.\" Left to child classes that have a specific feature format. \"\"\" pass def _setup_selection_option ( self ): \"\"\" ???+ note \"Create a group of checkbox(es) for advanced selection options.\" \"\"\" from bokeh.models import RadioGroup self . selection_option_box = RadioGroup ( labels = [ \"keep selecting: none\" , \"union\" , \"intersection\" , \"difference\" ], active = 0 , ) def _setup_subset_toggle ( self ): \"\"\" ???+ note \"Create a group of buttons for toggling which data subsets to show.\" \"\"\" from bokeh.models import CheckboxButtonGroup , Div from bokeh.layouts import column data_keys = list ( self . __class__ . SUBSET_GLYPH_KWARGS . keys ()) self . data_key_button_group = CheckboxButtonGroup ( labels = data_keys , active = list ( range ( len ( data_keys ))) ) self . data_key_button_group_help = Div ( text = \"Toggle data subset display\" ) self . subset_toggle_widget_column = column ( self . data_key_button_group_help , self . data_key_button_group ) def update_data_key_display ( active ): visible_keys = { self . data_key_button_group . labels [ idx ] for idx in active } for _renderer in self . figure . renderers : # if the renderer has a name \"on the list\", update its visibility if _renderer . name in self . __class__ . SUBSET_GLYPH_KWARGS . keys (): _renderer . visible = _renderer . name in visible_keys # store the callback (useful, for example, during automated tests) and link it self . _callback_subset_display = lambda : update_data_key_display ( self . data_key_button_group . active ) self . data_key_button_group . on_click ( update_data_key_display ) def _setup_axes_dropdown ( self ): \"\"\" ???+ note \"Find embedding fields and allow any of them to be set as the x or y axis.\" \"\"\" from bokeh.models import Dropdown embed_cols = self . find_embedding_fields () init_x , init_y = embed_cols [: 2 ] self . dropdown_x_axis = Dropdown ( label = f \"X coord: { init_x } \" , menu = embed_cols ) self . dropdown_y_axis = Dropdown ( label = f \"Y coord: { init_y } \" , menu = embed_cols ) def change_x ( event ): self . dropdown_x_axis . label = f \"X coord: { event . item } \" for _renderer in self . figure . renderers : _renderer . glyph . x = event . item def change_y ( event ): self . dropdown_y_axis . label = f \"Y coord: { event . item } \" for _renderer in self . figure . renderers : _renderer . glyph . y = event . item self . dropdown_x_axis . on_click ( change_x ) self . dropdown_y_axis . on_click ( change_y ) # consider allowing dynamic menu refreshment # def refresh_axes_list(): # embed_cols = self.find_embedding_fields() # self.dropdown_x_axis.menu = embed_cols[:] # self.dropdown_y_axis.menu = embed_cols[:] def value_patch_by_slider ( self , col_original , col_patch , ** kwargs ): \"\"\" ???+ note \"Allow source values to be dynamically patched through a slider.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `col_original` | `str` | column of values before the patch | | `col_patch` | `str` | column of list of values to use as patches | | `**kwargs` | | forwarded to the slider | [Reference](https://github.com/bokeh/bokeh/blob/2.4.2/examples/howto/patch_app.py) \"\"\" # add a patch slider to widgets, if none exist if \"patch_slider\" not in self . _dynamic_widgets : slider = Slider ( start = 0 , end = 1 , value = 0 , step = 1 , ** kwargs ) slider . disabled = True self . _dynamic_widgets [ \"patch_slider\" ] = slider else : slider = self . _dynamic_widgets [ \"patch_slider\" ] # create a slider-adjusting callback exposed to the outside def adjust_slider (): \"\"\" Infer slider length from the number of patch values. \"\"\" num_patches = None for _key , _df in self . dfs . items (): assert ( col_patch in _df . columns ), f \"Subset { _key } expecting column { col_patch } among columns, got { _df . columns } \" # find all array lengths; note that the data subset can be empty _num_patches_seen = _df [ col_patch ] . apply ( len ) . values assert ( len ( set ( _num_patches_seen )) <= 1 ), f \"Expecting consistent number of patches, got { _num_patches_seen } \" _num_patches = _num_patches_seen [ 0 ] if _df . shape [ 0 ] > 0 else None # if a previous subset has implied the number of patches, run a consistency check if num_patches is None : num_patches = _num_patches else : assert ( num_patches == _num_patches ), f \"Conflicting number of patches: { num_patches } vs { _num_patches } \" assert num_patches >= 2 , f \"Expecting at least 2 patches, got { num_patches } \" slider . end = num_patches - 1 slider . disabled = False self . _dynamic_callbacks [ \"adjust_patch_slider\" ] = adjust_slider # create the callback for patching values def update_patch ( attr , old , new ): for _key , _df in self . dfs . items (): # calculate the patch corresponding to slider value _value = [ _arr [ new ] for _arr in _df [ col_patch ] . values ] _slice = slice ( _df . shape [ 0 ]) _patch = { col_original : [( _slice , _value )]} self . sources [ _key ] . patch ( _patch ) slider . on_change ( \"value\" , update_patch ) self . _good ( f \"Patching { col_original } using { col_patch } \" ) def _mandatory_column_defaults ( self ): \"\"\" ???+ note \"Mandatory columns and default values.\" If default value is None, will raise exception if the column is not found. \"\"\" return { _col : None for _col in self . __class__ . MANDATORY_COLUMNS } def _setup_dfs ( self , df_dict , copy = False ): \"\"\" ???+ note \"Check and store DataFrames **by reference by default**.\" Intended to be extended in child classes for pre/post processing. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `copy` | `bool` | whether to copy `DataFrame`s | \"\"\" self . _info ( \"Setting up DataFrames\" ) supplied_keys = set ( df_dict . keys ()) expected_keys = set ( self . __class__ . SUBSET_GLYPH_KWARGS . keys ()) # perform high-level df key checks expected_and_supplied = supplied_keys . intersection ( expected_keys ) supplied_not_expected = supplied_keys . difference ( expected_keys ) expected_not_supplied = expected_keys . difference ( supplied_keys ) for _key in supplied_not_expected : self . _warn ( f \" { self . __class__ . __name__ } .__init__(): got unexpected df key { _key } \" ) for _key in expected_not_supplied : self . _warn ( f \" { self . __class__ . __name__ } .__init__(): missing expected df key { _key } \" ) # assign df with column checks self . dfs = dict () mandatory_col_to_default = self . _mandatory_column_defaults () for _key in expected_and_supplied : _df = df_dict [ _key ] for _col , _default in mandatory_col_to_default . items (): # column exists: all good if _col in _df . columns : continue # no default value: column must be explicitly provided if _default is None : # edge case: DataFrame has zero rows _msg = f \"Expecting column ' { _col } ' from { _key } df: found { list ( _df . columns ) } \" assert _df . shape [ 0 ] == 0 , _msg # default value available, will use it to create column else : _df [ _col ] = _default self . dfs [ _key ] = _df . copy () if copy else _df # expected dfs must be present for _key in expected_not_supplied : _df = pd . DataFrame ( columns = list ( mandatory_col_to_default . keys ())) self . dfs [ _key ] = _df def _setup_sources ( self ): \"\"\" ???+ note \"Create, **(not update)**, `ColumnDataSource` objects.\" Intended to be extended in child classes for pre/post processing. \"\"\" self . _info ( \"Setting up sources\" ) self . sources = { _key : ColumnDataSource ( _df ) for _key , _df in self . dfs . items ()} self . _postprocess_sources () # initialize attributes that couple with sources # extra columns for dynamic plotting self . _extra_source_cols = defaultdict ( dict ) self . _setup_selection_tools () def _setup_subroutine_selection_store ( self ): \"\"\" ???+ note \"Subroutine of `_setup_selection_tools`.\" Setup callbacks that interact with manual selections. \"\"\" def store_selection (): \"\"\" Keep track of the last manual selection. Useful for applying cumulation / filters dynamically. \"\"\" # determine selection mode selection_option_code = self . selection_option_box . active for _key , _source in self . sources . items (): _selected = _source . selected . indices # use sets' in-place methods instead of assignment if selection_option_code == 1 : self . _last_selections [ _key ] . data . update ( _selected ) elif selection_option_code == 2 : self . _last_selections [ _key ] . data . intersection_update ( _selected ) elif selection_option_code == 3 : self . _last_selections [ _key ] . data . difference_update ( _selected ) else : assert selection_option_code == 0 self . _last_selections [ _key ] . data . clear () self . _last_selections [ _key ] . data . update ( _selected ) _source . selected . indices = list ( self . _last_selections [ _key ] . data ) self . _store_selection = store_selection self . figure . on_event ( SelectionGeometry , lambda event : self . _store_selection () if event . final else None , ) def _setup_subroutine_selection_filter ( self ): \"\"\" ???+ note \"Subroutine of `_setup_selection_tools`.\" Setup callbacks that interact with selection filters. \"\"\" def trigger_selection_filters ( subsets = None ): \"\"\" Filter selection indices on specified subsets. \"\"\" if subsets is None : subsets = self . sources . keys () else : assert set ( subsets ) . issubset ( self . sources . keys () ), f \"Expected subsets from { self . sources . keys () } \" for _key in subsets : _selected = self . _last_selections [ _key ] . data for _func in self . _selection_filters [ _key ] . data : _selected = _func ( _selected , _key ) self . sources [ _key ] . selected . indices = list ( _selected ) # keep reference to trigger_selection_filter() for further access # for example, toggling filters should call the trigger self . _trigger_selection_filters = trigger_selection_filters self . figure . on_event ( SelectionGeometry , lambda event : self . _trigger_selection_filters () if event . final else None , ) def _setup_subroutine_selection_reset ( self ): \"\"\" ???+ note \"Subroutine of `_setup_selection_tools`.\" Setup callbacks for scenarios where the selection should be reset. For example, when the plot sources have changed. \"\"\" def reset_selection (): \"\"\" Clear last manual selections and source selections. Useful during post-processing of refreshed data source. Not directly defined as a method because of `self._last_selections`. \"\"\" for _key , _source in self . sources . items (): self . _last_selections [ _key ] . data . clear () _source . selected . indices = [] self . _reset_selection = reset_selection def _setup_selection_tools ( self ): \"\"\" ???+ note \"Create data structures and callbacks for dynamic selections.\" Useful for linking and filtering selections across explorers. \"\"\" # store the last manual selections self . _last_selections = { _key : RootUnionFind ( set ()) for _key in self . sources . keys () } # store commutative, idempotent index filters self . _selection_filters = { _key : RootUnionFind ( set ()) for _key in self . sources . keys () } self . _setup_subroutine_selection_store () self . _setup_subroutine_selection_filter () self . _setup_subroutine_selection_reset () def _update_sources ( self ): \"\"\" ???+ note \"Update the sources with the corresponding dfs.\" Note that the shapes and fields of sources are overriden. Thus supplementary fields (those that do not exist in the dfs), such as dynamic plotting kwargs, need to be re-assigned. \"\"\" for _key in self . dfs . keys (): self . sources [ _key ] . data = self . dfs [ _key ] self . _postprocess_sources () # reset selections now that source indices may have changed self . _reset_selection () # reset attribute values that couple with sources for _key in self . sources . keys (): _num_points = len ( self . sources [ _key ] . data [ \"label\" ]) # add extra columns for _col , _fill_value in self . _extra_source_cols [ _key ] . items (): self . sources [ _key ] . add ([ _fill_value ] * _num_points , _col ) # clear last selection but keep the set object self . _last_selections [ _key ] . data . clear () # DON'T DO: self._last_selections = {_key: set() for _key in self.sources.keys()} def _postprocess_sources ( self ): \"\"\" ???+ note \"Infer source attributes from the dfs, without altering the dfs.\" Useful for assigning dynamic glyph attributes, similarly to `activate_search()`. \"\"\" pass def activate_search ( self ): \"\"\" ???+ note \"Assign search response callbacks to search results. Child methods should bind those callbacks to search widgets.\" This is a parent method which takes care of common denominators of parent methods. Child methods may inherit the logic here and preprocess/postprocess as needed. \"\"\" self . _subroutine_search_create_callbacks () self . _subroutine_search_activate_callbacks () def _subroutine_search_create_callbacks ( self ): \"\"\" ???+ note \"Create search callback functions based on feature attributes.\" \"\"\" # allow dynamically updated search response through dict element retrieval self . _dynamic_callbacks [ \"search_response\" ] = dict () def search_base_response ( attr , old , new ): for _subset in self . sources . keys (): _func = self . _dynamic_callbacks [ \"search_response\" ] . get ( _subset , blank ) _func ( attr , old , new ) return self . search_base_response = search_base_response for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items (): # create a field that holds search results that could be used elsewhere _num_points = len ( self . sources [ _key ] . data [ self . __class__ . PRIMARY_FEATURE ]) self . _extra_source_cols [ _key ][ SEARCH_SCORE_FIELD ] = 0 self . sources [ _key ] . add ([ 0 ] * _num_points , SEARCH_SCORE_FIELD ) # make attributes respond to search for _ , _params in _dict [ \"search\" ] . items (): _updated_kwargs = self . _subroutine_search_source_change ( _key , self . glyph_kwargs [ _key ], altered_param = _params , ) self . glyph_kwargs [ _key ] . clear () self . glyph_kwargs [ _key ] . update ( _updated_kwargs ) def _subroutine_search_activate_callbacks ( self ): \"\"\" ???+ note \"Activate search callback functions by binding them to widgets.\" \"\"\" for _widget in self . _search_watch_widgets (): _widget . on_change ( \"value\" , self . search_base_response ) self . _info ( f \"activated search base response on { _widget } \" ) @abstractmethod def _search_watch_widgets ( self ): \"\"\" ???+ note \"Widgets to trigger search callbacks automatically, which can be different across subclasses.\" Intended for binding callback functions to widgets. \"\"\" @abstractmethod def _validate_search_input ( self ): \"\"\" ???+ note \"Check the search input, skipping callbacks if it's invalid.\" \"\"\" pass @abstractmethod def _get_search_score_function ( self ): \"\"\" ???+ note \"Dynamically create a single-argument scoring function.\" \"\"\" pass def _subroutine_search_source_change ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 ) ): \"\"\" ???+ note \"Subroutine of `activate_search()` on a specific subset.\" Modifies the plotting source in-place. | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () feature_key = self . __class__ . PRIMARY_FEATURE param_key , param_pos , param_neg , param_default = altered_param initial_num = len ( self . sources [ subset ] . data [ feature_key ]) self . sources [ subset ] . add ([ param_default ] * initial_num , param_key ) self . _extra_source_cols [ subset ][ param_key ] = param_default updated_kwargs [ param_key ] = param_key def score_to_param ( score ): if score > 0 : return param_pos elif score == 0 : return param_default else : return param_neg def search_response ( attr , old , new ): valid_flag = self . _validate_search_input () if not valid_flag : return score_func = self . _get_search_score_function () patch_slice = slice ( len ( self . sources [ subset ] . data [ feature_key ])) features = self . sources [ subset ] . data [ self . __class__ . PRIMARY_FEATURE ] # score_func is potentially slow; track its progress # search_scores = list(map(score_func, tqdm(features, desc=\"Search score\"))) search_scores = list ( map ( score_func , features )) search_params = list ( map ( score_to_param , search_scores )) self . sources [ subset ] . patch ( { SEARCH_SCORE_FIELD : [( patch_slice , search_scores )]} ) self . sources [ subset ] . patch ({ param_key : [( patch_slice , search_params )]}) return # assign dynamic callback self . _dynamic_callbacks [ \"search_response\" ][ subset ] = search_response return updated_kwargs def _prelink_check ( self , other ): \"\"\" ???+ note \"Sanity check before linking two explorers.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" assert other is not self , \"Self-loops are fordidden\" assert isinstance ( other , BokehBaseExplorer ), \"Must link to BokehBaseExplorer\" def link_selection ( self , key , other , other_key ): \"\"\" ???+ note \"Synchronize the selected indices between specified sources.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `key` | `str` | the key of the subset to link | | `other` | `BokehBaseExplorer` | the other explorer | | `other_key` | `str` | the key of the other subset | \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] def left_to_right ( attr , old , new ): sr . selected . indices = sl . selected . indices [:] def right_to_left ( attr , old , new ): sl . selected . indices = sr . selected . indices [:] sl . selected . on_change ( \"indices\" , left_to_right ) sr . selected . on_change ( \"indices\" , right_to_left ) # link last manual selections (pointing to the same set) self . _last_selections [ key ] . union ( other . _last_selections [ other_key ]) # link selection filter functions (pointing to the same set) self . _selection_filters [ key ] . data . update ( other . _selection_filters [ other_key ] . data ) self . _selection_filters [ key ] . union ( other . _selection_filters [ other_key ]) def link_selection_options ( self , other ): \"\"\" ???+ note \"Synchronize the selection option values between explorers.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" def left_to_right ( attr , old , new ): other . selection_option_box . active = self . selection_option_box . active def right_to_left ( attr , old , new ): self . selection_option_box . active = other . selection_option_box . active self . selection_option_box . on_change ( \"active\" , left_to_right ) other . selection_option_box . on_change ( \"active\" , right_to_left ) def link_xy_range ( self , other ): \"\"\" ???+ note \"Synchronize plotting ranges on the xy-plane.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr ) @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot something onto the figure.\" Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | `*args` | | left to child classes | | `**kwargs` | | left to child classes | \"\"\" pass def find_embedding_fields ( self ): \"\"\" ???+ note \"Find embedding fields from dataframes.\" Intended for scenarios where the embedding is higher than two-dimensional. \"\"\" embedding_cols = None for _key , _df in self . dfs . items (): # edge case: dataframe is empty if _df . shape [ 0 ] == 0 : continue # automatically find embedding columns _emb_cols = sorted ( filter ( is_embedding_field , _df . columns )) if embedding_cols is None : embedding_cols = _emb_cols else : # embedding columns must be the same across subsets assert embedding_cols == _emb_cols , \"Inconsistent embedding columns\" assert ( len ( embedding_cols ) >= 2 ), f \"Expected at least two embedding columns, found { embedding_cols } \" return embedding_cols def auto_color_mapping ( self ): \"\"\" ???+ note \"Find all labels and an appropriate color for each.\" \"\"\" from hover.utils.bokeh_helper import auto_label_color labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) return auto_label_color ( labels ) # def auto_legend_correction(self): # \"\"\" # ???+ note \"Find legend items and deduplicate by label, keeping the last glyph / legend item of each label.\" # This is to resolve duplicate legend items due to automatic legend_group and incremental plotting. # \"\"\" # from collections import OrderedDict # # if not hasattr(self.figure, \"legend\"): # self._fail(\"Attempting auto_legend_correction when there is no legend\") # return # # extract all items and start over # items = self.figure.legend.items[:] # self.figure.legend.items.clear() # # # use one item to hold all renderers matching its label # label_to_item = OrderedDict() # # # deduplication # for _item in items: # _label = _item.label.get(\"value\", \"\") # label_to_item[_label] = _item # # # WARNING: the current implementation discards renderer references. # # This could be for the best because renderers add up their glyphs to the legend item. # # To keep renderer references, see this example: # # if _label not in label_to_item.keys(): # # label_to_item[_label] = _item # # else: # # label_to_item[_label].renderers.extend(_item.renderers) # # self.figure.legend.items = list(label_to_item.values()) # # return # # @staticmethod # def auto_legend(method): # \"\"\" # ???+ note \"Decorator that handles legend pre/post-processing issues.\" # Usage: # # ```python # # in a child class # # @BokehBaseExplorer.auto_legend # def plot(self, *args, **kwargs): # # put code here # pass # ``` # \"\"\" # from functools import wraps # # @wraps(method) # def wrapped(ref, *args, **kwargs): # if hasattr(ref.figure, \"legend\"): # if hasattr(ref.figure.legend, \"items\"): # ref.figure.legend.items.clear() # # retval = method(ref, *args, **kwargs) # # ref.auto_legend_correction() # # return retval # # return wrapped __init__ ( self , df_dict , ** kwargs ) special Constructor shared by all child classes. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults set up dataframes to sync with create widgets that child classes can override create data sources the correspond to class-specific data subsets. initialize a figure under the settings above Source code in hover/core/explorer/base.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Constructor shared by all child classes.\" | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | 1. settle the figure settings by using child class defaults & kwargs overrides 2. settle the glyph settings by using child class defaults 3. set up dataframes to sync with 4. create widgets that child classes can override 5. create data sources the correspond to class-specific data subsets. 6. initialize a figure under the settings above \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip ( kwargs . pop ( \"tooltips\" , \"\" )), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . figure = figure ( ** self . figure_kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_dfs ( df_dict ) self . _setup_sources () self . _setup_widgets () activate_search ( self ) Assign search response callbacks to search results. Child methods should bind those callbacks to search widgets. This is a parent method which takes care of common denominators of parent methods. Child methods may inherit the logic here and preprocess/postprocess as needed. Source code in hover/core/explorer/base.py def activate_search ( self ): \"\"\" ???+ note \"Assign search response callbacks to search results. Child methods should bind those callbacks to search widgets.\" This is a parent method which takes care of common denominators of parent methods. Child methods may inherit the logic here and preprocess/postprocess as needed. \"\"\" self . _subroutine_search_create_callbacks () self . _subroutine_search_activate_callbacks () auto_color_mapping ( self ) Find all labels and an appropriate color for each. Source code in hover/core/explorer/base.py def auto_color_mapping ( self ): \"\"\" ???+ note \"Find all labels and an appropriate color for each.\" \"\"\" from hover.utils.bokeh_helper import auto_label_color labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) return auto_label_color ( labels ) find_embedding_fields ( self ) Find embedding fields from dataframes. Intended for scenarios where the embedding is higher than two-dimensional. Source code in hover/core/explorer/base.py def find_embedding_fields ( self ): \"\"\" ???+ note \"Find embedding fields from dataframes.\" Intended for scenarios where the embedding is higher than two-dimensional. \"\"\" embedding_cols = None for _key , _df in self . dfs . items (): # edge case: dataframe is empty if _df . shape [ 0 ] == 0 : continue # automatically find embedding columns _emb_cols = sorted ( filter ( is_embedding_field , _df . columns )) if embedding_cols is None : embedding_cols = _emb_cols else : # embedding columns must be the same across subsets assert embedding_cols == _emb_cols , \"Inconsistent embedding columns\" assert ( len ( embedding_cols ) >= 2 ), f \"Expected at least two embedding columns, found { embedding_cols } \" return embedding_cols from_dataset ( dataset , subset_mapping , * args , ** kwargs ) classmethod Alternative constructor from a SupervisableDataset . Param Type Description dataset SupervisableDataset dataset with DataFrame s subset_mapping dict dataset -> explorer subset mapping *args forwarded to the constructor **kwargs forwarded to the constructor Source code in hover/core/explorer/base.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" ???+ note \"Alternative constructor from a `SupervisableDataset`.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `dataset` | `SupervisableDataset` | dataset with `DataFrame`s | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | | `*args` | | forwarded to the constructor | | `**kwargs` | | forwarded to the constructor | \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer = cls ( df_dict , * args , ** kwargs ) explorer . link_dataset ( dataset ) return explorer link_dataset ( self , dataset ) Get tied to a dataset, which is common for explorers. Source code in hover/core/explorer/base.py def link_dataset ( self , dataset ): \"\"\" ???+ note \"Get tied to a dataset, which is common for explorers.\" \"\"\" if not hasattr ( self , \"linked_dataset\" ): self . linked_dataset = dataset else : assert self . linked_dataset is dataset , \"cannot link to two datasets\" link_selection ( self , key , other , other_key ) Synchronize the selected indices between specified sources. Param Type Description key str the key of the subset to link other BokehBaseExplorer the other explorer other_key str the key of the other subset Source code in hover/core/explorer/base.py def link_selection ( self , key , other , other_key ): \"\"\" ???+ note \"Synchronize the selected indices between specified sources.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `key` | `str` | the key of the subset to link | | `other` | `BokehBaseExplorer` | the other explorer | | `other_key` | `str` | the key of the other subset | \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] def left_to_right ( attr , old , new ): sr . selected . indices = sl . selected . indices [:] def right_to_left ( attr , old , new ): sl . selected . indices = sr . selected . indices [:] sl . selected . on_change ( \"indices\" , left_to_right ) sr . selected . on_change ( \"indices\" , right_to_left ) # link last manual selections (pointing to the same set) self . _last_selections [ key ] . union ( other . _last_selections [ other_key ]) # link selection filter functions (pointing to the same set) self . _selection_filters [ key ] . data . update ( other . _selection_filters [ other_key ] . data ) self . _selection_filters [ key ] . union ( other . _selection_filters [ other_key ]) link_selection_options ( self , other ) Synchronize the selection option values between explorers. Param Type Description other BokehBaseExplorer the other explorer Source code in hover/core/explorer/base.py def link_selection_options ( self , other ): \"\"\" ???+ note \"Synchronize the selection option values between explorers.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" def left_to_right ( attr , old , new ): other . selection_option_box . active = self . selection_option_box . active def right_to_left ( attr , old , new ): self . selection_option_box . active = other . selection_option_box . active self . selection_option_box . on_change ( \"active\" , left_to_right ) other . selection_option_box . on_change ( \"active\" , right_to_left ) link_xy_range ( self , other ) Synchronize plotting ranges on the xy-plane. Param Type Description other BokehBaseExplorer the other explorer Source code in hover/core/explorer/base.py def link_xy_range ( self , other ): \"\"\" ???+ note \"Synchronize plotting ranges on the xy-plane.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr ) plot ( self , * args , ** kwargs ) Plot something onto the figure. Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | *args | | left to child classes | | **kwargs | | left to child classes | Source code in hover/core/explorer/base.py @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot something onto the figure.\" Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | `*args` | | left to child classes | | `**kwargs` | | left to child classes | \"\"\" pass value_patch_by_slider ( self , col_original , col_patch , ** kwargs ) Allow source values to be dynamically patched through a slider. Param Type Description col_original str column of values before the patch col_patch str column of list of values to use as patches **kwargs forwarded to the slider Reference Source code in hover/core/explorer/base.py def value_patch_by_slider ( self , col_original , col_patch , ** kwargs ): \"\"\" ???+ note \"Allow source values to be dynamically patched through a slider.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `col_original` | `str` | column of values before the patch | | `col_patch` | `str` | column of list of values to use as patches | | `**kwargs` | | forwarded to the slider | [Reference](https://github.com/bokeh/bokeh/blob/2.4.2/examples/howto/patch_app.py) \"\"\" # add a patch slider to widgets, if none exist if \"patch_slider\" not in self . _dynamic_widgets : slider = Slider ( start = 0 , end = 1 , value = 0 , step = 1 , ** kwargs ) slider . disabled = True self . _dynamic_widgets [ \"patch_slider\" ] = slider else : slider = self . _dynamic_widgets [ \"patch_slider\" ] # create a slider-adjusting callback exposed to the outside def adjust_slider (): \"\"\" Infer slider length from the number of patch values. \"\"\" num_patches = None for _key , _df in self . dfs . items (): assert ( col_patch in _df . columns ), f \"Subset { _key } expecting column { col_patch } among columns, got { _df . columns } \" # find all array lengths; note that the data subset can be empty _num_patches_seen = _df [ col_patch ] . apply ( len ) . values assert ( len ( set ( _num_patches_seen )) <= 1 ), f \"Expecting consistent number of patches, got { _num_patches_seen } \" _num_patches = _num_patches_seen [ 0 ] if _df . shape [ 0 ] > 0 else None # if a previous subset has implied the number of patches, run a consistency check if num_patches is None : num_patches = _num_patches else : assert ( num_patches == _num_patches ), f \"Conflicting number of patches: { num_patches } vs { _num_patches } \" assert num_patches >= 2 , f \"Expecting at least 2 patches, got { num_patches } \" slider . end = num_patches - 1 slider . disabled = False self . _dynamic_callbacks [ \"adjust_patch_slider\" ] = adjust_slider # create the callback for patching values def update_patch ( attr , old , new ): for _key , _df in self . dfs . items (): # calculate the patch corresponding to slider value _value = [ _arr [ new ] for _arr in _df [ col_patch ] . values ] _slice = slice ( _df . shape [ 0 ]) _patch = { col_original : [( _slice , _value )]} self . sources [ _key ] . patch ( _patch ) slider . on_change ( \"value\" , update_patch ) self . _good ( f \"Patching { col_original } using { col_patch } \" ) view ( self ) Define the high-level visual layout of the whole explorer. Source code in hover/core/explorer/base.py def view ( self ): \"\"\" ???+ note \"Define the high-level visual layout of the whole explorer.\" \"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure )","title":".base"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer","text":"Base class for visually exploring data with Bokeh . Assumes: in supplied dataframes (always) xy coordinates in x and y columns; (always) an index for the rows; (always) classification label (or ABSTAIN) in a label column. Does not assume: a specific form of data; what the map serves to do. Source code in hover/core/explorer/base.py class BokehBaseExplorer ( Loggable , ABC , metaclass = RichTracebackABCMeta ): \"\"\" ???+ note \"Base class for visually exploring data with `Bokeh`.\" Assumes: - in supplied dataframes - (always) xy coordinates in `x` and `y` columns; - (always) an index for the rows; - (always) classification label (or ABSTAIN) in a `label` column. Does not assume: - a specific form of data; - what the map serves to do. \"\"\" SUBSET_GLYPH_KWARGS = {} DEFAULT_SUBSET_MAPPING = { _k : _k for _k in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]} PRIMARY_FEATURE = None MANDATORY_COLUMNS = [ \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"coords\" : True , \"index\" : True } def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Constructor shared by all child classes.\" | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | 1. settle the figure settings by using child class defaults & kwargs overrides 2. settle the glyph settings by using child class defaults 3. set up dataframes to sync with 4. create widgets that child classes can override 5. create data sources the correspond to class-specific data subsets. 6. initialize a figure under the settings above \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip ( kwargs . pop ( \"tooltips\" , \"\" )), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . figure = figure ( ** self . figure_kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_dfs ( df_dict ) self . _setup_sources () self . _setup_widgets () @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" ???+ note \"Alternative constructor from a `SupervisableDataset`.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `dataset` | `SupervisableDataset` | dataset with `DataFrame`s | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | | `*args` | | forwarded to the constructor | | `**kwargs` | | forwarded to the constructor | \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer = cls ( df_dict , * args , ** kwargs ) explorer . link_dataset ( dataset ) return explorer def link_dataset ( self , dataset ): \"\"\" ???+ note \"Get tied to a dataset, which is common for explorers.\" \"\"\" if not hasattr ( self , \"linked_dataset\" ): self . linked_dataset = dataset else : assert self . linked_dataset is dataset , \"cannot link to two datasets\" def view ( self ): \"\"\" ???+ note \"Define the high-level visual layout of the whole explorer.\" \"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure ) def _build_tooltip ( self , extra ): \"\"\" ???+ note \"Define a windowed tooltip which shows inspection details.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `extra` | `str` | user-supplied extra HTML | Note that this is a method rather than a class attribute because child classes may involve instance attributes in the tooltip. \"\"\" standard = bokeh_hover_tooltip ( ** self . __class__ . TOOLTIP_KWARGS ) return f \" { standard } \\n { extra } \" def _setup_widgets ( self ): \"\"\" ???+ note \"High-level function creating widgets for interactive functionality.\" \"\"\" self . _info ( \"Setting up widgets\" ) self . _dynamic_widgets = OrderedDict () self . _dynamic_callbacks = OrderedDict () self . _dynamic_resources = OrderedDict () self . _setup_search_widgets () self . _setup_selection_option () self . _setup_subset_toggle () self . _setup_axes_dropdown () @abstractmethod def _layout_widgets ( self ): \"\"\" ???+ note \"Define the low-level layout of widgets.\" \"\"\" pass @abstractmethod def _setup_search_widgets ( self ): \"\"\" ???+ note \"Define how to search data points.\" Left to child classes that have a specific feature format. \"\"\" pass def _setup_selection_option ( self ): \"\"\" ???+ note \"Create a group of checkbox(es) for advanced selection options.\" \"\"\" from bokeh.models import RadioGroup self . selection_option_box = RadioGroup ( labels = [ \"keep selecting: none\" , \"union\" , \"intersection\" , \"difference\" ], active = 0 , ) def _setup_subset_toggle ( self ): \"\"\" ???+ note \"Create a group of buttons for toggling which data subsets to show.\" \"\"\" from bokeh.models import CheckboxButtonGroup , Div from bokeh.layouts import column data_keys = list ( self . __class__ . SUBSET_GLYPH_KWARGS . keys ()) self . data_key_button_group = CheckboxButtonGroup ( labels = data_keys , active = list ( range ( len ( data_keys ))) ) self . data_key_button_group_help = Div ( text = \"Toggle data subset display\" ) self . subset_toggle_widget_column = column ( self . data_key_button_group_help , self . data_key_button_group ) def update_data_key_display ( active ): visible_keys = { self . data_key_button_group . labels [ idx ] for idx in active } for _renderer in self . figure . renderers : # if the renderer has a name \"on the list\", update its visibility if _renderer . name in self . __class__ . SUBSET_GLYPH_KWARGS . keys (): _renderer . visible = _renderer . name in visible_keys # store the callback (useful, for example, during automated tests) and link it self . _callback_subset_display = lambda : update_data_key_display ( self . data_key_button_group . active ) self . data_key_button_group . on_click ( update_data_key_display ) def _setup_axes_dropdown ( self ): \"\"\" ???+ note \"Find embedding fields and allow any of them to be set as the x or y axis.\" \"\"\" from bokeh.models import Dropdown embed_cols = self . find_embedding_fields () init_x , init_y = embed_cols [: 2 ] self . dropdown_x_axis = Dropdown ( label = f \"X coord: { init_x } \" , menu = embed_cols ) self . dropdown_y_axis = Dropdown ( label = f \"Y coord: { init_y } \" , menu = embed_cols ) def change_x ( event ): self . dropdown_x_axis . label = f \"X coord: { event . item } \" for _renderer in self . figure . renderers : _renderer . glyph . x = event . item def change_y ( event ): self . dropdown_y_axis . label = f \"Y coord: { event . item } \" for _renderer in self . figure . renderers : _renderer . glyph . y = event . item self . dropdown_x_axis . on_click ( change_x ) self . dropdown_y_axis . on_click ( change_y ) # consider allowing dynamic menu refreshment # def refresh_axes_list(): # embed_cols = self.find_embedding_fields() # self.dropdown_x_axis.menu = embed_cols[:] # self.dropdown_y_axis.menu = embed_cols[:] def value_patch_by_slider ( self , col_original , col_patch , ** kwargs ): \"\"\" ???+ note \"Allow source values to be dynamically patched through a slider.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `col_original` | `str` | column of values before the patch | | `col_patch` | `str` | column of list of values to use as patches | | `**kwargs` | | forwarded to the slider | [Reference](https://github.com/bokeh/bokeh/blob/2.4.2/examples/howto/patch_app.py) \"\"\" # add a patch slider to widgets, if none exist if \"patch_slider\" not in self . _dynamic_widgets : slider = Slider ( start = 0 , end = 1 , value = 0 , step = 1 , ** kwargs ) slider . disabled = True self . _dynamic_widgets [ \"patch_slider\" ] = slider else : slider = self . _dynamic_widgets [ \"patch_slider\" ] # create a slider-adjusting callback exposed to the outside def adjust_slider (): \"\"\" Infer slider length from the number of patch values. \"\"\" num_patches = None for _key , _df in self . dfs . items (): assert ( col_patch in _df . columns ), f \"Subset { _key } expecting column { col_patch } among columns, got { _df . columns } \" # find all array lengths; note that the data subset can be empty _num_patches_seen = _df [ col_patch ] . apply ( len ) . values assert ( len ( set ( _num_patches_seen )) <= 1 ), f \"Expecting consistent number of patches, got { _num_patches_seen } \" _num_patches = _num_patches_seen [ 0 ] if _df . shape [ 0 ] > 0 else None # if a previous subset has implied the number of patches, run a consistency check if num_patches is None : num_patches = _num_patches else : assert ( num_patches == _num_patches ), f \"Conflicting number of patches: { num_patches } vs { _num_patches } \" assert num_patches >= 2 , f \"Expecting at least 2 patches, got { num_patches } \" slider . end = num_patches - 1 slider . disabled = False self . _dynamic_callbacks [ \"adjust_patch_slider\" ] = adjust_slider # create the callback for patching values def update_patch ( attr , old , new ): for _key , _df in self . dfs . items (): # calculate the patch corresponding to slider value _value = [ _arr [ new ] for _arr in _df [ col_patch ] . values ] _slice = slice ( _df . shape [ 0 ]) _patch = { col_original : [( _slice , _value )]} self . sources [ _key ] . patch ( _patch ) slider . on_change ( \"value\" , update_patch ) self . _good ( f \"Patching { col_original } using { col_patch } \" ) def _mandatory_column_defaults ( self ): \"\"\" ???+ note \"Mandatory columns and default values.\" If default value is None, will raise exception if the column is not found. \"\"\" return { _col : None for _col in self . __class__ . MANDATORY_COLUMNS } def _setup_dfs ( self , df_dict , copy = False ): \"\"\" ???+ note \"Check and store DataFrames **by reference by default**.\" Intended to be extended in child classes for pre/post processing. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `copy` | `bool` | whether to copy `DataFrame`s | \"\"\" self . _info ( \"Setting up DataFrames\" ) supplied_keys = set ( df_dict . keys ()) expected_keys = set ( self . __class__ . SUBSET_GLYPH_KWARGS . keys ()) # perform high-level df key checks expected_and_supplied = supplied_keys . intersection ( expected_keys ) supplied_not_expected = supplied_keys . difference ( expected_keys ) expected_not_supplied = expected_keys . difference ( supplied_keys ) for _key in supplied_not_expected : self . _warn ( f \" { self . __class__ . __name__ } .__init__(): got unexpected df key { _key } \" ) for _key in expected_not_supplied : self . _warn ( f \" { self . __class__ . __name__ } .__init__(): missing expected df key { _key } \" ) # assign df with column checks self . dfs = dict () mandatory_col_to_default = self . _mandatory_column_defaults () for _key in expected_and_supplied : _df = df_dict [ _key ] for _col , _default in mandatory_col_to_default . items (): # column exists: all good if _col in _df . columns : continue # no default value: column must be explicitly provided if _default is None : # edge case: DataFrame has zero rows _msg = f \"Expecting column ' { _col } ' from { _key } df: found { list ( _df . columns ) } \" assert _df . shape [ 0 ] == 0 , _msg # default value available, will use it to create column else : _df [ _col ] = _default self . dfs [ _key ] = _df . copy () if copy else _df # expected dfs must be present for _key in expected_not_supplied : _df = pd . DataFrame ( columns = list ( mandatory_col_to_default . keys ())) self . dfs [ _key ] = _df def _setup_sources ( self ): \"\"\" ???+ note \"Create, **(not update)**, `ColumnDataSource` objects.\" Intended to be extended in child classes for pre/post processing. \"\"\" self . _info ( \"Setting up sources\" ) self . sources = { _key : ColumnDataSource ( _df ) for _key , _df in self . dfs . items ()} self . _postprocess_sources () # initialize attributes that couple with sources # extra columns for dynamic plotting self . _extra_source_cols = defaultdict ( dict ) self . _setup_selection_tools () def _setup_subroutine_selection_store ( self ): \"\"\" ???+ note \"Subroutine of `_setup_selection_tools`.\" Setup callbacks that interact with manual selections. \"\"\" def store_selection (): \"\"\" Keep track of the last manual selection. Useful for applying cumulation / filters dynamically. \"\"\" # determine selection mode selection_option_code = self . selection_option_box . active for _key , _source in self . sources . items (): _selected = _source . selected . indices # use sets' in-place methods instead of assignment if selection_option_code == 1 : self . _last_selections [ _key ] . data . update ( _selected ) elif selection_option_code == 2 : self . _last_selections [ _key ] . data . intersection_update ( _selected ) elif selection_option_code == 3 : self . _last_selections [ _key ] . data . difference_update ( _selected ) else : assert selection_option_code == 0 self . _last_selections [ _key ] . data . clear () self . _last_selections [ _key ] . data . update ( _selected ) _source . selected . indices = list ( self . _last_selections [ _key ] . data ) self . _store_selection = store_selection self . figure . on_event ( SelectionGeometry , lambda event : self . _store_selection () if event . final else None , ) def _setup_subroutine_selection_filter ( self ): \"\"\" ???+ note \"Subroutine of `_setup_selection_tools`.\" Setup callbacks that interact with selection filters. \"\"\" def trigger_selection_filters ( subsets = None ): \"\"\" Filter selection indices on specified subsets. \"\"\" if subsets is None : subsets = self . sources . keys () else : assert set ( subsets ) . issubset ( self . sources . keys () ), f \"Expected subsets from { self . sources . keys () } \" for _key in subsets : _selected = self . _last_selections [ _key ] . data for _func in self . _selection_filters [ _key ] . data : _selected = _func ( _selected , _key ) self . sources [ _key ] . selected . indices = list ( _selected ) # keep reference to trigger_selection_filter() for further access # for example, toggling filters should call the trigger self . _trigger_selection_filters = trigger_selection_filters self . figure . on_event ( SelectionGeometry , lambda event : self . _trigger_selection_filters () if event . final else None , ) def _setup_subroutine_selection_reset ( self ): \"\"\" ???+ note \"Subroutine of `_setup_selection_tools`.\" Setup callbacks for scenarios where the selection should be reset. For example, when the plot sources have changed. \"\"\" def reset_selection (): \"\"\" Clear last manual selections and source selections. Useful during post-processing of refreshed data source. Not directly defined as a method because of `self._last_selections`. \"\"\" for _key , _source in self . sources . items (): self . _last_selections [ _key ] . data . clear () _source . selected . indices = [] self . _reset_selection = reset_selection def _setup_selection_tools ( self ): \"\"\" ???+ note \"Create data structures and callbacks for dynamic selections.\" Useful for linking and filtering selections across explorers. \"\"\" # store the last manual selections self . _last_selections = { _key : RootUnionFind ( set ()) for _key in self . sources . keys () } # store commutative, idempotent index filters self . _selection_filters = { _key : RootUnionFind ( set ()) for _key in self . sources . keys () } self . _setup_subroutine_selection_store () self . _setup_subroutine_selection_filter () self . _setup_subroutine_selection_reset () def _update_sources ( self ): \"\"\" ???+ note \"Update the sources with the corresponding dfs.\" Note that the shapes and fields of sources are overriden. Thus supplementary fields (those that do not exist in the dfs), such as dynamic plotting kwargs, need to be re-assigned. \"\"\" for _key in self . dfs . keys (): self . sources [ _key ] . data = self . dfs [ _key ] self . _postprocess_sources () # reset selections now that source indices may have changed self . _reset_selection () # reset attribute values that couple with sources for _key in self . sources . keys (): _num_points = len ( self . sources [ _key ] . data [ \"label\" ]) # add extra columns for _col , _fill_value in self . _extra_source_cols [ _key ] . items (): self . sources [ _key ] . add ([ _fill_value ] * _num_points , _col ) # clear last selection but keep the set object self . _last_selections [ _key ] . data . clear () # DON'T DO: self._last_selections = {_key: set() for _key in self.sources.keys()} def _postprocess_sources ( self ): \"\"\" ???+ note \"Infer source attributes from the dfs, without altering the dfs.\" Useful for assigning dynamic glyph attributes, similarly to `activate_search()`. \"\"\" pass def activate_search ( self ): \"\"\" ???+ note \"Assign search response callbacks to search results. Child methods should bind those callbacks to search widgets.\" This is a parent method which takes care of common denominators of parent methods. Child methods may inherit the logic here and preprocess/postprocess as needed. \"\"\" self . _subroutine_search_create_callbacks () self . _subroutine_search_activate_callbacks () def _subroutine_search_create_callbacks ( self ): \"\"\" ???+ note \"Create search callback functions based on feature attributes.\" \"\"\" # allow dynamically updated search response through dict element retrieval self . _dynamic_callbacks [ \"search_response\" ] = dict () def search_base_response ( attr , old , new ): for _subset in self . sources . keys (): _func = self . _dynamic_callbacks [ \"search_response\" ] . get ( _subset , blank ) _func ( attr , old , new ) return self . search_base_response = search_base_response for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items (): # create a field that holds search results that could be used elsewhere _num_points = len ( self . sources [ _key ] . data [ self . __class__ . PRIMARY_FEATURE ]) self . _extra_source_cols [ _key ][ SEARCH_SCORE_FIELD ] = 0 self . sources [ _key ] . add ([ 0 ] * _num_points , SEARCH_SCORE_FIELD ) # make attributes respond to search for _ , _params in _dict [ \"search\" ] . items (): _updated_kwargs = self . _subroutine_search_source_change ( _key , self . glyph_kwargs [ _key ], altered_param = _params , ) self . glyph_kwargs [ _key ] . clear () self . glyph_kwargs [ _key ] . update ( _updated_kwargs ) def _subroutine_search_activate_callbacks ( self ): \"\"\" ???+ note \"Activate search callback functions by binding them to widgets.\" \"\"\" for _widget in self . _search_watch_widgets (): _widget . on_change ( \"value\" , self . search_base_response ) self . _info ( f \"activated search base response on { _widget } \" ) @abstractmethod def _search_watch_widgets ( self ): \"\"\" ???+ note \"Widgets to trigger search callbacks automatically, which can be different across subclasses.\" Intended for binding callback functions to widgets. \"\"\" @abstractmethod def _validate_search_input ( self ): \"\"\" ???+ note \"Check the search input, skipping callbacks if it's invalid.\" \"\"\" pass @abstractmethod def _get_search_score_function ( self ): \"\"\" ???+ note \"Dynamically create a single-argument scoring function.\" \"\"\" pass def _subroutine_search_source_change ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 ) ): \"\"\" ???+ note \"Subroutine of `activate_search()` on a specific subset.\" Modifies the plotting source in-place. | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () feature_key = self . __class__ . PRIMARY_FEATURE param_key , param_pos , param_neg , param_default = altered_param initial_num = len ( self . sources [ subset ] . data [ feature_key ]) self . sources [ subset ] . add ([ param_default ] * initial_num , param_key ) self . _extra_source_cols [ subset ][ param_key ] = param_default updated_kwargs [ param_key ] = param_key def score_to_param ( score ): if score > 0 : return param_pos elif score == 0 : return param_default else : return param_neg def search_response ( attr , old , new ): valid_flag = self . _validate_search_input () if not valid_flag : return score_func = self . _get_search_score_function () patch_slice = slice ( len ( self . sources [ subset ] . data [ feature_key ])) features = self . sources [ subset ] . data [ self . __class__ . PRIMARY_FEATURE ] # score_func is potentially slow; track its progress # search_scores = list(map(score_func, tqdm(features, desc=\"Search score\"))) search_scores = list ( map ( score_func , features )) search_params = list ( map ( score_to_param , search_scores )) self . sources [ subset ] . patch ( { SEARCH_SCORE_FIELD : [( patch_slice , search_scores )]} ) self . sources [ subset ] . patch ({ param_key : [( patch_slice , search_params )]}) return # assign dynamic callback self . _dynamic_callbacks [ \"search_response\" ][ subset ] = search_response return updated_kwargs def _prelink_check ( self , other ): \"\"\" ???+ note \"Sanity check before linking two explorers.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" assert other is not self , \"Self-loops are fordidden\" assert isinstance ( other , BokehBaseExplorer ), \"Must link to BokehBaseExplorer\" def link_selection ( self , key , other , other_key ): \"\"\" ???+ note \"Synchronize the selected indices between specified sources.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `key` | `str` | the key of the subset to link | | `other` | `BokehBaseExplorer` | the other explorer | | `other_key` | `str` | the key of the other subset | \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] def left_to_right ( attr , old , new ): sr . selected . indices = sl . selected . indices [:] def right_to_left ( attr , old , new ): sl . selected . indices = sr . selected . indices [:] sl . selected . on_change ( \"indices\" , left_to_right ) sr . selected . on_change ( \"indices\" , right_to_left ) # link last manual selections (pointing to the same set) self . _last_selections [ key ] . union ( other . _last_selections [ other_key ]) # link selection filter functions (pointing to the same set) self . _selection_filters [ key ] . data . update ( other . _selection_filters [ other_key ] . data ) self . _selection_filters [ key ] . union ( other . _selection_filters [ other_key ]) def link_selection_options ( self , other ): \"\"\" ???+ note \"Synchronize the selection option values between explorers.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" def left_to_right ( attr , old , new ): other . selection_option_box . active = self . selection_option_box . active def right_to_left ( attr , old , new ): self . selection_option_box . active = other . selection_option_box . active self . selection_option_box . on_change ( \"active\" , left_to_right ) other . selection_option_box . on_change ( \"active\" , right_to_left ) def link_xy_range ( self , other ): \"\"\" ???+ note \"Synchronize plotting ranges on the xy-plane.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr ) @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot something onto the figure.\" Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | `*args` | | left to child classes | | `**kwargs` | | left to child classes | \"\"\" pass def find_embedding_fields ( self ): \"\"\" ???+ note \"Find embedding fields from dataframes.\" Intended for scenarios where the embedding is higher than two-dimensional. \"\"\" embedding_cols = None for _key , _df in self . dfs . items (): # edge case: dataframe is empty if _df . shape [ 0 ] == 0 : continue # automatically find embedding columns _emb_cols = sorted ( filter ( is_embedding_field , _df . columns )) if embedding_cols is None : embedding_cols = _emb_cols else : # embedding columns must be the same across subsets assert embedding_cols == _emb_cols , \"Inconsistent embedding columns\" assert ( len ( embedding_cols ) >= 2 ), f \"Expected at least two embedding columns, found { embedding_cols } \" return embedding_cols def auto_color_mapping ( self ): \"\"\" ???+ note \"Find all labels and an appropriate color for each.\" \"\"\" from hover.utils.bokeh_helper import auto_label_color labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) return auto_label_color ( labels ) # def auto_legend_correction(self): # \"\"\" # ???+ note \"Find legend items and deduplicate by label, keeping the last glyph / legend item of each label.\" # This is to resolve duplicate legend items due to automatic legend_group and incremental plotting. # \"\"\" # from collections import OrderedDict # # if not hasattr(self.figure, \"legend\"): # self._fail(\"Attempting auto_legend_correction when there is no legend\") # return # # extract all items and start over # items = self.figure.legend.items[:] # self.figure.legend.items.clear() # # # use one item to hold all renderers matching its label # label_to_item = OrderedDict() # # # deduplication # for _item in items: # _label = _item.label.get(\"value\", \"\") # label_to_item[_label] = _item # # # WARNING: the current implementation discards renderer references. # # This could be for the best because renderers add up their glyphs to the legend item. # # To keep renderer references, see this example: # # if _label not in label_to_item.keys(): # # label_to_item[_label] = _item # # else: # # label_to_item[_label].renderers.extend(_item.renderers) # # self.figure.legend.items = list(label_to_item.values()) # # return # # @staticmethod # def auto_legend(method): # \"\"\" # ???+ note \"Decorator that handles legend pre/post-processing issues.\" # Usage: # # ```python # # in a child class # # @BokehBaseExplorer.auto_legend # def plot(self, *args, **kwargs): # # put code here # pass # ``` # \"\"\" # from functools import wraps # # @wraps(method) # def wrapped(ref, *args, **kwargs): # if hasattr(ref.figure, \"legend\"): # if hasattr(ref.figure.legend, \"items\"): # ref.figure.legend.items.clear() # # retval = method(ref, *args, **kwargs) # # ref.auto_legend_correction() # # return retval # # return wrapped","title":"BokehBaseExplorer"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.__init__","text":"Constructor shared by all child classes. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults set up dataframes to sync with create widgets that child classes can override create data sources the correspond to class-specific data subsets. initialize a figure under the settings above Source code in hover/core/explorer/base.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Constructor shared by all child classes.\" | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | 1. settle the figure settings by using child class defaults & kwargs overrides 2. settle the glyph settings by using child class defaults 3. set up dataframes to sync with 4. create widgets that child classes can override 5. create data sources the correspond to class-specific data subsets. 6. initialize a figure under the settings above \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip ( kwargs . pop ( \"tooltips\" , \"\" )), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . figure = figure ( ** self . figure_kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_dfs ( df_dict ) self . _setup_sources () self . _setup_widgets ()","title":"__init__()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.activate_search","text":"Assign search response callbacks to search results. Child methods should bind those callbacks to search widgets. This is a parent method which takes care of common denominators of parent methods. Child methods may inherit the logic here and preprocess/postprocess as needed. Source code in hover/core/explorer/base.py def activate_search ( self ): \"\"\" ???+ note \"Assign search response callbacks to search results. Child methods should bind those callbacks to search widgets.\" This is a parent method which takes care of common denominators of parent methods. Child methods may inherit the logic here and preprocess/postprocess as needed. \"\"\" self . _subroutine_search_create_callbacks () self . _subroutine_search_activate_callbacks ()","title":"activate_search()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.auto_color_mapping","text":"Find all labels and an appropriate color for each. Source code in hover/core/explorer/base.py def auto_color_mapping ( self ): \"\"\" ???+ note \"Find all labels and an appropriate color for each.\" \"\"\" from hover.utils.bokeh_helper import auto_label_color labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) return auto_label_color ( labels )","title":"auto_color_mapping()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.find_embedding_fields","text":"Find embedding fields from dataframes. Intended for scenarios where the embedding is higher than two-dimensional. Source code in hover/core/explorer/base.py def find_embedding_fields ( self ): \"\"\" ???+ note \"Find embedding fields from dataframes.\" Intended for scenarios where the embedding is higher than two-dimensional. \"\"\" embedding_cols = None for _key , _df in self . dfs . items (): # edge case: dataframe is empty if _df . shape [ 0 ] == 0 : continue # automatically find embedding columns _emb_cols = sorted ( filter ( is_embedding_field , _df . columns )) if embedding_cols is None : embedding_cols = _emb_cols else : # embedding columns must be the same across subsets assert embedding_cols == _emb_cols , \"Inconsistent embedding columns\" assert ( len ( embedding_cols ) >= 2 ), f \"Expected at least two embedding columns, found { embedding_cols } \" return embedding_cols","title":"find_embedding_fields()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.from_dataset","text":"Alternative constructor from a SupervisableDataset . Param Type Description dataset SupervisableDataset dataset with DataFrame s subset_mapping dict dataset -> explorer subset mapping *args forwarded to the constructor **kwargs forwarded to the constructor Source code in hover/core/explorer/base.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" ???+ note \"Alternative constructor from a `SupervisableDataset`.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `dataset` | `SupervisableDataset` | dataset with `DataFrame`s | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | | `*args` | | forwarded to the constructor | | `**kwargs` | | forwarded to the constructor | \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer = cls ( df_dict , * args , ** kwargs ) explorer . link_dataset ( dataset ) return explorer","title":"from_dataset()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_dataset","text":"Get tied to a dataset, which is common for explorers. Source code in hover/core/explorer/base.py def link_dataset ( self , dataset ): \"\"\" ???+ note \"Get tied to a dataset, which is common for explorers.\" \"\"\" if not hasattr ( self , \"linked_dataset\" ): self . linked_dataset = dataset else : assert self . linked_dataset is dataset , \"cannot link to two datasets\"","title":"link_dataset()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_selection","text":"Synchronize the selected indices between specified sources. Param Type Description key str the key of the subset to link other BokehBaseExplorer the other explorer other_key str the key of the other subset Source code in hover/core/explorer/base.py def link_selection ( self , key , other , other_key ): \"\"\" ???+ note \"Synchronize the selected indices between specified sources.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `key` | `str` | the key of the subset to link | | `other` | `BokehBaseExplorer` | the other explorer | | `other_key` | `str` | the key of the other subset | \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] def left_to_right ( attr , old , new ): sr . selected . indices = sl . selected . indices [:] def right_to_left ( attr , old , new ): sl . selected . indices = sr . selected . indices [:] sl . selected . on_change ( \"indices\" , left_to_right ) sr . selected . on_change ( \"indices\" , right_to_left ) # link last manual selections (pointing to the same set) self . _last_selections [ key ] . union ( other . _last_selections [ other_key ]) # link selection filter functions (pointing to the same set) self . _selection_filters [ key ] . data . update ( other . _selection_filters [ other_key ] . data ) self . _selection_filters [ key ] . union ( other . _selection_filters [ other_key ])","title":"link_selection()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_selection_options","text":"Synchronize the selection option values between explorers. Param Type Description other BokehBaseExplorer the other explorer Source code in hover/core/explorer/base.py def link_selection_options ( self , other ): \"\"\" ???+ note \"Synchronize the selection option values between explorers.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" def left_to_right ( attr , old , new ): other . selection_option_box . active = self . selection_option_box . active def right_to_left ( attr , old , new ): self . selection_option_box . active = other . selection_option_box . active self . selection_option_box . on_change ( \"active\" , left_to_right ) other . selection_option_box . on_change ( \"active\" , right_to_left )","title":"link_selection_options()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_xy_range","text":"Synchronize plotting ranges on the xy-plane. Param Type Description other BokehBaseExplorer the other explorer Source code in hover/core/explorer/base.py def link_xy_range ( self , other ): \"\"\" ???+ note \"Synchronize plotting ranges on the xy-plane.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr )","title":"link_xy_range()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.plot","text":"Plot something onto the figure. Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | *args | | left to child classes | | **kwargs | | left to child classes | Source code in hover/core/explorer/base.py @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot something onto the figure.\" Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | `*args` | | left to child classes | | `**kwargs` | | left to child classes | \"\"\" pass","title":"plot()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.value_patch_by_slider","text":"Allow source values to be dynamically patched through a slider. Param Type Description col_original str column of values before the patch col_patch str column of list of values to use as patches **kwargs forwarded to the slider Reference Source code in hover/core/explorer/base.py def value_patch_by_slider ( self , col_original , col_patch , ** kwargs ): \"\"\" ???+ note \"Allow source values to be dynamically patched through a slider.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `col_original` | `str` | column of values before the patch | | `col_patch` | `str` | column of list of values to use as patches | | `**kwargs` | | forwarded to the slider | [Reference](https://github.com/bokeh/bokeh/blob/2.4.2/examples/howto/patch_app.py) \"\"\" # add a patch slider to widgets, if none exist if \"patch_slider\" not in self . _dynamic_widgets : slider = Slider ( start = 0 , end = 1 , value = 0 , step = 1 , ** kwargs ) slider . disabled = True self . _dynamic_widgets [ \"patch_slider\" ] = slider else : slider = self . _dynamic_widgets [ \"patch_slider\" ] # create a slider-adjusting callback exposed to the outside def adjust_slider (): \"\"\" Infer slider length from the number of patch values. \"\"\" num_patches = None for _key , _df in self . dfs . items (): assert ( col_patch in _df . columns ), f \"Subset { _key } expecting column { col_patch } among columns, got { _df . columns } \" # find all array lengths; note that the data subset can be empty _num_patches_seen = _df [ col_patch ] . apply ( len ) . values assert ( len ( set ( _num_patches_seen )) <= 1 ), f \"Expecting consistent number of patches, got { _num_patches_seen } \" _num_patches = _num_patches_seen [ 0 ] if _df . shape [ 0 ] > 0 else None # if a previous subset has implied the number of patches, run a consistency check if num_patches is None : num_patches = _num_patches else : assert ( num_patches == _num_patches ), f \"Conflicting number of patches: { num_patches } vs { _num_patches } \" assert num_patches >= 2 , f \"Expecting at least 2 patches, got { num_patches } \" slider . end = num_patches - 1 slider . disabled = False self . _dynamic_callbacks [ \"adjust_patch_slider\" ] = adjust_slider # create the callback for patching values def update_patch ( attr , old , new ): for _key , _df in self . dfs . items (): # calculate the patch corresponding to slider value _value = [ _arr [ new ] for _arr in _df [ col_patch ] . values ] _slice = slice ( _df . shape [ 0 ]) _patch = { col_original : [( _slice , _value )]} self . sources [ _key ] . patch ( _patch ) slider . on_change ( \"value\" , update_patch ) self . _good ( f \"Patching { col_original } using { col_patch } \" )","title":"value_patch_by_slider()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.view","text":"Define the high-level visual layout of the whole explorer. Source code in hover/core/explorer/base.py def view ( self ): \"\"\" ???+ note \"Define the high-level visual layout of the whole explorer.\" \"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure )","title":"view()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base","text":"","title":"hover.core.explorer.base"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer","text":"","title":"BokehBaseExplorer"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.activate_search","text":"","title":"activate_search()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.auto_color_mapping","text":"","title":"auto_color_mapping()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.find_embedding_fields","text":"","title":"find_embedding_fields()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.from_dataset","text":"","title":"from_dataset()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_dataset","text":"","title":"link_dataset()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_selection","text":"","title":"link_selection()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_selection_options","text":"","title":"link_selection_options()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_xy_range","text":"","title":"link_xy_range()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.value_patch_by_slider","text":"","title":"value_patch_by_slider()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.view","text":"","title":"view()"},{"location":"pages/reference/core-explorer-feature/","text":"Intermediate classes based on the main feature. BokehForAudio ( BokehForUrlToVector ) BokehForUrlToVector with audio (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) audio urls in an audio column Does not assume: what the explorer serves to do. Source code in hover/core/explorer/feature.py class BokehForAudio ( BokehForUrlToVector ): \"\"\" ???+ note \"`BokehForUrlToVector` with `audio` (path like `\"http://\"` or `\"file:///\"`) as the main feature.\" Assumes on top of its parent class: - in supplied dataframes - (always) audio urls in an `audio` column Does not assume: - what the explorer serves to do. \"\"\" PRIMARY_FEATURE = \"audio\" MANDATORY_COLUMNS = [ PRIMARY_FEATURE , \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"audio\" : True , \"coords\" : True , \"index\" : True } BokehForImage ( BokehForUrlToVector ) BokehForUrlToVector with image (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) image urls in an image column Does not assume: what the explorer serves to do. Source code in hover/core/explorer/feature.py class BokehForImage ( BokehForUrlToVector ): \"\"\" ???+ note \"`BokehForUrlToVector` with `image` (path like `\"http://\"` or `\"file:///\"`) as the main feature.\" Assumes on top of its parent class: - in supplied dataframes - (always) image urls in an `image` column Does not assume: - what the explorer serves to do. \"\"\" PRIMARY_FEATURE = \"image\" MANDATORY_COLUMNS = [ PRIMARY_FEATURE , \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"image\" : True , \"coords\" : True , \"index\" : True } BokehForText ( BokehBaseExplorer ) BokehBaseExplorer with text ( str ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) text data in a text column Does not assume: what the explorer serves to do. Source code in hover/core/explorer/feature.py class BokehForText ( BokehBaseExplorer ): \"\"\" ???+ note \"`BokehBaseExplorer` with `text` (`str`) as the main feature.\" Assumes on top of its parent class: - in supplied dataframes - (always) text data in a `text` column Does not assume: - what the explorer serves to do. \"\"\" PRIMARY_FEATURE = \"text\" MANDATORY_COLUMNS = [ PRIMARY_FEATURE , \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"text\" : True , \"coords\" : True , \"index\" : True } def _setup_search_widgets ( self ): \"\"\" ???+ note \"Create positive/negative text search boxes.\" \"\"\" common_kwargs = dict ( width_policy = \"fit\" , height_policy = \"fit\" ) pos_title , neg_title = \"Text contains (python regex):\" , \"Text does not contain:\" self . search_pos = TextInput ( title = pos_title , ** common_kwargs ) self . search_neg = TextInput ( title = neg_title , ** common_kwargs ) def _search_watch_widgets ( self ): return [ self . search_pos , self . search_neg ] def _validate_search_input ( self ): \"\"\" ???+ note \"Text uses regex search, for which any string can be considered valid.\" \"\"\" return True def _get_search_score_function ( self ): \"\"\" ???+ note \"Dynamically create a single-argument scoring function.\" \"\"\" pos_regex , neg_regex = self . search_pos . value , self . search_neg . value def regex_score ( text ): score = 0 if len ( pos_regex ) > 0 : score += 1 if re . search ( pos_regex , text ) else - 2 if len ( neg_regex ) > 0 : score += - 2 if re . search ( neg_regex , text ) else 1 return score return regex_score BokehForUrlToVector ( BokehBaseExplorer ) A layer of abstraction for BokehBaseExplorer subclasses whose feature-type-specific mechanisms work the same way through vectors. Source code in hover/core/explorer/feature.py class BokehForUrlToVector ( BokehBaseExplorer ): \"\"\" ???+ note \"A layer of abstraction for `BokehBaseExplorer` subclasses whose feature-type-specific mechanisms work the same way through vectors.\" \"\"\" def _setup_search_widgets ( self ): \"\"\" ???+ note \"Create similarity search widgets.\" \"\"\" self . search_sim = TextInput ( title = f \" { self . __class__ . PRIMARY_FEATURE } similarity search (enter URL)\" . capitalize (), width_policy = \"fit\" , height_policy = \"fit\" , ) self . search_threshold = Slider ( start = 0.0 , end = 1.0 , value = 0.9 , # fewer steps allowed because refreshing search result can be expensive step = 0.1 , title = \"Similarity threshold\" , ) def _search_watch_widgets ( self ): return [ self . search_sim , self . search_threshold ] def _subroutine_search_create_callbacks ( self ): \"\"\" ???+ note \"Create search callback functions based on feature attributes.\" \"\"\" # determine cache size for normalized vectorizer num_points = sum ([ _df . shape [ 0 ] for _df in self . dfs . values ()]) cache_size = min ( num_points , int ( 1e5 )) # find vectorizer assert hasattr ( self , \"linked_dataset\" ), \"need linked_dataset for its vectorizer\" found_vectorizers = self . linked_dataset . vectorizer_lookup . values () assert len ( found_vectorizers ) > 0 , \"dataset has no known vectorizer\" raw_vectorizer = list ( found_vectorizers )[ 0 ] # gain speed up by caching and normalization @lru_cache ( maxsize = cache_size ) def normalized_vectorizer ( feature ): try : vec = raw_vectorizer ( feature ) except Exception as e : self . _warn ( f \"vectorizer crashed: { e } ; assigning None as vector.\" ) return None norm = np . linalg . norm ( vec ) return vec / ( norm + 1e-16 ) self . _dynamic_resources [ \"normalized_vectorizer\" ] = normalized_vectorizer super () . _subroutine_search_create_callbacks () def _get_search_score_function ( self ): \"\"\" ???+ note \"Dynamically create a single-argument scoring function.\" \"\"\" vectorizer = self . _dynamic_resources [ \"normalized_vectorizer\" ] url_query = self . search_sim . value sim_thresh = self . search_threshold . value vec_query = vectorizer ( url_query ) def cosine_based_score ( url_doc ): # edge case: query or doc is invalid for vectorization vec_doc = vectorizer ( url_doc ) if vec_query is None or vec_doc is None : return 0 # common case: query and doc are both valid query_doc_sim = ( np . dot ( vec_query , vec_doc ) + 1.0 ) / 2.0 if query_doc_sim >= sim_thresh : return 1 else : return - 1 return cosine_based_score def _validate_search_input ( self ): \"\"\" ???+ note \"Must be some url pointing to a suffixed file.\" For speed, avoid sending web requests in this validation step. \"\"\" from urllib.parse import urlparse from pathlib import Path url_query = self . search_sim . value file_path = Path ( urlparse ( url_query ) . path ) return bool ( file_path . suffix )","title":".feature"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForAudio","text":"BokehForUrlToVector with audio (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) audio urls in an audio column Does not assume: what the explorer serves to do. Source code in hover/core/explorer/feature.py class BokehForAudio ( BokehForUrlToVector ): \"\"\" ???+ note \"`BokehForUrlToVector` with `audio` (path like `\"http://\"` or `\"file:///\"`) as the main feature.\" Assumes on top of its parent class: - in supplied dataframes - (always) audio urls in an `audio` column Does not assume: - what the explorer serves to do. \"\"\" PRIMARY_FEATURE = \"audio\" MANDATORY_COLUMNS = [ PRIMARY_FEATURE , \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"audio\" : True , \"coords\" : True , \"index\" : True }","title":"BokehForAudio"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForImage","text":"BokehForUrlToVector with image (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) image urls in an image column Does not assume: what the explorer serves to do. Source code in hover/core/explorer/feature.py class BokehForImage ( BokehForUrlToVector ): \"\"\" ???+ note \"`BokehForUrlToVector` with `image` (path like `\"http://\"` or `\"file:///\"`) as the main feature.\" Assumes on top of its parent class: - in supplied dataframes - (always) image urls in an `image` column Does not assume: - what the explorer serves to do. \"\"\" PRIMARY_FEATURE = \"image\" MANDATORY_COLUMNS = [ PRIMARY_FEATURE , \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"image\" : True , \"coords\" : True , \"index\" : True }","title":"BokehForImage"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForText","text":"BokehBaseExplorer with text ( str ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) text data in a text column Does not assume: what the explorer serves to do. Source code in hover/core/explorer/feature.py class BokehForText ( BokehBaseExplorer ): \"\"\" ???+ note \"`BokehBaseExplorer` with `text` (`str`) as the main feature.\" Assumes on top of its parent class: - in supplied dataframes - (always) text data in a `text` column Does not assume: - what the explorer serves to do. \"\"\" PRIMARY_FEATURE = \"text\" MANDATORY_COLUMNS = [ PRIMARY_FEATURE , \"label\" ] TOOLTIP_KWARGS = { \"label\" : True , \"text\" : True , \"coords\" : True , \"index\" : True } def _setup_search_widgets ( self ): \"\"\" ???+ note \"Create positive/negative text search boxes.\" \"\"\" common_kwargs = dict ( width_policy = \"fit\" , height_policy = \"fit\" ) pos_title , neg_title = \"Text contains (python regex):\" , \"Text does not contain:\" self . search_pos = TextInput ( title = pos_title , ** common_kwargs ) self . search_neg = TextInput ( title = neg_title , ** common_kwargs ) def _search_watch_widgets ( self ): return [ self . search_pos , self . search_neg ] def _validate_search_input ( self ): \"\"\" ???+ note \"Text uses regex search, for which any string can be considered valid.\" \"\"\" return True def _get_search_score_function ( self ): \"\"\" ???+ note \"Dynamically create a single-argument scoring function.\" \"\"\" pos_regex , neg_regex = self . search_pos . value , self . search_neg . value def regex_score ( text ): score = 0 if len ( pos_regex ) > 0 : score += 1 if re . search ( pos_regex , text ) else - 2 if len ( neg_regex ) > 0 : score += - 2 if re . search ( neg_regex , text ) else 1 return score return regex_score","title":"BokehForText"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForUrlToVector","text":"A layer of abstraction for BokehBaseExplorer subclasses whose feature-type-specific mechanisms work the same way through vectors. Source code in hover/core/explorer/feature.py class BokehForUrlToVector ( BokehBaseExplorer ): \"\"\" ???+ note \"A layer of abstraction for `BokehBaseExplorer` subclasses whose feature-type-specific mechanisms work the same way through vectors.\" \"\"\" def _setup_search_widgets ( self ): \"\"\" ???+ note \"Create similarity search widgets.\" \"\"\" self . search_sim = TextInput ( title = f \" { self . __class__ . PRIMARY_FEATURE } similarity search (enter URL)\" . capitalize (), width_policy = \"fit\" , height_policy = \"fit\" , ) self . search_threshold = Slider ( start = 0.0 , end = 1.0 , value = 0.9 , # fewer steps allowed because refreshing search result can be expensive step = 0.1 , title = \"Similarity threshold\" , ) def _search_watch_widgets ( self ): return [ self . search_sim , self . search_threshold ] def _subroutine_search_create_callbacks ( self ): \"\"\" ???+ note \"Create search callback functions based on feature attributes.\" \"\"\" # determine cache size for normalized vectorizer num_points = sum ([ _df . shape [ 0 ] for _df in self . dfs . values ()]) cache_size = min ( num_points , int ( 1e5 )) # find vectorizer assert hasattr ( self , \"linked_dataset\" ), \"need linked_dataset for its vectorizer\" found_vectorizers = self . linked_dataset . vectorizer_lookup . values () assert len ( found_vectorizers ) > 0 , \"dataset has no known vectorizer\" raw_vectorizer = list ( found_vectorizers )[ 0 ] # gain speed up by caching and normalization @lru_cache ( maxsize = cache_size ) def normalized_vectorizer ( feature ): try : vec = raw_vectorizer ( feature ) except Exception as e : self . _warn ( f \"vectorizer crashed: { e } ; assigning None as vector.\" ) return None norm = np . linalg . norm ( vec ) return vec / ( norm + 1e-16 ) self . _dynamic_resources [ \"normalized_vectorizer\" ] = normalized_vectorizer super () . _subroutine_search_create_callbacks () def _get_search_score_function ( self ): \"\"\" ???+ note \"Dynamically create a single-argument scoring function.\" \"\"\" vectorizer = self . _dynamic_resources [ \"normalized_vectorizer\" ] url_query = self . search_sim . value sim_thresh = self . search_threshold . value vec_query = vectorizer ( url_query ) def cosine_based_score ( url_doc ): # edge case: query or doc is invalid for vectorization vec_doc = vectorizer ( url_doc ) if vec_query is None or vec_doc is None : return 0 # common case: query and doc are both valid query_doc_sim = ( np . dot ( vec_query , vec_doc ) + 1.0 ) / 2.0 if query_doc_sim >= sim_thresh : return 1 else : return - 1 return cosine_based_score def _validate_search_input ( self ): \"\"\" ???+ note \"Must be some url pointing to a suffixed file.\" For speed, avoid sending web requests in this validation step. \"\"\" from urllib.parse import urlparse from pathlib import Path url_query = self . search_sim . value file_path = Path ( urlparse ( url_query ) . path ) return bool ( file_path . suffix )","title":"BokehForUrlToVector"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature","text":"","title":"hover.core.explorer.feature"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForAudio","text":"","title":"BokehForAudio"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForImage","text":"","title":"BokehForImage"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForText","text":"","title":"BokehForText"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForUrlToVector","text":"","title":"BokehForUrlToVector"},{"location":"pages/reference/core-explorer-functionality/","text":"Intermediate classes based on the functionality. BokehDataAnnotator ( BokehBaseExplorer ) Annoate data points via callbacks on the buttons. Features: alter values in the 'label' column through the widgets. Source code in hover/core/explorer/functionality.py class BokehDataAnnotator ( BokehBaseExplorer ): \"\"\" ???+ note \"Annoate data points via callbacks on the buttons.\" Features: - alter values in the 'label' column through the widgets. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"line_alpha\" : 0.3 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 ), \"fill_alpha\" : ( \"fill_alpha\" , 0.5 , 0.1 , 0.4 ), }, } for _key in [ \"raw\" , \"train\" , \"dev\" , \"test\" ] } def _postprocess_sources ( self ): \"\"\" ???+ note \"Infer glyph colors from the label dynamically.\" This is during initialization or re-plotting, creating a new attribute column for each data source. \"\"\" color_dict = self . auto_color_mapping () for _key , _df in self . dfs . items (): _color = ( _df [ \"label\" ] . apply ( lambda label : color_dict . get ( label , \"gainsboro\" )) . tolist () ) self . sources [ _key ] . add ( _color , SOURCE_COLOR_FIELD ) def _update_colors ( self ): \"\"\" ???+ note \"Infer glyph colors from the label dynamically.\" This is during annotation callbacks, patching an existing column for the `raw` subset only. \"\"\" # infer glyph colors dynamically color_dict = self . auto_color_mapping () color_list = ( self . dfs [ \"raw\" ][ \"label\" ] . apply ( lambda label : color_dict . get ( label , \"gainsboro\" )) . tolist () ) self . sources [ \"raw\" ] . patch ( { SOURCE_COLOR_FIELD : [( slice ( len ( color_list )), color_list )]} ) self . _good ( f \"Updated annotator plot at { current_time () } \" ) def _setup_widgets ( self ): \"\"\" ???+ note \"Create annotator widgets and assign Python callbacks.\" \"\"\" from bokeh.models import TextInput super () . _setup_widgets () self . annotator_input = TextInput ( title = \"Label:\" ) self . annotator_apply = Button ( label = \"Apply\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def callback_apply (): \"\"\" A callback on clicking the 'self.annotator_apply' button. Update labels in the source. \"\"\" label = self . annotator_input . value selected_idx = self . sources [ \"raw\" ] . selected . indices if not selected_idx : self . _warn ( \"attempting annotation: did not select any data points. Eligible subset is 'raw'.\" ) return self . _info ( f \"applying { len ( selected_idx ) } annotations...\" ) # update label in both the df and the data source self . dfs [ \"raw\" ] . loc [ selected_idx , \"label\" ] = label patch_to_apply = [( _idx , label ) for _idx in selected_idx ] self . sources [ \"raw\" ] . patch ({ \"label\" : patch_to_apply }) self . _good ( f \"applied { len ( selected_idx ) } annotations: { label } \" ) self . _update_colors () # assign the callback and keep the reference self . _callback_apply = callback_apply self . annotator_apply . on_click ( self . _callback_apply ) self . annotator_apply . on_click ( self . _callback_subset_display ) def plot ( self ): \"\"\" ???+ note \"Re-plot all data points with the new labels.\" Overrides the parent method. Determines the label -> color mapping dynamically. \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , color = SOURCE_COLOR_FIELD , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) plot ( self ) Re-plot all data points with the new labels. Overrides the parent method. Determines the label -> color mapping dynamically. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Re-plot all data points with the new labels.\" Overrides the parent method. Determines the label -> color mapping dynamically. \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , color = SOURCE_COLOR_FIELD , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) BokehDataFinder ( BokehBaseExplorer ) Plot data points in grey ('gainsboro') and highlight search positives in coral. Features: the search widgets will highlight the results through a change of color. the search results can be used as a filter condition. Source code in hover/core/explorer/functionality.py class BokehDataFinder ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points in grey ('gainsboro') and highlight search positives in coral.\" Features: - the search widgets will highlight the results through a change of color. - the search results can be used as a filter condition. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"line_alpha\" : 0.4 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 ), \"fill_alpha\" : ( \"fill_alpha\" , 0.4 , 0.1 , 0.2 ), \"color\" : ( \"color\" , \"coral\" , \"linen\" , \"gainsboro\" ), }, } for _key in [ \"raw\" , \"train\" , \"dev\" , \"test\" ] } def _setup_widgets ( self ): \"\"\" ???+ note \"Create score range slider that filters selections.\" \"\"\" from bokeh.models import CheckboxGroup super () . _setup_widgets () self . search_filter_box = CheckboxGroup ( labels = [ \"use as selection filter\" ], active = [] ) def _subroutine_search_activate_callbacks ( self ): \"\"\" ???+ note \"Activate search callback functions by binding them to widgets.\" \"\"\" super () . _subroutine_search_activate_callbacks () def filter_flag (): return bool ( 0 in self . search_filter_box . active ) def filter_by_search ( indices , subset ): \"\"\" Filter selection with search results on a subset. \"\"\" if not filter_flag (): return indices search_scores = self . sources [ subset ] . data [ SEARCH_SCORE_FIELD ] matched = set ( np . where ( np . array ( search_scores ) > 0 )[ 0 ]) return indices . intersection ( matched ) for _key in self . sources . keys (): self . _selection_filters [ _key ] . data . add ( filter_by_search ) # when toggled as active, search changes trigger selection filter for _widget in self . _search_watch_widgets (): _widget . on_change ( \"value\" , lambda attr , old , new : self . _trigger_selection_filters () if filter_flag () else None , ) # active toggles always trigger selection filter self . search_filter_box . on_change ( \"active\" , lambda attr , old , new : self . _trigger_selection_filters () ) def plot ( self ): \"\"\" ???+ note \"Plot all data points.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) plot ( self ) Plot all data points. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Plot all data points.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) BokehMarginExplorer ( BokehBaseExplorer ) Plot data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios. Source code in hover/core/explorer/functionality.py class BokehMarginExplorer ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points along with two versions of labels.\" Could be useful for A/B tests. Features: - can choose to only plot the margins about specific labels. - currently not considering multi-label scenarios. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"color\" : \"gainsboro\" , \"line_alpha\" : 0.5 , \"fill_alpha\" : 0.0 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 )}, } for _key in [ \"raw\" , \"train\" , \"dev\" ] } DEFAULT_SUBSET_MAPPING = { _k : _k for _k in [ \"raw\" , \"train\" , \"dev\" ]} def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col_a and label_col_b for \"label margins\". | Param | Type | Description | | :------------ | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col_a` | `str` | column for label set A | | `label_col_b` | `str` | column for label set B | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs ) def _mandatory_column_defaults ( self ): \"\"\" ???+ note \"Mandatory columns and default values.\" If default value is None, will raise exception if the column is not found. \"\"\" column_to_value = super () . _mandatory_column_defaults () column_to_value . update ( { self . label_col_a : None , self . label_col_b : None , } ) return column_to_value def plot ( self , label , ** kwargs ): \"\"\" ???+ note \"Plot the margins about a single label.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `label` | | the label to plot about | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( * xy_axes , name = _key , source = _source , view = _view , ** eff_kwargs ) __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ) special Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col_a and label_col_b for \"label margins\". Param Type Description df_dict dict str -> DataFrame mapping label_col_a str column for label set A label_col_b str column for label set B **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col_a and label_col_b for \"label margins\". | Param | Type | Description | | :------------ | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col_a` | `str` | column for label set A | | `label_col_b` | `str` | column for label set B | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs ) plot ( self , label , ** kwargs ) Plot the margins about a single label. Param Type Description label the label to plot about **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , label , ** kwargs ): \"\"\" ???+ note \"Plot the margins about a single label.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `label` | | the label to plot about | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( * xy_axes , name = _key , source = _source , view = _view , ** eff_kwargs ) BokehSnorkelExplorer ( BokehBaseExplorer ) Plot data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set. Source code in hover/core/explorer/functionality.py class BokehSnorkelExplorer ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points along with labeling function (LF) outputs.\" Features: - each labeling function corresponds to its own line_color. - uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. - 'correct': the LF made a correct prediction on a point in the 'labeled' set. - 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. - 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. - 'hit': the LF made a prediction on a point in the 'raw' set. \"\"\" SUBSET_GLYPH_KWARGS = { \"raw\" : { \"constant\" : { \"line_alpha\" : 1.0 , \"color\" : \"gainsboro\" }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 ), \"fill_alpha\" : ( \"fill_alpha\" , 0.4 , 0.05 , 0.2 ), }, }, \"labeled\" : { \"constant\" : { \"line_alpha\" : 1.0 , \"fill_alpha\" : 0.0 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 )}, }, } DEFAULT_SUBSET_MAPPING = { \"raw\" : \"raw\" , \"dev\" : \"labeled\" } def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" Set up - a list to keep track of plotted labeling functions. - a palette for plotting labeling function predictions. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" super () . __init__ ( df_dict , ** kwargs ) self . palette = list ( Category20 [ 20 ]) self . _subscribed_lf_list = None def _setup_sources ( self ): \"\"\" ???+ note \"Create data structures that source interactions will need.\" \"\"\" # keep track of plotted LFs and glyphs, which will interact with sources self . lf_data = OrderedDict () super () . _setup_sources () @property def subscribed_lf_list ( self ): \"\"\" ???+ note \"A list of LFs to which the explorer can be lazily synchronized.\" Intended for recipes where the user can modify LFs without having access to the explorer. \"\"\" return self . _subscribed_lf_list @subscribed_lf_list . setter def subscribed_lf_list ( self , lf_list ): \"\"\" ???+ note \"Subscribe to a list of LFs.\" \"\"\" assert isinstance ( lf_list , list ), f \"Expected a list of LFs, got { lf_list } \" if self . subscribed_lf_list is None : self . _good ( \"Subscribed to a labeling function list BY REFERENCE.\" ) else : self . _warn ( \"Changing labeling function list subscription.\" ) self . _subscribed_lf_list = lf_list self . _callback_refresh_lf_menu () def _setup_widgets ( self ): \"\"\" ???+ note \"Create labeling function support widgets and assign Python callbacks.\" \"\"\" super () . _setup_widgets () self . _subroutine_setup_lf_list_refresher () self . _subroutine_setup_lf_apply_trigger () self . _subroutine_setup_lf_filter_trigger () def _subroutine_setup_lf_list_refresher ( self ): \"\"\" ???+ note \"Create widget for refreshing LF list and replotting.\" \"\"\" self . lf_list_refresher = Button ( label = \"Refresh Functions\" , height_policy = \"fit\" , width_policy = \"min\" , ) def callback_refresh_lf_plot (): \"\"\" Re-plot according to subscribed_lf_list. \"\"\" if self . subscribed_lf_list is None : self . _warn ( \"cannot refresh LF plot without subscribed LF list.\" ) return lf_names_to_keep = set ([ _lf . name for _lf in self . subscribed_lf_list ]) lf_names_to_drop = set ( self . lf_data . keys ()) . difference ( lf_names_to_keep ) for _lf_name in lf_names_to_drop : self . unplot_lf ( _lf_name ) for _lf in self . subscribed_lf_list : self . plot_lf ( _lf ) def callback_refresh_lf_menu (): \"\"\" The menu was assigned by value and needs to stay consistent with LF updates. To be triggered in self.plot_new_lf() and self.unplot_lf(). \"\"\" self . lf_apply_trigger . menu = list ( self . lf_data . keys ()) self . lf_filter_trigger . menu = list ( self . lf_data . keys ()) self . _callback_refresh_lf_menu = callback_refresh_lf_menu self . lf_list_refresher . on_click ( callback_refresh_lf_plot ) # self.lf_list_refresher.on_click(callback_refresh_lf_menu) def _subroutine_setup_lf_apply_trigger ( self ): \"\"\" ???+ note \"Create widget for applying LFs on data.\" \"\"\" self . lf_apply_trigger = Dropdown ( label = \"Apply Labels\" , button_type = \"warning\" , menu = list ( self . lf_data . keys ()), height_policy = \"fit\" , width_policy = \"min\" , ) def callback_apply ( event ): \"\"\" A callback on clicking the 'self.lf_apply_trigger' button. Update labels in the source similarly to the annotator. However, in this explorer, because LFs already use color, the produced labels will not. \"\"\" lf = self . lf_data [ event . item ][ \"lf\" ] assert callable ( lf ), f \"Expected a function, got { lf } \" selected_idx = self . sources [ \"raw\" ] . selected . indices if not selected_idx : self . _warn ( \"attempting labeling by function: did not select any data points. Eligible subset is 'raw'.\" ) return labels = self . dfs [ \"raw\" ] . iloc [ selected_idx ] . apply ( lf , axis = 1 ) . values num_nontrivial = len ( list ( filter ( lambda l : l != module_config . ABSTAIN_DECODED , labels )) ) # update label in both the df and the data source self . dfs [ \"raw\" ] . loc [ selected_idx , \"label\" ] = labels for _idx , _label in zip ( selected_idx , labels ): _idx = int ( _idx ) self . sources [ \"raw\" ] . patch ({ \"label\" : [( _idx , _label )]}) self . _info ( f \"applied { num_nontrivial } / { len ( labels ) } annotations by func { lf . name } \" ) self . lf_apply_trigger . on_click ( callback_apply ) def _subroutine_setup_lf_filter_trigger ( self ): \"\"\" ???+ note \"Create widget for using LFs to filter data.\" \"\"\" self . lf_filter_trigger = Dropdown ( label = \"Use as Selection Filter\" , button_type = \"primary\" , menu = list ( self . lf_data . keys ()), height_policy = \"fit\" , width_policy = \"min\" , ) def callback_filter ( event ): \"\"\" A callback on clicking the 'self.lf_filter_trigger' button. Update selected indices in a one-time manner. \"\"\" lf = self . lf_data [ event . item ][ \"lf\" ] assert callable ( lf ), f \"Expected a function, got { lf } \" for _key , _source in self . sources . items (): _selected = _source . selected . indices _labels = self . dfs [ _key ] . iloc [ _selected ] . apply ( lf , axis = 1 ) . values _kept = [ _idx for _idx , _label in zip ( _selected , _labels ) if _label != module_config . ABSTAIN_DECODED ] self . sources [ _key ] . selected . indices = _kept self . lf_filter_trigger . on_click ( callback_filter ) def _postprocess_sources ( self ): \"\"\" ???+ note \"Refresh all LF glyphs because data source has changed.\" \"\"\" for _lf_name in self . lf_data . keys (): self . refresh_glyphs ( _lf_name ) def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot the raw subset in the background.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] self . figure . circle ( * xy_axes , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" ) def plot_lf ( self , lf , ** kwargs ): \"\"\" ???+ note \"Add or refresh a single labeling function on the plot.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `**kwargs` | | forwarded to `self.plot_new_lf()` | \"\"\" # keep track of added LF if lf . name in self . lf_data : # skip if the functions are identical if self . lf_data [ lf . name ][ \"lf\" ] is lf : return # overwrite the function and refresh glyphs self . lf_data [ lf . name ][ \"lf\" ] = lf self . refresh_glyphs ( lf . name ) return self . plot_new_lf ( lf , ** kwargs ) def unplot_lf ( self , lf_name ): \"\"\" ???+ note \"Remove a single labeling function from the plot.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | \"\"\" assert lf_name in self . lf_data , f \"trying to remove non-existing LF: { lf_name } \" data_dict = self . lf_data . pop ( lf_name ) lf , glyph_dict = data_dict [ \"lf\" ], data_dict [ \"glyphs\" ] assert lf . name == lf_name , f \"LF name mismatch: { lf . name } vs { lf_name } \" # remove from legend, checking that there is exactly one entry legend_idx_to_pop = None for i , _item in enumerate ( self . figure . legend . items ): _label = _item . label . get ( \"value\" , \"\" ) if _label == lf_name : assert legend_idx_to_pop is None , f \"Legend collision: { lf_name } \" legend_idx_to_pop = i assert isinstance ( legend_idx_to_pop , int ), f \"Missing from legend: { lf_name } \" self . figure . legend . items . pop ( legend_idx_to_pop ) # remove from renderers # get indices to pop in ascending order renderer_indices_to_pop = [] for i , _renderer in enumerate ( self . figure . renderers ): if lf_name in _renderer . glyph . tags : renderer_indices_to_pop . append ( i ) # check that the number of glyphs founded matches expected value num_fnd , num_exp = len ( renderer_indices_to_pop ), len ( glyph_dict ) assert num_fnd == num_exp , f \"Glyph mismatch: { num_fnd } vs. { num_exp } \" # process indices in descending order to avoid shifts for i in renderer_indices_to_pop [:: - 1 ]: self . figure . renderers . pop ( i ) # return color to palette so that another LF can use it self . palette . append ( data_dict [ \"color\" ]) self . _callback_refresh_lf_menu () self . _good ( f \"Unplotted LF { lf_name } \" ) def refresh_glyphs ( self , lf_name ): \"\"\" ???+ note \"Refresh the glyph(s) of a single LF based on its name.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | Assumes that specified C/I/M/H glyphs are stored. 1. re-compute L_raw/L_labeled and CDSViews 2. update the view for each glyph \"\"\" assert lf_name in self . lf_data , f \"trying to refresh non-existing LF: { lf_name } \" lf = self . lf_data [ lf_name ][ \"lf\" ] L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values glyph_codes = self . lf_data [ lf_name ][ \"glyphs\" ] . keys () if \"C\" in glyph_codes : c_view = self . _view_correct ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"C\" ] . view = c_view if \"I\" in glyph_codes : i_view = self . _view_incorrect ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"I\" ] . view = i_view if \"M\" in glyph_codes : m_view = self . _view_missed ( L_labeled , lf . targets ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"M\" ] . view = m_view if \"H\" in glyph_codes : h_view = self . _view_hit ( L_raw ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"H\" ] . view = h_view self . _good ( f \"Refreshed the glyphs of LF { lf_name } \" ) def plot_new_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" ???+ note \"Plot a single labeling function and keep its settings for update.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `L_raw` | `np.ndarray` | predictions, in decoded `str`, on the `\"raw\"` set | | `L_labeled` | `np.ndarray` | predictions, in decoded `str`, on the `\"labeled\"` set | | `include` | `tuple` of `str` | \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot | | `**kwargs` | | forwarded to plotting markers | - lf: labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` - L_raw: numpy.ndarray - L_labeled: numpy.ndarray - include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # existing LF should not trigger this method assert lf . name not in self . lf_data , f \"LF collision: { lf . name } \" # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings assert self . palette , f \"Palette depleted, # LFs: { len ( self . lf_data ) } \" legend_label = lf . name color = self . palette . pop ( 0 ) xy_axes = self . find_embedding_fields ()[: 2 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create dictionary to prepare for dynamic lf & glyph updates data_dict = { \"lf\" : lf , \"color\" : color , \"glyphs\" : {}} # add correct/incorrect/missed/hit glyphs if \"C\" in include : view = self . _view_correct ( L_labeled ) data_dict [ \"glyphs\" ][ \"C\" ] = self . figure . square ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"I\" in include : view = self . _view_incorrect ( L_labeled ) data_dict [ \"glyphs\" ][ \"I\" ] = self . figure . x ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"M\" in include : view = self . _view_missed ( L_labeled , lf . targets ) data_dict [ \"glyphs\" ][ \"M\" ] = self . figure . cross ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"H\" in include : view = self . _view_hit ( L_raw ) data_dict [ \"glyphs\" ][ \"H\" ] = self . figure . circle ( * xy_axes , source = view . source , view = view , name = \"raw\" , tags = [ lf . name ], ** raw_glyph_kwargs , ) # assign the completed dictionary self . lf_data [ lf . name ] = data_dict # reflect LF update in widgets self . _callback_refresh_lf_menu () self . _good ( f \"Plotted new LF { lf . name } \" ) def _view_correct ( self , L_labeled ): \"\"\" ???+ note \"Determine the portion correctly labeled by a labeling function.\" | Param | Type | Description | | :---------- | :----------- | :--------------------------- | | `L_labeled` | `np.ndarray` | predictions on the labeled subset | \"\"\" if L_labeled . shape [ 0 ] == 0 : indices = [] else : agreed = self . dfs [ \"labeled\" ][ \"label\" ] . values == L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( agreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view def _view_incorrect ( self , L_labeled ): \"\"\" ???+ note \"Determine the portion incorrectly labeled by a labeling function.\" | Param | Type | Description | | :---------- | :----------- | :--------------------------- | | `L_labeled` | `np.ndarray` | predictions on the labeled subset | \"\"\" if L_labeled . shape [ 0 ] == 0 : indices = [] else : disagreed = self . dfs [ \"labeled\" ][ \"label\" ] . values != L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( disagreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view def _view_missed ( self , L_labeled , targets ): \"\"\" ???+ note \"Determine the portion missed by a labeling function.\" | Param | Type | Description | | :---------- | :------------ | :--------------------------- | | `L_labeled` | `np.ndarray` | predictions on the labeled subset | | `targets` | `list` of `str` | labels that the function aims for | \"\"\" if L_labeled . shape [ 0 ] == 0 : indices = [] else : targetable = np . isin ( self . dfs [ \"labeled\" ][ \"label\" ], targets ) abstained = L_labeled == module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( targetable , abstained ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view def _view_hit ( self , L_raw ): \"\"\" ???+ note \"Determine the portion hit by a labeling function.\" | Param | Type | Description | | :---------- | :----------- | :--------------------------- | | `L_raw` | `np.ndarray` | predictions on the raw subset | \"\"\" if L_raw . shape [ 0 ] == 0 : indices = [] else : indices = np . where ( L_raw != module_config . ABSTAIN_DECODED )[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( indices )]) return view subscribed_lf_list property writable A list of LFs to which the explorer can be lazily synchronized. Intended for recipes where the user can modify LFs without having access to the explorer. __init__ ( self , df_dict , ** kwargs ) special Additional construtor Set up a list to keep track of plotted labeling functions. a palette for plotting labeling function predictions. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" Set up - a list to keep track of plotted labeling functions. - a palette for plotting labeling function predictions. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" super () . __init__ ( df_dict , ** kwargs ) self . palette = list ( Category20 [ 20 ]) self . _subscribed_lf_list = None plot ( self , * args , ** kwargs ) Plot the raw subset in the background. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot the raw subset in the background.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] self . figure . circle ( * xy_axes , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" ) plot_lf ( self , lf , ** kwargs ) Add or refresh a single labeling function on the plot. Param Type Description lf callable labeling function decorated by @labeling_function() from hover.utils.snorkel_helper **kwargs forwarded to self.plot_new_lf() Source code in hover/core/explorer/functionality.py def plot_lf ( self , lf , ** kwargs ): \"\"\" ???+ note \"Add or refresh a single labeling function on the plot.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `**kwargs` | | forwarded to `self.plot_new_lf()` | \"\"\" # keep track of added LF if lf . name in self . lf_data : # skip if the functions are identical if self . lf_data [ lf . name ][ \"lf\" ] is lf : return # overwrite the function and refresh glyphs self . lf_data [ lf . name ][ \"lf\" ] = lf self . refresh_glyphs ( lf . name ) return self . plot_new_lf ( lf , ** kwargs ) plot_new_lf ( self , lf , L_raw = None , L_labeled = None , include = ( 'C' , 'I' , 'M' ), ** kwargs ) Plot a single labeling function and keep its settings for update. Param Type Description lf callable labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw np.ndarray predictions, in decoded str , on the \"raw\" set L_labeled np.ndarray predictions, in decoded str , on the \"labeled\" set include tuple of str \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot **kwargs forwarded to plotting markers lf: labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw: numpy.ndarray L_labeled: numpy.ndarray include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer/functionality.py def plot_new_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" ???+ note \"Plot a single labeling function and keep its settings for update.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `L_raw` | `np.ndarray` | predictions, in decoded `str`, on the `\"raw\"` set | | `L_labeled` | `np.ndarray` | predictions, in decoded `str`, on the `\"labeled\"` set | | `include` | `tuple` of `str` | \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot | | `**kwargs` | | forwarded to plotting markers | - lf: labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` - L_raw: numpy.ndarray - L_labeled: numpy.ndarray - include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # existing LF should not trigger this method assert lf . name not in self . lf_data , f \"LF collision: { lf . name } \" # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings assert self . palette , f \"Palette depleted, # LFs: { len ( self . lf_data ) } \" legend_label = lf . name color = self . palette . pop ( 0 ) xy_axes = self . find_embedding_fields ()[: 2 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create dictionary to prepare for dynamic lf & glyph updates data_dict = { \"lf\" : lf , \"color\" : color , \"glyphs\" : {}} # add correct/incorrect/missed/hit glyphs if \"C\" in include : view = self . _view_correct ( L_labeled ) data_dict [ \"glyphs\" ][ \"C\" ] = self . figure . square ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"I\" in include : view = self . _view_incorrect ( L_labeled ) data_dict [ \"glyphs\" ][ \"I\" ] = self . figure . x ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"M\" in include : view = self . _view_missed ( L_labeled , lf . targets ) data_dict [ \"glyphs\" ][ \"M\" ] = self . figure . cross ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"H\" in include : view = self . _view_hit ( L_raw ) data_dict [ \"glyphs\" ][ \"H\" ] = self . figure . circle ( * xy_axes , source = view . source , view = view , name = \"raw\" , tags = [ lf . name ], ** raw_glyph_kwargs , ) # assign the completed dictionary self . lf_data [ lf . name ] = data_dict # reflect LF update in widgets self . _callback_refresh_lf_menu () self . _good ( f \"Plotted new LF { lf . name } \" ) refresh_glyphs ( self , lf_name ) Refresh the glyph(s) of a single LF based on its name. Param Type Description lf_name str name of labeling function Assumes that specified C/I/M/H glyphs are stored. 1. re-compute L_raw/L_labeled and CDSViews 2. update the view for each glyph Source code in hover/core/explorer/functionality.py def refresh_glyphs ( self , lf_name ): \"\"\" ???+ note \"Refresh the glyph(s) of a single LF based on its name.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | Assumes that specified C/I/M/H glyphs are stored. 1. re-compute L_raw/L_labeled and CDSViews 2. update the view for each glyph \"\"\" assert lf_name in self . lf_data , f \"trying to refresh non-existing LF: { lf_name } \" lf = self . lf_data [ lf_name ][ \"lf\" ] L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values glyph_codes = self . lf_data [ lf_name ][ \"glyphs\" ] . keys () if \"C\" in glyph_codes : c_view = self . _view_correct ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"C\" ] . view = c_view if \"I\" in glyph_codes : i_view = self . _view_incorrect ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"I\" ] . view = i_view if \"M\" in glyph_codes : m_view = self . _view_missed ( L_labeled , lf . targets ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"M\" ] . view = m_view if \"H\" in glyph_codes : h_view = self . _view_hit ( L_raw ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"H\" ] . view = h_view self . _good ( f \"Refreshed the glyphs of LF { lf_name } \" ) unplot_lf ( self , lf_name ) Remove a single labeling function from the plot. Param Type Description lf_name str name of labeling function Source code in hover/core/explorer/functionality.py def unplot_lf ( self , lf_name ): \"\"\" ???+ note \"Remove a single labeling function from the plot.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | \"\"\" assert lf_name in self . lf_data , f \"trying to remove non-existing LF: { lf_name } \" data_dict = self . lf_data . pop ( lf_name ) lf , glyph_dict = data_dict [ \"lf\" ], data_dict [ \"glyphs\" ] assert lf . name == lf_name , f \"LF name mismatch: { lf . name } vs { lf_name } \" # remove from legend, checking that there is exactly one entry legend_idx_to_pop = None for i , _item in enumerate ( self . figure . legend . items ): _label = _item . label . get ( \"value\" , \"\" ) if _label == lf_name : assert legend_idx_to_pop is None , f \"Legend collision: { lf_name } \" legend_idx_to_pop = i assert isinstance ( legend_idx_to_pop , int ), f \"Missing from legend: { lf_name } \" self . figure . legend . items . pop ( legend_idx_to_pop ) # remove from renderers # get indices to pop in ascending order renderer_indices_to_pop = [] for i , _renderer in enumerate ( self . figure . renderers ): if lf_name in _renderer . glyph . tags : renderer_indices_to_pop . append ( i ) # check that the number of glyphs founded matches expected value num_fnd , num_exp = len ( renderer_indices_to_pop ), len ( glyph_dict ) assert num_fnd == num_exp , f \"Glyph mismatch: { num_fnd } vs. { num_exp } \" # process indices in descending order to avoid shifts for i in renderer_indices_to_pop [:: - 1 ]: self . figure . renderers . pop ( i ) # return color to palette so that another LF can use it self . palette . append ( data_dict [ \"color\" ]) self . _callback_refresh_lf_menu () self . _good ( f \"Unplotted LF { lf_name } \" ) BokehSoftLabelExplorer ( BokehBaseExplorer ) Plot data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios. Source code in hover/core/explorer/functionality.py class BokehSoftLabelExplorer ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points according to their labels and confidence scores.\" Features: - the predicted label will correspond to fill_color. - the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. - currently not considering multi-label scenarios. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"line_alpha\" : 0.5 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 )}} for _key in [ \"raw\" , \"train\" , \"dev\" ] } DEFAULT_SUBSET_MAPPING = { _k : _k for _k in [ \"raw\" , \"train\" , \"dev\" ]} def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col and score_col for \"soft predictions\". | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col` | `str` | column for the soft label | | `score_col` | `str` | column for the soft score | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs ) def _build_tooltip ( self , extra ): \"\"\" ???+ note \"On top of the parent method, add the soft label fields to the tooltip.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `extra` | `str` | user-supplied extra HTML | \"\"\" standard = bokeh_hover_tooltip ( ** self . __class__ . TOOLTIP_KWARGS , custom = { \"Soft Label\" : self . label_col , \"Soft Score\" : self . score_col }, ) return f \" { standard } \\n { extra } \" def _mandatory_column_defaults ( self ): \"\"\" ???+ note \"Mandatory columns and default values.\" If default value is None, will raise exception if the column is not found. \"\"\" column_to_value = super () . _mandatory_column_defaults () column_to_value . update ( { self . label_col : module_config . ABSTAIN_DECODED , self . score_col : 0.5 , } ) return column_to_value def _postprocess_sources ( self ): \"\"\" ???+ note \"Infer glyph colors from the label dynamically.\" \"\"\" # infer glyph color from labels color_dict = self . auto_color_mapping () def get_color ( label ): return color_dict . get ( label , \"gainsboro\" ) # infer glyph alpha from pseudo-percentile of soft label scores scores = np . concatenate ( [ _df [ self . score_col ] . tolist () for _df in self . dfs . values ()] ) scores_mean = scores . mean () scores_std = scores . std () + 1e-4 def pseudo_percentile ( confidence , lower = 0.1 , upper = 0.9 ): # pretend that 2*std on each side covers everything unit_shift = upper - lower / 4 # shift = unit_shift * z_score shift = unit_shift * ( confidence - scores_mean ) / scores_std percentile = 0.5 + shift return min ( upper , max ( lower , percentile )) # infer alpha from score percentiles for _key , _df in self . dfs . items (): _color = _df [ self . label_col ] . apply ( get_color ) . tolist () _alpha = _df [ self . score_col ] . apply ( pseudo_percentile ) . tolist () self . sources [ _key ] . add ( _color , SOURCE_COLOR_FIELD ) self . sources [ _key ] . add ( _alpha , SOURCE_ALPHA_FIELD ) def _setup_widgets ( self ): \"\"\" ???+ note \"Create score range slider that filters selections.\" \"\"\" from bokeh.models import RangeSlider , CheckboxGroup super () . _setup_widgets () self . score_range = RangeSlider ( start = 0.0 , end = 1.0 , value = ( 0.0 , 1.0 ), step = 0.01 , title = \"Score range\" , ) self . score_filter_box = CheckboxGroup ( labels = [ \"use as selection filter\" ], active = [] ) self . score_filter = row ( self . score_range , self . score_filter_box ) def filter_flag (): return bool ( 0 in self . score_filter_box . active ) def subroutine ( df , lower , upper ): \"\"\" Calculate indices with score between lower/upper bounds. \"\"\" keep_l = set ( np . where ( df [ self . score_col ] >= lower )[ 0 ]) keep_u = set ( np . where ( df [ self . score_col ] <= upper )[ 0 ]) kept = keep_l . intersection ( keep_u ) return kept def filter_by_score ( indices , subset ): \"\"\" Filter selection with slider range on a subset. \"\"\" if not filter_flag (): return indices in_range = subroutine ( self . dfs [ subset ], * self . score_range . value ) return indices . intersection ( in_range ) # selection change triggers score filter on the changed subset IFF filter box is toggled for _key in self . sources . keys (): self . _selection_filters [ _key ] . data . add ( filter_by_score ) # when toggled as active, score range change triggers selection filter self . score_range . on_change ( \"value\" , lambda attr , old , new : self . _trigger_selection_filters () if filter_flag () else None , ) # active toggles always trigger selection filter self . score_filter_box . on_change ( \"active\" , lambda attr , old , new : self . _trigger_selection_filters () ) def plot ( self , ** kwargs ): \"\"\" ???+ note \"Plot all data points, setting color alpha based on the soft score.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"color\" : SOURCE_COLOR_FIELD , \"fill_alpha\" : SOURCE_ALPHA_FIELD , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( * xy_axes , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) __init__ ( self , df_dict , label_col , score_col , ** kwargs ) special Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col and score_col for \"soft predictions\". Param Type Description df_dict dict str -> DataFrame mapping label_col str column for the soft label score_col str column for the soft score **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col and score_col for \"soft predictions\". | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col` | `str` | column for the soft label | | `score_col` | `str` | column for the soft score | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs ) plot ( self , ** kwargs ) Plot all data points, setting color alpha based on the soft score. Param Type Description **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , ** kwargs ): \"\"\" ???+ note \"Plot all data points, setting color alpha based on the soft score.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"color\" : SOURCE_COLOR_FIELD , \"fill_alpha\" : SOURCE_ALPHA_FIELD , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( * xy_axes , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":".functionality"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator","text":"Annoate data points via callbacks on the buttons. Features: alter values in the 'label' column through the widgets. Source code in hover/core/explorer/functionality.py class BokehDataAnnotator ( BokehBaseExplorer ): \"\"\" ???+ note \"Annoate data points via callbacks on the buttons.\" Features: - alter values in the 'label' column through the widgets. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"line_alpha\" : 0.3 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 ), \"fill_alpha\" : ( \"fill_alpha\" , 0.5 , 0.1 , 0.4 ), }, } for _key in [ \"raw\" , \"train\" , \"dev\" , \"test\" ] } def _postprocess_sources ( self ): \"\"\" ???+ note \"Infer glyph colors from the label dynamically.\" This is during initialization or re-plotting, creating a new attribute column for each data source. \"\"\" color_dict = self . auto_color_mapping () for _key , _df in self . dfs . items (): _color = ( _df [ \"label\" ] . apply ( lambda label : color_dict . get ( label , \"gainsboro\" )) . tolist () ) self . sources [ _key ] . add ( _color , SOURCE_COLOR_FIELD ) def _update_colors ( self ): \"\"\" ???+ note \"Infer glyph colors from the label dynamically.\" This is during annotation callbacks, patching an existing column for the `raw` subset only. \"\"\" # infer glyph colors dynamically color_dict = self . auto_color_mapping () color_list = ( self . dfs [ \"raw\" ][ \"label\" ] . apply ( lambda label : color_dict . get ( label , \"gainsboro\" )) . tolist () ) self . sources [ \"raw\" ] . patch ( { SOURCE_COLOR_FIELD : [( slice ( len ( color_list )), color_list )]} ) self . _good ( f \"Updated annotator plot at { current_time () } \" ) def _setup_widgets ( self ): \"\"\" ???+ note \"Create annotator widgets and assign Python callbacks.\" \"\"\" from bokeh.models import TextInput super () . _setup_widgets () self . annotator_input = TextInput ( title = \"Label:\" ) self . annotator_apply = Button ( label = \"Apply\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def callback_apply (): \"\"\" A callback on clicking the 'self.annotator_apply' button. Update labels in the source. \"\"\" label = self . annotator_input . value selected_idx = self . sources [ \"raw\" ] . selected . indices if not selected_idx : self . _warn ( \"attempting annotation: did not select any data points. Eligible subset is 'raw'.\" ) return self . _info ( f \"applying { len ( selected_idx ) } annotations...\" ) # update label in both the df and the data source self . dfs [ \"raw\" ] . loc [ selected_idx , \"label\" ] = label patch_to_apply = [( _idx , label ) for _idx in selected_idx ] self . sources [ \"raw\" ] . patch ({ \"label\" : patch_to_apply }) self . _good ( f \"applied { len ( selected_idx ) } annotations: { label } \" ) self . _update_colors () # assign the callback and keep the reference self . _callback_apply = callback_apply self . annotator_apply . on_click ( self . _callback_apply ) self . annotator_apply . on_click ( self . _callback_subset_display ) def plot ( self ): \"\"\" ???+ note \"Re-plot all data points with the new labels.\" Overrides the parent method. Determines the label -> color mapping dynamically. \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , color = SOURCE_COLOR_FIELD , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"BokehDataAnnotator"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator.plot","text":"Re-plot all data points with the new labels. Overrides the parent method. Determines the label -> color mapping dynamically. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Re-plot all data points with the new labels.\" Overrides the parent method. Determines the label -> color mapping dynamically. \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , color = SOURCE_COLOR_FIELD , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder","text":"Plot data points in grey ('gainsboro') and highlight search positives in coral. Features: the search widgets will highlight the results through a change of color. the search results can be used as a filter condition. Source code in hover/core/explorer/functionality.py class BokehDataFinder ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points in grey ('gainsboro') and highlight search positives in coral.\" Features: - the search widgets will highlight the results through a change of color. - the search results can be used as a filter condition. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"line_alpha\" : 0.4 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 ), \"fill_alpha\" : ( \"fill_alpha\" , 0.4 , 0.1 , 0.2 ), \"color\" : ( \"color\" , \"coral\" , \"linen\" , \"gainsboro\" ), }, } for _key in [ \"raw\" , \"train\" , \"dev\" , \"test\" ] } def _setup_widgets ( self ): \"\"\" ???+ note \"Create score range slider that filters selections.\" \"\"\" from bokeh.models import CheckboxGroup super () . _setup_widgets () self . search_filter_box = CheckboxGroup ( labels = [ \"use as selection filter\" ], active = [] ) def _subroutine_search_activate_callbacks ( self ): \"\"\" ???+ note \"Activate search callback functions by binding them to widgets.\" \"\"\" super () . _subroutine_search_activate_callbacks () def filter_flag (): return bool ( 0 in self . search_filter_box . active ) def filter_by_search ( indices , subset ): \"\"\" Filter selection with search results on a subset. \"\"\" if not filter_flag (): return indices search_scores = self . sources [ subset ] . data [ SEARCH_SCORE_FIELD ] matched = set ( np . where ( np . array ( search_scores ) > 0 )[ 0 ]) return indices . intersection ( matched ) for _key in self . sources . keys (): self . _selection_filters [ _key ] . data . add ( filter_by_search ) # when toggled as active, search changes trigger selection filter for _widget in self . _search_watch_widgets (): _widget . on_change ( \"value\" , lambda attr , old , new : self . _trigger_selection_filters () if filter_flag () else None , ) # active toggles always trigger selection filter self . search_filter_box . on_change ( \"active\" , lambda attr , old , new : self . _trigger_selection_filters () ) def plot ( self ): \"\"\" ???+ note \"Plot all data points.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"BokehDataFinder"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder.plot","text":"Plot all data points. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Plot all data points.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): self . figure . circle ( * xy_axes , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer","text":"Plot data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios. Source code in hover/core/explorer/functionality.py class BokehMarginExplorer ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points along with two versions of labels.\" Could be useful for A/B tests. Features: - can choose to only plot the margins about specific labels. - currently not considering multi-label scenarios. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"color\" : \"gainsboro\" , \"line_alpha\" : 0.5 , \"fill_alpha\" : 0.0 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 )}, } for _key in [ \"raw\" , \"train\" , \"dev\" ] } DEFAULT_SUBSET_MAPPING = { _k : _k for _k in [ \"raw\" , \"train\" , \"dev\" ]} def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col_a and label_col_b for \"label margins\". | Param | Type | Description | | :------------ | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col_a` | `str` | column for label set A | | `label_col_b` | `str` | column for label set B | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs ) def _mandatory_column_defaults ( self ): \"\"\" ???+ note \"Mandatory columns and default values.\" If default value is None, will raise exception if the column is not found. \"\"\" column_to_value = super () . _mandatory_column_defaults () column_to_value . update ( { self . label_col_a : None , self . label_col_b : None , } ) return column_to_value def plot ( self , label , ** kwargs ): \"\"\" ???+ note \"Plot the margins about a single label.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `label` | | the label to plot about | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( * xy_axes , name = _key , source = _source , view = _view , ** eff_kwargs )","title":"BokehMarginExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.__init__","text":"Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col_a and label_col_b for \"label margins\". Param Type Description df_dict dict str -> DataFrame mapping label_col_a str column for label set A label_col_b str column for label set B **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col_a and label_col_b for \"label margins\". | Param | Type | Description | | :------------ | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col_a` | `str` | column for label set A | | `label_col_b` | `str` | column for label set B | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.plot","text":"Plot the margins about a single label. Param Type Description label the label to plot about **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , label , ** kwargs ): \"\"\" ???+ note \"Plot the margins about a single label.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `label` | | the label to plot about | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( * xy_axes , name = _key , source = _source , view = _view , ** eff_kwargs )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer","text":"Plot data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set. Source code in hover/core/explorer/functionality.py class BokehSnorkelExplorer ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points along with labeling function (LF) outputs.\" Features: - each labeling function corresponds to its own line_color. - uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. - 'correct': the LF made a correct prediction on a point in the 'labeled' set. - 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. - 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. - 'hit': the LF made a prediction on a point in the 'raw' set. \"\"\" SUBSET_GLYPH_KWARGS = { \"raw\" : { \"constant\" : { \"line_alpha\" : 1.0 , \"color\" : \"gainsboro\" }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 ), \"fill_alpha\" : ( \"fill_alpha\" , 0.4 , 0.05 , 0.2 ), }, }, \"labeled\" : { \"constant\" : { \"line_alpha\" : 1.0 , \"fill_alpha\" : 0.0 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 )}, }, } DEFAULT_SUBSET_MAPPING = { \"raw\" : \"raw\" , \"dev\" : \"labeled\" } def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" Set up - a list to keep track of plotted labeling functions. - a palette for plotting labeling function predictions. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" super () . __init__ ( df_dict , ** kwargs ) self . palette = list ( Category20 [ 20 ]) self . _subscribed_lf_list = None def _setup_sources ( self ): \"\"\" ???+ note \"Create data structures that source interactions will need.\" \"\"\" # keep track of plotted LFs and glyphs, which will interact with sources self . lf_data = OrderedDict () super () . _setup_sources () @property def subscribed_lf_list ( self ): \"\"\" ???+ note \"A list of LFs to which the explorer can be lazily synchronized.\" Intended for recipes where the user can modify LFs without having access to the explorer. \"\"\" return self . _subscribed_lf_list @subscribed_lf_list . setter def subscribed_lf_list ( self , lf_list ): \"\"\" ???+ note \"Subscribe to a list of LFs.\" \"\"\" assert isinstance ( lf_list , list ), f \"Expected a list of LFs, got { lf_list } \" if self . subscribed_lf_list is None : self . _good ( \"Subscribed to a labeling function list BY REFERENCE.\" ) else : self . _warn ( \"Changing labeling function list subscription.\" ) self . _subscribed_lf_list = lf_list self . _callback_refresh_lf_menu () def _setup_widgets ( self ): \"\"\" ???+ note \"Create labeling function support widgets and assign Python callbacks.\" \"\"\" super () . _setup_widgets () self . _subroutine_setup_lf_list_refresher () self . _subroutine_setup_lf_apply_trigger () self . _subroutine_setup_lf_filter_trigger () def _subroutine_setup_lf_list_refresher ( self ): \"\"\" ???+ note \"Create widget for refreshing LF list and replotting.\" \"\"\" self . lf_list_refresher = Button ( label = \"Refresh Functions\" , height_policy = \"fit\" , width_policy = \"min\" , ) def callback_refresh_lf_plot (): \"\"\" Re-plot according to subscribed_lf_list. \"\"\" if self . subscribed_lf_list is None : self . _warn ( \"cannot refresh LF plot without subscribed LF list.\" ) return lf_names_to_keep = set ([ _lf . name for _lf in self . subscribed_lf_list ]) lf_names_to_drop = set ( self . lf_data . keys ()) . difference ( lf_names_to_keep ) for _lf_name in lf_names_to_drop : self . unplot_lf ( _lf_name ) for _lf in self . subscribed_lf_list : self . plot_lf ( _lf ) def callback_refresh_lf_menu (): \"\"\" The menu was assigned by value and needs to stay consistent with LF updates. To be triggered in self.plot_new_lf() and self.unplot_lf(). \"\"\" self . lf_apply_trigger . menu = list ( self . lf_data . keys ()) self . lf_filter_trigger . menu = list ( self . lf_data . keys ()) self . _callback_refresh_lf_menu = callback_refresh_lf_menu self . lf_list_refresher . on_click ( callback_refresh_lf_plot ) # self.lf_list_refresher.on_click(callback_refresh_lf_menu) def _subroutine_setup_lf_apply_trigger ( self ): \"\"\" ???+ note \"Create widget for applying LFs on data.\" \"\"\" self . lf_apply_trigger = Dropdown ( label = \"Apply Labels\" , button_type = \"warning\" , menu = list ( self . lf_data . keys ()), height_policy = \"fit\" , width_policy = \"min\" , ) def callback_apply ( event ): \"\"\" A callback on clicking the 'self.lf_apply_trigger' button. Update labels in the source similarly to the annotator. However, in this explorer, because LFs already use color, the produced labels will not. \"\"\" lf = self . lf_data [ event . item ][ \"lf\" ] assert callable ( lf ), f \"Expected a function, got { lf } \" selected_idx = self . sources [ \"raw\" ] . selected . indices if not selected_idx : self . _warn ( \"attempting labeling by function: did not select any data points. Eligible subset is 'raw'.\" ) return labels = self . dfs [ \"raw\" ] . iloc [ selected_idx ] . apply ( lf , axis = 1 ) . values num_nontrivial = len ( list ( filter ( lambda l : l != module_config . ABSTAIN_DECODED , labels )) ) # update label in both the df and the data source self . dfs [ \"raw\" ] . loc [ selected_idx , \"label\" ] = labels for _idx , _label in zip ( selected_idx , labels ): _idx = int ( _idx ) self . sources [ \"raw\" ] . patch ({ \"label\" : [( _idx , _label )]}) self . _info ( f \"applied { num_nontrivial } / { len ( labels ) } annotations by func { lf . name } \" ) self . lf_apply_trigger . on_click ( callback_apply ) def _subroutine_setup_lf_filter_trigger ( self ): \"\"\" ???+ note \"Create widget for using LFs to filter data.\" \"\"\" self . lf_filter_trigger = Dropdown ( label = \"Use as Selection Filter\" , button_type = \"primary\" , menu = list ( self . lf_data . keys ()), height_policy = \"fit\" , width_policy = \"min\" , ) def callback_filter ( event ): \"\"\" A callback on clicking the 'self.lf_filter_trigger' button. Update selected indices in a one-time manner. \"\"\" lf = self . lf_data [ event . item ][ \"lf\" ] assert callable ( lf ), f \"Expected a function, got { lf } \" for _key , _source in self . sources . items (): _selected = _source . selected . indices _labels = self . dfs [ _key ] . iloc [ _selected ] . apply ( lf , axis = 1 ) . values _kept = [ _idx for _idx , _label in zip ( _selected , _labels ) if _label != module_config . ABSTAIN_DECODED ] self . sources [ _key ] . selected . indices = _kept self . lf_filter_trigger . on_click ( callback_filter ) def _postprocess_sources ( self ): \"\"\" ???+ note \"Refresh all LF glyphs because data source has changed.\" \"\"\" for _lf_name in self . lf_data . keys (): self . refresh_glyphs ( _lf_name ) def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot the raw subset in the background.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] self . figure . circle ( * xy_axes , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" ) def plot_lf ( self , lf , ** kwargs ): \"\"\" ???+ note \"Add or refresh a single labeling function on the plot.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `**kwargs` | | forwarded to `self.plot_new_lf()` | \"\"\" # keep track of added LF if lf . name in self . lf_data : # skip if the functions are identical if self . lf_data [ lf . name ][ \"lf\" ] is lf : return # overwrite the function and refresh glyphs self . lf_data [ lf . name ][ \"lf\" ] = lf self . refresh_glyphs ( lf . name ) return self . plot_new_lf ( lf , ** kwargs ) def unplot_lf ( self , lf_name ): \"\"\" ???+ note \"Remove a single labeling function from the plot.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | \"\"\" assert lf_name in self . lf_data , f \"trying to remove non-existing LF: { lf_name } \" data_dict = self . lf_data . pop ( lf_name ) lf , glyph_dict = data_dict [ \"lf\" ], data_dict [ \"glyphs\" ] assert lf . name == lf_name , f \"LF name mismatch: { lf . name } vs { lf_name } \" # remove from legend, checking that there is exactly one entry legend_idx_to_pop = None for i , _item in enumerate ( self . figure . legend . items ): _label = _item . label . get ( \"value\" , \"\" ) if _label == lf_name : assert legend_idx_to_pop is None , f \"Legend collision: { lf_name } \" legend_idx_to_pop = i assert isinstance ( legend_idx_to_pop , int ), f \"Missing from legend: { lf_name } \" self . figure . legend . items . pop ( legend_idx_to_pop ) # remove from renderers # get indices to pop in ascending order renderer_indices_to_pop = [] for i , _renderer in enumerate ( self . figure . renderers ): if lf_name in _renderer . glyph . tags : renderer_indices_to_pop . append ( i ) # check that the number of glyphs founded matches expected value num_fnd , num_exp = len ( renderer_indices_to_pop ), len ( glyph_dict ) assert num_fnd == num_exp , f \"Glyph mismatch: { num_fnd } vs. { num_exp } \" # process indices in descending order to avoid shifts for i in renderer_indices_to_pop [:: - 1 ]: self . figure . renderers . pop ( i ) # return color to palette so that another LF can use it self . palette . append ( data_dict [ \"color\" ]) self . _callback_refresh_lf_menu () self . _good ( f \"Unplotted LF { lf_name } \" ) def refresh_glyphs ( self , lf_name ): \"\"\" ???+ note \"Refresh the glyph(s) of a single LF based on its name.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | Assumes that specified C/I/M/H glyphs are stored. 1. re-compute L_raw/L_labeled and CDSViews 2. update the view for each glyph \"\"\" assert lf_name in self . lf_data , f \"trying to refresh non-existing LF: { lf_name } \" lf = self . lf_data [ lf_name ][ \"lf\" ] L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values glyph_codes = self . lf_data [ lf_name ][ \"glyphs\" ] . keys () if \"C\" in glyph_codes : c_view = self . _view_correct ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"C\" ] . view = c_view if \"I\" in glyph_codes : i_view = self . _view_incorrect ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"I\" ] . view = i_view if \"M\" in glyph_codes : m_view = self . _view_missed ( L_labeled , lf . targets ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"M\" ] . view = m_view if \"H\" in glyph_codes : h_view = self . _view_hit ( L_raw ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"H\" ] . view = h_view self . _good ( f \"Refreshed the glyphs of LF { lf_name } \" ) def plot_new_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" ???+ note \"Plot a single labeling function and keep its settings for update.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `L_raw` | `np.ndarray` | predictions, in decoded `str`, on the `\"raw\"` set | | `L_labeled` | `np.ndarray` | predictions, in decoded `str`, on the `\"labeled\"` set | | `include` | `tuple` of `str` | \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot | | `**kwargs` | | forwarded to plotting markers | - lf: labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` - L_raw: numpy.ndarray - L_labeled: numpy.ndarray - include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # existing LF should not trigger this method assert lf . name not in self . lf_data , f \"LF collision: { lf . name } \" # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings assert self . palette , f \"Palette depleted, # LFs: { len ( self . lf_data ) } \" legend_label = lf . name color = self . palette . pop ( 0 ) xy_axes = self . find_embedding_fields ()[: 2 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create dictionary to prepare for dynamic lf & glyph updates data_dict = { \"lf\" : lf , \"color\" : color , \"glyphs\" : {}} # add correct/incorrect/missed/hit glyphs if \"C\" in include : view = self . _view_correct ( L_labeled ) data_dict [ \"glyphs\" ][ \"C\" ] = self . figure . square ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"I\" in include : view = self . _view_incorrect ( L_labeled ) data_dict [ \"glyphs\" ][ \"I\" ] = self . figure . x ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"M\" in include : view = self . _view_missed ( L_labeled , lf . targets ) data_dict [ \"glyphs\" ][ \"M\" ] = self . figure . cross ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"H\" in include : view = self . _view_hit ( L_raw ) data_dict [ \"glyphs\" ][ \"H\" ] = self . figure . circle ( * xy_axes , source = view . source , view = view , name = \"raw\" , tags = [ lf . name ], ** raw_glyph_kwargs , ) # assign the completed dictionary self . lf_data [ lf . name ] = data_dict # reflect LF update in widgets self . _callback_refresh_lf_menu () self . _good ( f \"Plotted new LF { lf . name } \" ) def _view_correct ( self , L_labeled ): \"\"\" ???+ note \"Determine the portion correctly labeled by a labeling function.\" | Param | Type | Description | | :---------- | :----------- | :--------------------------- | | `L_labeled` | `np.ndarray` | predictions on the labeled subset | \"\"\" if L_labeled . shape [ 0 ] == 0 : indices = [] else : agreed = self . dfs [ \"labeled\" ][ \"label\" ] . values == L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( agreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view def _view_incorrect ( self , L_labeled ): \"\"\" ???+ note \"Determine the portion incorrectly labeled by a labeling function.\" | Param | Type | Description | | :---------- | :----------- | :--------------------------- | | `L_labeled` | `np.ndarray` | predictions on the labeled subset | \"\"\" if L_labeled . shape [ 0 ] == 0 : indices = [] else : disagreed = self . dfs [ \"labeled\" ][ \"label\" ] . values != L_labeled attempted = L_labeled != module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( disagreed , attempted ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view def _view_missed ( self , L_labeled , targets ): \"\"\" ???+ note \"Determine the portion missed by a labeling function.\" | Param | Type | Description | | :---------- | :------------ | :--------------------------- | | `L_labeled` | `np.ndarray` | predictions on the labeled subset | | `targets` | `list` of `str` | labels that the function aims for | \"\"\" if L_labeled . shape [ 0 ] == 0 : indices = [] else : targetable = np . isin ( self . dfs [ \"labeled\" ][ \"label\" ], targets ) abstained = L_labeled == module_config . ABSTAIN_DECODED indices = np . where ( np . multiply ( targetable , abstained ))[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"labeled\" ], filters = [ IndexFilter ( indices )]) return view def _view_hit ( self , L_raw ): \"\"\" ???+ note \"Determine the portion hit by a labeling function.\" | Param | Type | Description | | :---------- | :----------- | :--------------------------- | | `L_raw` | `np.ndarray` | predictions on the raw subset | \"\"\" if L_raw . shape [ 0 ] == 0 : indices = [] else : indices = np . where ( L_raw != module_config . ABSTAIN_DECODED )[ 0 ] . tolist () view = CDSView ( source = self . sources [ \"raw\" ], filters = [ IndexFilter ( indices )]) return view","title":"BokehSnorkelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.subscribed_lf_list","text":"A list of LFs to which the explorer can be lazily synchronized. Intended for recipes where the user can modify LFs without having access to the explorer.","title":"subscribed_lf_list"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.__init__","text":"Additional construtor Set up a list to keep track of plotted labeling functions. a palette for plotting labeling function predictions. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" Set up - a list to keep track of plotted labeling functions. - a palette for plotting labeling function predictions. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" super () . __init__ ( df_dict , ** kwargs ) self . palette = list ( Category20 [ 20 ]) self . _subscribed_lf_list = None","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot","text":"Plot the raw subset in the background. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot the raw subset in the background.\" \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] self . figure . circle ( * xy_axes , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_lf","text":"Add or refresh a single labeling function on the plot. Param Type Description lf callable labeling function decorated by @labeling_function() from hover.utils.snorkel_helper **kwargs forwarded to self.plot_new_lf() Source code in hover/core/explorer/functionality.py def plot_lf ( self , lf , ** kwargs ): \"\"\" ???+ note \"Add or refresh a single labeling function on the plot.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `**kwargs` | | forwarded to `self.plot_new_lf()` | \"\"\" # keep track of added LF if lf . name in self . lf_data : # skip if the functions are identical if self . lf_data [ lf . name ][ \"lf\" ] is lf : return # overwrite the function and refresh glyphs self . lf_data [ lf . name ][ \"lf\" ] = lf self . refresh_glyphs ( lf . name ) return self . plot_new_lf ( lf , ** kwargs )","title":"plot_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_new_lf","text":"Plot a single labeling function and keep its settings for update. Param Type Description lf callable labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw np.ndarray predictions, in decoded str , on the \"raw\" set L_labeled np.ndarray predictions, in decoded str , on the \"labeled\" set include tuple of str \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot **kwargs forwarded to plotting markers lf: labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw: numpy.ndarray L_labeled: numpy.ndarray include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer/functionality.py def plot_new_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" ???+ note \"Plot a single labeling function and keep its settings for update.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `L_raw` | `np.ndarray` | predictions, in decoded `str`, on the `\"raw\"` set | | `L_labeled` | `np.ndarray` | predictions, in decoded `str`, on the `\"labeled\"` set | | `include` | `tuple` of `str` | \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot | | `**kwargs` | | forwarded to plotting markers | - lf: labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` - L_raw: numpy.ndarray - L_labeled: numpy.ndarray - include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # existing LF should not trigger this method assert lf . name not in self . lf_data , f \"LF collision: { lf . name } \" # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings assert self . palette , f \"Palette depleted, # LFs: { len ( self . lf_data ) } \" legend_label = lf . name color = self . palette . pop ( 0 ) xy_axes = self . find_embedding_fields ()[: 2 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create dictionary to prepare for dynamic lf & glyph updates data_dict = { \"lf\" : lf , \"color\" : color , \"glyphs\" : {}} # add correct/incorrect/missed/hit glyphs if \"C\" in include : view = self . _view_correct ( L_labeled ) data_dict [ \"glyphs\" ][ \"C\" ] = self . figure . square ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"I\" in include : view = self . _view_incorrect ( L_labeled ) data_dict [ \"glyphs\" ][ \"I\" ] = self . figure . x ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"M\" in include : view = self . _view_missed ( L_labeled , lf . targets ) data_dict [ \"glyphs\" ][ \"M\" ] = self . figure . cross ( * xy_axes , source = view . source , view = view , name = \"labeled\" , tags = [ lf . name ], ** labeled_glyph_kwargs , ) if \"H\" in include : view = self . _view_hit ( L_raw ) data_dict [ \"glyphs\" ][ \"H\" ] = self . figure . circle ( * xy_axes , source = view . source , view = view , name = \"raw\" , tags = [ lf . name ], ** raw_glyph_kwargs , ) # assign the completed dictionary self . lf_data [ lf . name ] = data_dict # reflect LF update in widgets self . _callback_refresh_lf_menu () self . _good ( f \"Plotted new LF { lf . name } \" )","title":"plot_new_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.refresh_glyphs","text":"Refresh the glyph(s) of a single LF based on its name. Param Type Description lf_name str name of labeling function Assumes that specified C/I/M/H glyphs are stored. 1. re-compute L_raw/L_labeled and CDSViews 2. update the view for each glyph Source code in hover/core/explorer/functionality.py def refresh_glyphs ( self , lf_name ): \"\"\" ???+ note \"Refresh the glyph(s) of a single LF based on its name.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | Assumes that specified C/I/M/H glyphs are stored. 1. re-compute L_raw/L_labeled and CDSViews 2. update the view for each glyph \"\"\" assert lf_name in self . lf_data , f \"trying to refresh non-existing LF: { lf_name } \" lf = self . lf_data [ lf_name ][ \"lf\" ] L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values glyph_codes = self . lf_data [ lf_name ][ \"glyphs\" ] . keys () if \"C\" in glyph_codes : c_view = self . _view_correct ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"C\" ] . view = c_view if \"I\" in glyph_codes : i_view = self . _view_incorrect ( L_labeled ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"I\" ] . view = i_view if \"M\" in glyph_codes : m_view = self . _view_missed ( L_labeled , lf . targets ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"M\" ] . view = m_view if \"H\" in glyph_codes : h_view = self . _view_hit ( L_raw ) self . lf_data [ lf_name ][ \"glyphs\" ][ \"H\" ] . view = h_view self . _good ( f \"Refreshed the glyphs of LF { lf_name } \" )","title":"refresh_glyphs()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.unplot_lf","text":"Remove a single labeling function from the plot. Param Type Description lf_name str name of labeling function Source code in hover/core/explorer/functionality.py def unplot_lf ( self , lf_name ): \"\"\" ???+ note \"Remove a single labeling function from the plot.\" | Param | Type | Description | | :-------- | :----- | :------------------------ | | `lf_name` | `str` | name of labeling function | \"\"\" assert lf_name in self . lf_data , f \"trying to remove non-existing LF: { lf_name } \" data_dict = self . lf_data . pop ( lf_name ) lf , glyph_dict = data_dict [ \"lf\" ], data_dict [ \"glyphs\" ] assert lf . name == lf_name , f \"LF name mismatch: { lf . name } vs { lf_name } \" # remove from legend, checking that there is exactly one entry legend_idx_to_pop = None for i , _item in enumerate ( self . figure . legend . items ): _label = _item . label . get ( \"value\" , \"\" ) if _label == lf_name : assert legend_idx_to_pop is None , f \"Legend collision: { lf_name } \" legend_idx_to_pop = i assert isinstance ( legend_idx_to_pop , int ), f \"Missing from legend: { lf_name } \" self . figure . legend . items . pop ( legend_idx_to_pop ) # remove from renderers # get indices to pop in ascending order renderer_indices_to_pop = [] for i , _renderer in enumerate ( self . figure . renderers ): if lf_name in _renderer . glyph . tags : renderer_indices_to_pop . append ( i ) # check that the number of glyphs founded matches expected value num_fnd , num_exp = len ( renderer_indices_to_pop ), len ( glyph_dict ) assert num_fnd == num_exp , f \"Glyph mismatch: { num_fnd } vs. { num_exp } \" # process indices in descending order to avoid shifts for i in renderer_indices_to_pop [:: - 1 ]: self . figure . renderers . pop ( i ) # return color to palette so that another LF can use it self . palette . append ( data_dict [ \"color\" ]) self . _callback_refresh_lf_menu () self . _good ( f \"Unplotted LF { lf_name } \" )","title":"unplot_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer","text":"Plot data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios. Source code in hover/core/explorer/functionality.py class BokehSoftLabelExplorer ( BokehBaseExplorer ): \"\"\" ???+ note \"Plot data points according to their labels and confidence scores.\" Features: - the predicted label will correspond to fill_color. - the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. - currently not considering multi-label scenarios. \"\"\" SUBSET_GLYPH_KWARGS = { _key : { \"constant\" : { \"line_alpha\" : 0.5 }, \"search\" : { \"size\" : ( \"size\" , 10 , 5 , 7 )}} for _key in [ \"raw\" , \"train\" , \"dev\" ] } DEFAULT_SUBSET_MAPPING = { _k : _k for _k in [ \"raw\" , \"train\" , \"dev\" ]} def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col and score_col for \"soft predictions\". | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col` | `str` | column for the soft label | | `score_col` | `str` | column for the soft score | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs ) def _build_tooltip ( self , extra ): \"\"\" ???+ note \"On top of the parent method, add the soft label fields to the tooltip.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `extra` | `str` | user-supplied extra HTML | \"\"\" standard = bokeh_hover_tooltip ( ** self . __class__ . TOOLTIP_KWARGS , custom = { \"Soft Label\" : self . label_col , \"Soft Score\" : self . score_col }, ) return f \" { standard } \\n { extra } \" def _mandatory_column_defaults ( self ): \"\"\" ???+ note \"Mandatory columns and default values.\" If default value is None, will raise exception if the column is not found. \"\"\" column_to_value = super () . _mandatory_column_defaults () column_to_value . update ( { self . label_col : module_config . ABSTAIN_DECODED , self . score_col : 0.5 , } ) return column_to_value def _postprocess_sources ( self ): \"\"\" ???+ note \"Infer glyph colors from the label dynamically.\" \"\"\" # infer glyph color from labels color_dict = self . auto_color_mapping () def get_color ( label ): return color_dict . get ( label , \"gainsboro\" ) # infer glyph alpha from pseudo-percentile of soft label scores scores = np . concatenate ( [ _df [ self . score_col ] . tolist () for _df in self . dfs . values ()] ) scores_mean = scores . mean () scores_std = scores . std () + 1e-4 def pseudo_percentile ( confidence , lower = 0.1 , upper = 0.9 ): # pretend that 2*std on each side covers everything unit_shift = upper - lower / 4 # shift = unit_shift * z_score shift = unit_shift * ( confidence - scores_mean ) / scores_std percentile = 0.5 + shift return min ( upper , max ( lower , percentile )) # infer alpha from score percentiles for _key , _df in self . dfs . items (): _color = _df [ self . label_col ] . apply ( get_color ) . tolist () _alpha = _df [ self . score_col ] . apply ( pseudo_percentile ) . tolist () self . sources [ _key ] . add ( _color , SOURCE_COLOR_FIELD ) self . sources [ _key ] . add ( _alpha , SOURCE_ALPHA_FIELD ) def _setup_widgets ( self ): \"\"\" ???+ note \"Create score range slider that filters selections.\" \"\"\" from bokeh.models import RangeSlider , CheckboxGroup super () . _setup_widgets () self . score_range = RangeSlider ( start = 0.0 , end = 1.0 , value = ( 0.0 , 1.0 ), step = 0.01 , title = \"Score range\" , ) self . score_filter_box = CheckboxGroup ( labels = [ \"use as selection filter\" ], active = [] ) self . score_filter = row ( self . score_range , self . score_filter_box ) def filter_flag (): return bool ( 0 in self . score_filter_box . active ) def subroutine ( df , lower , upper ): \"\"\" Calculate indices with score between lower/upper bounds. \"\"\" keep_l = set ( np . where ( df [ self . score_col ] >= lower )[ 0 ]) keep_u = set ( np . where ( df [ self . score_col ] <= upper )[ 0 ]) kept = keep_l . intersection ( keep_u ) return kept def filter_by_score ( indices , subset ): \"\"\" Filter selection with slider range on a subset. \"\"\" if not filter_flag (): return indices in_range = subroutine ( self . dfs [ subset ], * self . score_range . value ) return indices . intersection ( in_range ) # selection change triggers score filter on the changed subset IFF filter box is toggled for _key in self . sources . keys (): self . _selection_filters [ _key ] . data . add ( filter_by_score ) # when toggled as active, score range change triggers selection filter self . score_range . on_change ( \"value\" , lambda attr , old , new : self . _trigger_selection_filters () if filter_flag () else None , ) # active toggles always trigger selection filter self . score_filter_box . on_change ( \"active\" , lambda attr , old , new : self . _trigger_selection_filters () ) def plot ( self , ** kwargs ): \"\"\" ???+ note \"Plot all data points, setting color alpha based on the soft score.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"color\" : SOURCE_COLOR_FIELD , \"fill_alpha\" : SOURCE_ALPHA_FIELD , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( * xy_axes , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"BokehSoftLabelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.__init__","text":"Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col and score_col for \"soft predictions\". Param Type Description df_dict dict str -> DataFrame mapping label_col str column for the soft label score_col str column for the soft score **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col and score_col for \"soft predictions\". | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col` | `str` | column for the soft label | | `score_col` | `str` | column for the soft score | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.plot","text":"Plot all data points, setting color alpha based on the soft score. Param Type Description **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , ** kwargs ): \"\"\" ???+ note \"Plot all data points, setting color alpha based on the soft score.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to plotting markers | \"\"\" xy_axes = self . find_embedding_fields ()[: 2 ] for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"color\" : SOURCE_COLOR_FIELD , \"fill_alpha\" : SOURCE_ALPHA_FIELD , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( * xy_axes , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality","text":"","title":"hover.core.explorer.functionality"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator","text":"","title":"BokehDataAnnotator"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder","text":"","title":"BokehDataFinder"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer","text":"","title":"BokehMarginExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer","text":"","title":"BokehSnorkelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.subscribed_lf_list","text":"","title":"subscribed_lf_list"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_lf","text":"","title":"plot_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_new_lf","text":"","title":"plot_new_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.refresh_glyphs","text":"","title":"refresh_glyphs()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.unplot_lf","text":"","title":"unplot_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer","text":"","title":"BokehSoftLabelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-specialization/","text":"Child classes which are functionality -by- feature products. This could resemble template specialization in C++. BokehAudioAnnotator ( BokehDataAnnotator , BokehForAudio ) The audio flavor of BokehDataAnnotator .\" Source code in hover/core/explorer/specialization.py class BokehAudioAnnotator ( BokehDataAnnotator , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehDataAnnotator`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataAnnotator . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . annotator_input , self . annotator_apply ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehAudioFinder ( BokehDataFinder , BokehForAudio ) The audio flavor of BokehDataFinder .\" Source code in hover/core/explorer/specialization.py class BokehAudioFinder ( BokehDataFinder , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehDataFinder`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataFinder . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( column ( self . search_sim , self . search_threshold ), column ( self . search_filter_box ), ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehAudioMargin ( BokehMarginExplorer , BokehForAudio ) The audio flavor of BokehMarginExplorer .\" Source code in hover/core/explorer/specialization.py class BokehAudioMargin ( BokehMarginExplorer , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehMarginExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehMarginExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehAudioSnorkel ( BokehSnorkelExplorer , BokehForAudio ) The audio flavor of BokehSnorkelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehAudioSnorkel ( BokehSnorkelExplorer , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehSnorkelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSnorkelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . lf_apply_trigger , self . lf_filter_trigger , self . lf_list_refresher ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehAudioSoftLabel ( BokehSoftLabelExplorer , BokehForAudio ) The audio flavor of BokehSoftLabelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehAudioSoftLabel ( BokehSoftLabelExplorer , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehSoftLabelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSoftLabelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . score_filter ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehImageAnnotator ( BokehDataAnnotator , BokehForImage ) The image flavor of BokehDataAnnotator .\" Source code in hover/core/explorer/specialization.py class BokehImageAnnotator ( BokehDataAnnotator , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehDataAnnotator`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataAnnotator . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . annotator_input , self . annotator_apply ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehImageFinder ( BokehDataFinder , BokehForImage ) The image flavor of BokehDataFinder .\" Source code in hover/core/explorer/specialization.py class BokehImageFinder ( BokehDataFinder , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehDataFinder`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataFinder . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( column ( self . search_sim , self . search_threshold ), column ( self . search_filter_box ), ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehImageMargin ( BokehMarginExplorer , BokehForImage ) The image flavor of BokehMarginExplorer .\" Source code in hover/core/explorer/specialization.py class BokehImageMargin ( BokehMarginExplorer , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehMarginExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehMarginExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehImageSnorkel ( BokehSnorkelExplorer , BokehForImage ) The image flavor of BokehSnorkelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehImageSnorkel ( BokehSnorkelExplorer , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehSnorkelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSnorkelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . lf_apply_trigger , self . lf_filter_trigger , self . lf_list_refresher ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehImageSoftLabel ( BokehSoftLabelExplorer , BokehForImage ) The image flavor of BokehSoftLabelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehImageSoftLabel ( BokehSoftLabelExplorer , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehSoftLabelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSoftLabelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . score_filter ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehTextAnnotator ( BokehDataAnnotator , BokehForText ) The text flavor of BokehDataAnnotator .\" Source code in hover/core/explorer/specialization.py class BokehTextAnnotator ( BokehDataAnnotator , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehDataAnnotator`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataAnnotator . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . annotator_input , self . annotator_apply ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehTextFinder ( BokehDataFinder , BokehForText ) The text flavor of BokehDataFinder .\" Source code in hover/core/explorer/specialization.py class BokehTextFinder ( BokehDataFinder , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehDataFinder`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataFinder . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( column ( self . search_pos , self . search_neg ), column ( self . search_filter_box ), ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehTextMargin ( BokehMarginExplorer , BokehForText ) The text flavor of BokehMarginExplorer .\" Source code in hover/core/explorer/specialization.py class BokehTextMargin ( BokehMarginExplorer , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehMarginExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehMarginExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehTextSnorkel ( BokehSnorkelExplorer , BokehForText ) The text flavor of BokehSnorkelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehTextSnorkel ( BokehSnorkelExplorer , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehSnorkelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSnorkelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . lf_apply_trigger , self . lf_filter_trigger , self . lf_list_refresher ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows ) BokehTextSoftLabel ( BokehSoftLabelExplorer , BokehForText ) The text flavor of BokehSoftLabelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehTextSoftLabel ( BokehSoftLabelExplorer , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehSoftLabelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSoftLabelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . score_filter ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":".specialization"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioAnnotator","text":"The audio flavor of BokehDataAnnotator .\" Source code in hover/core/explorer/specialization.py class BokehAudioAnnotator ( BokehDataAnnotator , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehDataAnnotator`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataAnnotator . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . annotator_input , self . annotator_apply ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehAudioAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioFinder","text":"The audio flavor of BokehDataFinder .\" Source code in hover/core/explorer/specialization.py class BokehAudioFinder ( BokehDataFinder , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehDataFinder`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataFinder . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( column ( self . search_sim , self . search_threshold ), column ( self . search_filter_box ), ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehAudioFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioMargin","text":"The audio flavor of BokehMarginExplorer .\" Source code in hover/core/explorer/specialization.py class BokehAudioMargin ( BokehMarginExplorer , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehMarginExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehMarginExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehAudioMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSnorkel","text":"The audio flavor of BokehSnorkelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehAudioSnorkel ( BokehSnorkelExplorer , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehSnorkelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSnorkelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . lf_apply_trigger , self . lf_filter_trigger , self . lf_list_refresher ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehAudioSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSoftLabel","text":"The audio flavor of BokehSoftLabelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehAudioSoftLabel ( BokehSoftLabelExplorer , BokehForAudio ): \"\"\" ???+ note \"The audio flavor of `BokehSoftLabelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForAudio . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForAudio . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSoftLabelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . score_filter ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehAudioSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageAnnotator","text":"The image flavor of BokehDataAnnotator .\" Source code in hover/core/explorer/specialization.py class BokehImageAnnotator ( BokehDataAnnotator , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehDataAnnotator`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataAnnotator . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . annotator_input , self . annotator_apply ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehImageAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageFinder","text":"The image flavor of BokehDataFinder .\" Source code in hover/core/explorer/specialization.py class BokehImageFinder ( BokehDataFinder , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehDataFinder`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataFinder . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( column ( self . search_sim , self . search_threshold ), column ( self . search_filter_box ), ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehImageFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageMargin","text":"The image flavor of BokehMarginExplorer .\" Source code in hover/core/explorer/specialization.py class BokehImageMargin ( BokehMarginExplorer , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehMarginExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehMarginExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehImageMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSnorkel","text":"The image flavor of BokehSnorkelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehImageSnorkel ( BokehSnorkelExplorer , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehSnorkelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSnorkelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . lf_apply_trigger , self . lf_filter_trigger , self . lf_list_refresher ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehImageSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSoftLabel","text":"The image flavor of BokehSoftLabelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehImageSoftLabel ( BokehSoftLabelExplorer , BokehForImage ): \"\"\" ???+ note \"The image flavor of `BokehSoftLabelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForImage . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForImage . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSoftLabelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_sim , self . search_threshold ), row ( self . score_filter ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehImageSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextAnnotator","text":"The text flavor of BokehDataAnnotator .\" Source code in hover/core/explorer/specialization.py class BokehTextAnnotator ( BokehDataAnnotator , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehDataAnnotator`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataAnnotator . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . annotator_input , self . annotator_apply ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehTextAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextFinder","text":"The text flavor of BokehDataFinder .\" Source code in hover/core/explorer/specialization.py class BokehTextFinder ( BokehDataFinder , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehDataFinder`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehDataFinder . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( column ( self . search_pos , self . search_neg ), column ( self . search_filter_box ), ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehTextFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextMargin","text":"The text flavor of BokehMarginExplorer .\" Source code in hover/core/explorer/specialization.py class BokehTextMargin ( BokehMarginExplorer , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehMarginExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehMarginExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehTextMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSnorkel","text":"The text flavor of BokehSnorkelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehTextSnorkel ( BokehSnorkelExplorer , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehSnorkelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSnorkelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . lf_apply_trigger , self . lf_filter_trigger , self . lf_list_refresher ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehTextSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSoftLabel","text":"The text flavor of BokehSoftLabelExplorer .\" Source code in hover/core/explorer/specialization.py class BokehTextSoftLabel ( BokehSoftLabelExplorer , BokehForText ): \"\"\" ???+ note \"The text flavor of `BokehSoftLabelExplorer`.\"\" \"\"\" TOOLTIP_KWARGS = BokehForText . TOOLTIP_KWARGS MANDATORY_COLUMNS = BokehForText . MANDATORY_COLUMNS SUBSET_GLYPH_KWARGS = BokehSoftLabelExplorer . SUBSET_GLYPH_KWARGS def _layout_widgets ( self ): \"\"\"Define the layout of widgets.\"\"\" layout_rows = ( row ( self . subset_toggle_widget_column , self . selection_option_box ), row ( self . search_pos , self . search_neg ), row ( self . score_filter ), row ( self . dropdown_x_axis , self . dropdown_y_axis ), row ( * self . _dynamic_widgets . values ()), ) return column ( * layout_rows )","title":"BokehTextSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization","text":"","title":"hover.core.explorer.specialization"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioAnnotator","text":"","title":"BokehAudioAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioFinder","text":"","title":"BokehAudioFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioMargin","text":"","title":"BokehAudioMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSnorkel","text":"","title":"BokehAudioSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSoftLabel","text":"","title":"BokehAudioSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageAnnotator","text":"","title":"BokehImageAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageFinder","text":"","title":"BokehImageFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageMargin","text":"","title":"BokehImageMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSnorkel","text":"","title":"BokehImageSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSoftLabel","text":"","title":"BokehImageSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextAnnotator","text":"","title":"BokehTextAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextFinder","text":"","title":"BokehTextFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextMargin","text":"","title":"BokehTextMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSnorkel","text":"","title":"BokehTextSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSoftLabel","text":"","title":"BokehTextSoftLabel"},{"location":"pages/reference/core-neural/","text":"Neural network components. torch -based template classes for implementing neural nets that work the most smoothly with hover . BaseVectorNet ( Loggable ) Abstract transfer learning model defining common signatures. Intended to define crucial interactions with built-in recipes like hover.recipes.active_learning() . Source code in hover/core/neural.py class BaseVectorNet ( Loggable ): \"\"\" ???+ note \"Abstract transfer learning model defining common signatures.\" Intended to define crucial interactions with built-in recipes like `hover.recipes.active_learning()`. \"\"\" @abstractmethod def predict_proba ( self , inps ): pass @abstractmethod def manifold_trajectory ( self , inps , method = \"umap\" , reducer_kwargs = None , spline_kwargs = None ): pass @abstractmethod def prepare_loader ( self , dataset , key , ** kwargs ): pass @abstractmethod def train ( self , train_loader , dev_loader = None , epochs = None , ** kwargs ): pass VectorNet ( BaseVectorNet ) Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Coupled with: hover.utils.torch_helper.VectorDataset Source code in hover/core/neural.py class VectorNet ( BaseVectorNet ): \"\"\" ???+ note \"Simple transfer learning model: a user-supplied vectorizer followed by a neural net.\" This is a parent class whose children may use different training schemes. Coupled with: - `hover.utils.torch_helper.VectorDataset` \"\"\" DEFAULT_OPTIM_CLS = torch . optim . Adam DEFAULT_OPTIM_LOGLR = 2.0 DEFAULT_OPTIM_KWARGS = { \"lr\" : 0.1 ** DEFAULT_OPTIM_LOGLR , \"betas\" : ( 0.9 , 0.999 )} def __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , example_input = \"\" , ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `architecture` | `class` | a `torch.nn.Module` child class | | `state_dict_path` | `str` | path to a (could-be-empty) `torch` state dict | | `labels` | `list` | list of `str` classification labels | | `backup_state_dict` | `bool` | whether to backup the loaded state dict | | `optimizer_cls` | `subclass of torch.optim.Optimizer` | pytorch optimizer class | | `optimizer_kwargs` | `dict` | pytorch optimizer kwargs | | `verbose` | `int` | logging verbosity level | | `example_input` | any | example input to the vectorizer | \"\"\" assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose self . vectorizer = vectorizer self . example_input = example_input self . architecture = architecture self . setup_label_conversion ( labels ) self . _dynamic_params = {} # set a path to store updated parameters self . nn_update_path = state_dict_path if backup_state_dict and os . path . isfile ( state_dict_path ): state_dict_backup_path = f \" { state_dict_path } . { current_time ( '%Y%m %d %H%M%S' ) } \" copyfile ( state_dict_path , state_dict_backup_path ) # initialize an optimizer object and a dict to hold dynamic parameters optimizer_cls = optimizer_cls or self . __class__ . DEFAULT_OPTIM_CLS optimizer_kwargs = ( optimizer_kwargs or self . __class__ . DEFAULT_OPTIM_KWARGS . copy () ) def callback_reset_nn_optimizer (): \"\"\" Callback function which has access to optimizer init settings. \"\"\" self . nn_optimizer = optimizer_cls ( self . nn . parameters ()) assert isinstance ( self . nn_optimizer , torch . optim . Optimizer ), f \"Expected an optimizer, got { type ( self . nn_optimizer ) } \" self . _dynamic_params [ \"optimizer\" ] = optimizer_kwargs self . _callback_reset_nn_optimizer = callback_reset_nn_optimizer self . setup_nn ( use_existing_state_dict = True ) self . _setup_widgets () def auto_adjust_setup ( self , labels , auto_skip = True ): \"\"\" ???+ note \"Auto-(re)create label encoder/decoder and neural net.\" Intended to be called in and out of the constructor. | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `auto_skip` | `bool` | skip when labels did not change | \"\"\" # sanity check and skip assert isinstance ( labels , list ), f \"Expected a list of labels, got { labels } \" # if the sequence of labels matches label encoder exactly, skip label_match_flag = labels == sorted ( self . label_encoder . keys (), key = lambda k : self . label_encoder [ k ] ) if auto_skip and label_match_flag : return self . setup_label_conversion ( labels ) self . setup_nn ( use_existing_state_dict = False ) self . _good ( f \"adjusted to new list of labels: { labels } \" ) def setup_label_conversion ( self , labels ): \"\"\" ???+ note \"Set up label encoder/decoder and number of classes.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | \"\"\" self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) def setup_nn ( self , use_existing_state_dict = True ): \"\"\" ???+ note \"Set up neural network and optimizers.\" Intended to be called in and out of the constructor. - will try to load parameters from state dict by default - option to override and discard previous state dict - often used when the classification targets have changed | Param | Type | Description | | :------------------------ | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `use_existing_state_dict` | `bool` | whether to use existing state dict | \"\"\" # set up vectorizer and the neural network with appropriate dimensions vec_dim = self . vectorizer ( self . example_input ) . shape [ 0 ] self . nn = self . architecture ( vec_dim , self . num_classes ) self . _callback_reset_nn_optimizer () state_dict_exists = os . path . isfile ( self . nn_update_path ) # if state dict exists, load it (when consistent) or overwrite if state_dict_exists : if use_existing_state_dict : self . load ( self . nn_update_path ) else : self . save ( self . nn_update_path ) self . _good ( f \"reset neural net: in { vec_dim } out { self . num_classes } .\" ) def load ( self , load_path = None ): \"\"\" ???+ note \"Load neural net parameters if possible.\" Can be directed to a custom state dict. | Param | Type | Description | | :---------- | :--------- | :--------------------------- | | `load_path` | `str` | path to a `torch` state dict | \"\"\" load_path = load_path or self . nn_update_path # if the architecture cannot match the state dict, skip the load and warn try : self . nn . load_state_dict ( torch . load ( load_path )) self . _info ( f \"loaded state dict { load_path } .\" ) except Exception as e : self . _warn ( f \"load VectorNet state path failed with { type ( e ) } : { e } \" ) @classmethod def from_module ( cls , model_module , labels , ** kwargs ): \"\"\" ???+ note \"Create a VectorNet model from a loadable module.\" | Param | Type | Description | | :------------- | :--------- | :----------------------------------- | | `model_module` | `module` or `str` | (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable | | `labels` | `list` | list of `str` classification labels | | `**kwargs` | | forwarded to `self.__init__()` constructor | \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ** kwargs , ) return model def save ( self , save_path = None ): \"\"\" ???+ note \"Save the current state dict with authorization to overwrite.\" | Param | Type | Description | | :---------- | :---- | :------------------------------------ | | `save_path` | `str` | option alternative path to state dict | \"\"\" save_path = save_path or self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) verb = \"overwrote\" if os . path . isfile ( save_path ) else \"saved\" self . _info ( f \" { verb } state dict { save_path } .\" ) def _setup_widgets ( self ): \"\"\" ???+ note \"Bokeh widgets for changing hyperparameters through user interaction.\" \"\"\" self . epochs_slider = Slider ( start = 1 , end = 50 , value = 1 , step = 1 , title = \"# epochs\" ) self . loglr_slider = Slider ( title = \"learning rate\" , start = 1.0 , end = 7.0 , value = self . __class__ . DEFAULT_OPTIM_LOGLR , step = 0.1 , format = FuncTickFormatter ( code = \"return Math.pow(0.1, tick).toFixed(8)\" ), ) def update_lr ( attr , old , new ): self . _dynamic_params [ \"optimizer\" ][ \"lr\" ] = 0.1 ** self . loglr_slider . value self . loglr_slider . on_change ( \"value\" , update_lr ) def _layout_widgets ( self ): \"\"\" ???+ note \"Layout of widgets when plotted.\" \"\"\" from bokeh.layouts import row return row ( self . epochs_slider , self . loglr_slider ) def view ( self ): \"\"\" ???+ note \"Overall layout when plotted.\" \"\"\" return self . _layout_widgets () def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Dynamically change parameters of the neural net optimizer.\" - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ]) def predict_proba ( self , inps ): \"\"\" ???+ note \"End-to-end single/multi-piece prediction from inp to class probabilities.\" | Param | Type | Description | | :----- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = not isinstance ( inps , list ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ( np . array ([ self . vectorizer ( _inp ) for _inp in inps ])) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs def manifold_trajectory ( self , inps , method = \"umap\" , reducer_kwargs = None , spline_kwargs = None ): \"\"\" ???+ note \"Compute a propagation trajectory of the dataset manifold through the neural net.\" 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to N-D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines | Param | Type | Description | | :------- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | | `method` | `str` | reduction method: `\"umap\"` or `\"ivis\"` | | `reducer_kwargs` | | kwargs to forward to dimensionality reduction | | `spline_kwargs` | | kwargs to forward to spline calculation | \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline reducer_kwargs = reducer_kwargs or {} spline_kwargs = spline_kwargs or {} # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method , ** reducer_kwargs ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** spline_kwargs ) return traj_arr , seq_arr , disparities def prepare_loader ( self , dataset , key , ** kwargs ): \"\"\" ???+ note \"Create dataloader from `SupervisableDataset` with implied vectorizer(s).\" | Param | Type | Description | | :--------- | :---- | :------------------------- | | `dataset` | `hover.core.dataset.SupervisableDataset` | the dataset to load | | `key` | `str` | \"train\", \"dev\", or \"test\" | | `**kwargs` | | forwarded to `dataset.loader()` | \"\"\" return dataset . loader ( key , self . vectorizer , ** kwargs ) def train ( self , train_loader , dev_loader = None , epochs = None ): \"\"\" ???+ note \"Train the neural network part of the VecNet.\" - intended to be coupled with self.train_batch(). | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | | `epochs` | `int` | number of epochs to train | \"\"\" epochs = epochs or self . epochs_slider . value train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader ) if dev_loader is not None : dev_loader = train_loader acc , conf_mat = self . evaluate ( dev_loader ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" ???+ note \"Train the neural network for one epoch.\" - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `*args` | | arguments to forward to `train_batch` | | `**kwargs` | | kwargs to forward to `train_batch` | \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs ) def train_batch ( self , loaded_input , loaded_output ): \"\"\" ???+ note \"Train the neural network for one batch.\" | Param | Type | Description | | :-------------- | :------------- | :-------------------- | | `loaded_input` | `torch.Tensor` | input tensor | | `loaded_output` | `torch.Tensor` | output tensor | \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = F . cross_entropy ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) def evaluate ( self , dev_loader ): \"\"\" ???+ note \"Evaluate the VecNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose >= 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat DEFAULT_OPTIM_CLS ( Optimizer ) Implements Adam algorithm. .. math:: \\begin{aligned} &\\rule{110mm}{0.4pt} \\ &\\textbf{input} : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2 \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)} \\ &\\hspace{13mm} \\lambda \\text{ (weight decay)}, : amsgrad \\ &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ ( first moment)}, v_0\\leftarrow 0 \\text{ (second moment)},: \\widehat{v_0}^{max}\\leftarrow 0\\[-1.ex] &\\rule{110mm}{0.4pt} \\ &\\textbf{for} : t=1 : \\textbf{to} : \\ldots : \\textbf{do} \\ &\\hspace{5mm}g_t \\leftarrow \\nabla_{\\theta} f_t (\\theta_{t-1}) \\ &\\hspace{5mm}\\textbf{if} : \\lambda \\neq 0 \\ &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1} \\ &\\hspace{5mm}m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\ &\\hspace{5mm}v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t \\ &\\hspace{5mm}\\widehat{m_t} \\leftarrow m_t/\\big(1-\\beta_1^t \\big) \\ &\\hspace{5mm}\\widehat{v_t} \\leftarrow v_t/\\big(1-\\beta_2^t \\big) \\ &\\hspace{5mm}\\textbf{if} : amsgrad \\ &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max}, \\widehat{v_t}) \\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big) \\ &\\hspace{5mm}\\textbf{else} \\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big) \\ &\\rule{110mm}{0.4pt} \\[-1.ex] &\\bf{return} : \\theta_t \\[-1.ex] &\\rule{110mm}{0.4pt} \\[-1.ex] \\end{aligned} For further details regarding the algorithm we refer to Adam: A Method for Stochastic Optimization _. Parameters: Name Type Description Default params iterable iterable of parameters to optimize or dicts defining parameter groups required lr float learning rate (default: 1e-3) 0.001 betas Tuple[float, float] coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) (0.9, 0.999) eps float term added to the denominator to improve numerical stability (default: 1e-8) 1e-08 weight_decay float weight decay (L2 penalty) (default: 0) 0 amsgrad boolean whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond _ (default: False) False .. _Adam: A Method for Stochastic Optimization: https://arxiv.org/abs/1412.6980 .. _On the Convergence of Adam and Beyond: https://openreview.net/forum?id=ryQu7f-RZ Source code in hover/core/neural.py class Adam ( Optimizer ): r \"\"\"Implements Adam algorithm. .. math:: \\begin{aligned} &\\rule{110mm}{0.4pt} \\\\ &\\textbf{input} : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2 \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)} \\\\ &\\hspace{13mm} \\lambda \\text{ (weight decay)}, \\: amsgrad \\\\ &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ ( first moment)}, v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex] &\\rule{110mm}{0.4pt} \\\\ &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do} \\\\ &\\hspace{5mm}g_t \\leftarrow \\nabla_{\\theta} f_t (\\theta_{t-1}) \\\\ &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0 \\\\ &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1} \\\\ &\\hspace{5mm}m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ &\\hspace{5mm}v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t \\\\ &\\hspace{5mm}\\widehat{m_t} \\leftarrow m_t/\\big(1-\\beta_1^t \\big) \\\\ &\\hspace{5mm}\\widehat{v_t} \\leftarrow v_t/\\big(1-\\beta_2^t \\big) \\\\ &\\hspace{5mm}\\textbf{if} \\: amsgrad \\\\ &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max}, \\widehat{v_t}) \\\\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big) \\\\ &\\hspace{5mm}\\textbf{else} \\\\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big) \\\\ &\\rule{110mm}{0.4pt} \\\\[-1.ex] &\\bf{return} \\: \\theta_t \\\\[-1.ex] &\\rule{110mm}{0.4pt} \\\\[-1.ex] \\end{aligned} For further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_. Args: params (iterable): iterable of parameters to optimize or dicts defining parameter groups lr (float, optional): learning rate (default: 1e-3) betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8) weight_decay (float, optional): weight decay (L2 penalty) (default: 0) amsgrad (boolean, optional): whether to use the AMSGrad variant of this algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False) .. _Adam\\: A Method for Stochastic Optimization: https://arxiv.org/abs/1412.6980 .. _On the Convergence of Adam and Beyond: https://openreview.net/forum?id=ryQu7f-RZ \"\"\" def __init__ ( self , params , lr = 1e-3 , betas = ( 0.9 , 0.999 ), eps = 1e-8 , weight_decay = 0 , amsgrad = False ): if not 0.0 <= lr : raise ValueError ( \"Invalid learning rate: {} \" . format ( lr )) if not 0.0 <= eps : raise ValueError ( \"Invalid epsilon value: {} \" . format ( eps )) if not 0.0 <= betas [ 0 ] < 1.0 : raise ValueError ( \"Invalid beta parameter at index 0: {} \" . format ( betas [ 0 ])) if not 0.0 <= betas [ 1 ] < 1.0 : raise ValueError ( \"Invalid beta parameter at index 1: {} \" . format ( betas [ 1 ])) if not 0.0 <= weight_decay : raise ValueError ( \"Invalid weight_decay value: {} \" . format ( weight_decay )) defaults = dict ( lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad ) super ( Adam , self ) . __init__ ( params , defaults ) def __setstate__ ( self , state ): super ( Adam , self ) . __setstate__ ( state ) for group in self . param_groups : group . setdefault ( 'amsgrad' , False ) @torch . no_grad () def step ( self , closure = None ): \"\"\"Performs a single optimization step. Args: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None : with torch . enable_grad (): loss = closure () for group in self . param_groups : params_with_grad = [] grads = [] exp_avgs = [] exp_avg_sqs = [] max_exp_avg_sqs = [] state_steps = [] beta1 , beta2 = group [ 'betas' ] for p in group [ 'params' ]: if p . grad is not None : params_with_grad . append ( p ) if p . grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) grads . append ( p . grad ) state = self . state [ p ] # Lazy state initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) if group [ 'amsgrad' ]: # Maintains max of all exp. moving avg. of sq. grad. values state [ 'max_exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) exp_avgs . append ( state [ 'exp_avg' ]) exp_avg_sqs . append ( state [ 'exp_avg_sq' ]) if group [ 'amsgrad' ]: max_exp_avg_sqs . append ( state [ 'max_exp_avg_sq' ]) # update the steps for each param group update state [ 'step' ] += 1 # record the step after step update state_steps . append ( state [ 'step' ]) F . adam ( params_with_grad , grads , exp_avgs , exp_avg_sqs , max_exp_avg_sqs , state_steps , amsgrad = group [ 'amsgrad' ], beta1 = beta1 , beta2 = beta2 , lr = group [ 'lr' ], weight_decay = group [ 'weight_decay' ], eps = group [ 'eps' ]) return loss step ( self , closure = None ) Performs a single optimization step. Parameters: Name Type Description Default closure callable A closure that reevaluates the model and returns the loss. None Source code in hover/core/neural.py @torch . no_grad () def step ( self , closure = None ): \"\"\"Performs a single optimization step. Args: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None : with torch . enable_grad (): loss = closure () for group in self . param_groups : params_with_grad = [] grads = [] exp_avgs = [] exp_avg_sqs = [] max_exp_avg_sqs = [] state_steps = [] beta1 , beta2 = group [ 'betas' ] for p in group [ 'params' ]: if p . grad is not None : params_with_grad . append ( p ) if p . grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) grads . append ( p . grad ) state = self . state [ p ] # Lazy state initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) if group [ 'amsgrad' ]: # Maintains max of all exp. moving avg. of sq. grad. values state [ 'max_exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) exp_avgs . append ( state [ 'exp_avg' ]) exp_avg_sqs . append ( state [ 'exp_avg_sq' ]) if group [ 'amsgrad' ]: max_exp_avg_sqs . append ( state [ 'max_exp_avg_sq' ]) # update the steps for each param group update state [ 'step' ] += 1 # record the step after step update state_steps . append ( state [ 'step' ]) F . adam ( params_with_grad , grads , exp_avgs , exp_avg_sqs , max_exp_avg_sqs , state_steps , amsgrad = group [ 'amsgrad' ], beta1 = beta1 , beta2 = beta2 , lr = group [ 'lr' ], weight_decay = group [ 'weight_decay' ], eps = group [ 'eps' ]) return loss __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , example_input = '' ) special Create the VectorNet , loading parameters if available. Param Type Description vectorizer callable the feature -> vector function architecture class a torch.nn.Module child class state_dict_path str path to a (could-be-empty) torch state dict labels list list of str classification labels backup_state_dict bool whether to backup the loaded state dict optimizer_cls subclass of torch.optim.Optimizer pytorch optimizer class optimizer_kwargs dict pytorch optimizer kwargs verbose int logging verbosity level example_input any example input to the vectorizer Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , example_input = \"\" , ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `architecture` | `class` | a `torch.nn.Module` child class | | `state_dict_path` | `str` | path to a (could-be-empty) `torch` state dict | | `labels` | `list` | list of `str` classification labels | | `backup_state_dict` | `bool` | whether to backup the loaded state dict | | `optimizer_cls` | `subclass of torch.optim.Optimizer` | pytorch optimizer class | | `optimizer_kwargs` | `dict` | pytorch optimizer kwargs | | `verbose` | `int` | logging verbosity level | | `example_input` | any | example input to the vectorizer | \"\"\" assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose self . vectorizer = vectorizer self . example_input = example_input self . architecture = architecture self . setup_label_conversion ( labels ) self . _dynamic_params = {} # set a path to store updated parameters self . nn_update_path = state_dict_path if backup_state_dict and os . path . isfile ( state_dict_path ): state_dict_backup_path = f \" { state_dict_path } . { current_time ( '%Y%m %d %H%M%S' ) } \" copyfile ( state_dict_path , state_dict_backup_path ) # initialize an optimizer object and a dict to hold dynamic parameters optimizer_cls = optimizer_cls or self . __class__ . DEFAULT_OPTIM_CLS optimizer_kwargs = ( optimizer_kwargs or self . __class__ . DEFAULT_OPTIM_KWARGS . copy () ) def callback_reset_nn_optimizer (): \"\"\" Callback function which has access to optimizer init settings. \"\"\" self . nn_optimizer = optimizer_cls ( self . nn . parameters ()) assert isinstance ( self . nn_optimizer , torch . optim . Optimizer ), f \"Expected an optimizer, got { type ( self . nn_optimizer ) } \" self . _dynamic_params [ \"optimizer\" ] = optimizer_kwargs self . _callback_reset_nn_optimizer = callback_reset_nn_optimizer self . setup_nn ( use_existing_state_dict = True ) self . _setup_widgets () adjust_optimizer_params ( self ) Dynamically change parameters of the neural net optimizer. Intended to be polymorphic in child classes and to be called per epoch. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Dynamically change parameters of the neural net optimizer.\" - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ]) auto_adjust_setup ( self , labels , auto_skip = True ) Auto-(re)create label encoder/decoder and neural net. Intended to be called in and out of the constructor. Param Type Description labels list list of str classification labels auto_skip bool skip when labels did not change Source code in hover/core/neural.py def auto_adjust_setup ( self , labels , auto_skip = True ): \"\"\" ???+ note \"Auto-(re)create label encoder/decoder and neural net.\" Intended to be called in and out of the constructor. | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `auto_skip` | `bool` | skip when labels did not change | \"\"\" # sanity check and skip assert isinstance ( labels , list ), f \"Expected a list of labels, got { labels } \" # if the sequence of labels matches label encoder exactly, skip label_match_flag = labels == sorted ( self . label_encoder . keys (), key = lambda k : self . label_encoder [ k ] ) if auto_skip and label_match_flag : return self . setup_label_conversion ( labels ) self . setup_nn ( use_existing_state_dict = False ) self . _good ( f \"adjusted to new list of labels: { labels } \" ) evaluate ( self , dev_loader ) Evaluate the VecNet against a dev set. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate ( self , dev_loader ): \"\"\" ???+ note \"Evaluate the VecNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose >= 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat from_module ( model_module , labels , ** kwargs ) classmethod Create a VectorNet model from a loadable module. Param Type Description model_module module or str (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable labels list list of str classification labels **kwargs forwarded to self.__init__() constructor Source code in hover/core/neural.py @classmethod def from_module ( cls , model_module , labels , ** kwargs ): \"\"\" ???+ note \"Create a VectorNet model from a loadable module.\" | Param | Type | Description | | :------------- | :--------- | :----------------------------------- | | `model_module` | `module` or `str` | (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable | | `labels` | `list` | list of `str` classification labels | | `**kwargs` | | forwarded to `self.__init__()` constructor | \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ** kwargs , ) return model load ( self , load_path = None ) Load neural net parameters if possible. Can be directed to a custom state dict. Param Type Description load_path str path to a torch state dict Source code in hover/core/neural.py def load ( self , load_path = None ): \"\"\" ???+ note \"Load neural net parameters if possible.\" Can be directed to a custom state dict. | Param | Type | Description | | :---------- | :--------- | :--------------------------- | | `load_path` | `str` | path to a `torch` state dict | \"\"\" load_path = load_path or self . nn_update_path # if the architecture cannot match the state dict, skip the load and warn try : self . nn . load_state_dict ( torch . load ( load_path )) self . _info ( f \"loaded state dict { load_path } .\" ) except Exception as e : self . _warn ( f \"load VectorNet state path failed with { type ( e ) } : { e } \" ) manifold_trajectory ( self , inps , method = 'umap' , reducer_kwargs = None , spline_kwargs = None ) Compute a propagation trajectory of the dataset manifold through the neural net. vectorize inps forward propagate, keeping intermediates fit intermediates to N-D manifolds fit manifolds using Procrustes shape analysis fit shapes to trajectory splines Param Type Description inps dynamic (a list of) input features to vectorize method str reduction method: \"umap\" or \"ivis\" reducer_kwargs kwargs to forward to dimensionality reduction spline_kwargs kwargs to forward to spline calculation Source code in hover/core/neural.py def manifold_trajectory ( self , inps , method = \"umap\" , reducer_kwargs = None , spline_kwargs = None ): \"\"\" ???+ note \"Compute a propagation trajectory of the dataset manifold through the neural net.\" 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to N-D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines | Param | Type | Description | | :------- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | | `method` | `str` | reduction method: `\"umap\"` or `\"ivis\"` | | `reducer_kwargs` | | kwargs to forward to dimensionality reduction | | `spline_kwargs` | | kwargs to forward to spline calculation | \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline reducer_kwargs = reducer_kwargs or {} spline_kwargs = spline_kwargs or {} # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method , ** reducer_kwargs ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** spline_kwargs ) return traj_arr , seq_arr , disparities predict_proba ( self , inps ) End-to-end single/multi-piece prediction from inp to class probabilities. Param Type Description inps dynamic (a list of) input features to vectorize Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" ???+ note \"End-to-end single/multi-piece prediction from inp to class probabilities.\" | Param | Type | Description | | :----- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = not isinstance ( inps , list ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ( np . array ([ self . vectorizer ( _inp ) for _inp in inps ])) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs prepare_loader ( self , dataset , key , ** kwargs ) Create dataloader from SupervisableDataset with implied vectorizer(s). Param Type Description dataset hover.core.dataset.SupervisableDataset the dataset to load key str \"train\", \"dev\", or \"test\" **kwargs forwarded to dataset.loader() Source code in hover/core/neural.py def prepare_loader ( self , dataset , key , ** kwargs ): \"\"\" ???+ note \"Create dataloader from `SupervisableDataset` with implied vectorizer(s).\" | Param | Type | Description | | :--------- | :---- | :------------------------- | | `dataset` | `hover.core.dataset.SupervisableDataset` | the dataset to load | | `key` | `str` | \"train\", \"dev\", or \"test\" | | `**kwargs` | | forwarded to `dataset.loader()` | \"\"\" return dataset . loader ( key , self . vectorizer , ** kwargs ) save ( self , save_path = None ) Save the current state dict with authorization to overwrite. Param Type Description save_path str option alternative path to state dict Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" ???+ note \"Save the current state dict with authorization to overwrite.\" | Param | Type | Description | | :---------- | :---- | :------------------------------------ | | `save_path` | `str` | option alternative path to state dict | \"\"\" save_path = save_path or self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) verb = \"overwrote\" if os . path . isfile ( save_path ) else \"saved\" self . _info ( f \" { verb } state dict { save_path } .\" ) setup_label_conversion ( self , labels ) Set up label encoder/decoder and number of classes. Param Type Description labels list list of str classification labels Source code in hover/core/neural.py def setup_label_conversion ( self , labels ): \"\"\" ???+ note \"Set up label encoder/decoder and number of classes.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | \"\"\" self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) setup_nn ( self , use_existing_state_dict = True ) Set up neural network and optimizers. Intended to be called in and out of the constructor. will try to load parameters from state dict by default option to override and discard previous state dict often used when the classification targets have changed Param Type Description labels list list of str classification labels use_existing_state_dict bool whether to use existing state dict Source code in hover/core/neural.py def setup_nn ( self , use_existing_state_dict = True ): \"\"\" ???+ note \"Set up neural network and optimizers.\" Intended to be called in and out of the constructor. - will try to load parameters from state dict by default - option to override and discard previous state dict - often used when the classification targets have changed | Param | Type | Description | | :------------------------ | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `use_existing_state_dict` | `bool` | whether to use existing state dict | \"\"\" # set up vectorizer and the neural network with appropriate dimensions vec_dim = self . vectorizer ( self . example_input ) . shape [ 0 ] self . nn = self . architecture ( vec_dim , self . num_classes ) self . _callback_reset_nn_optimizer () state_dict_exists = os . path . isfile ( self . nn_update_path ) # if state dict exists, load it (when consistent) or overwrite if state_dict_exists : if use_existing_state_dict : self . load ( self . nn_update_path ) else : self . save ( self . nn_update_path ) self . _good ( f \"reset neural net: in { vec_dim } out { self . num_classes } .\" ) train ( self , train_loader , dev_loader = None , epochs = None ) Train the neural network part of the VecNet. intended to be coupled with self.train_batch(). Param Type Description train_loader torch.utils.data.DataLoader train set dev_loader torch.utils.data.DataLoader dev set epochs int number of epochs to train Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = None ): \"\"\" ???+ note \"Train the neural network part of the VecNet.\" - intended to be coupled with self.train_batch(). | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | | `epochs` | `int` | number of epochs to train | \"\"\" epochs = epochs or self . epochs_slider . value train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader ) if dev_loader is not None : dev_loader = train_loader acc , conf_mat = self . evaluate ( dev_loader ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info train_batch ( self , loaded_input , loaded_output ) Train the neural network for one batch. Param Type Description loaded_input torch.Tensor input tensor loaded_output torch.Tensor output tensor Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output ): \"\"\" ???+ note \"Train the neural network for one batch.\" | Param | Type | Description | | :-------------- | :------------- | :-------------------- | | `loaded_input` | `torch.Tensor` | input tensor | | `loaded_output` | `torch.Tensor` | output tensor | \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = F . cross_entropy ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) train_epoch ( self , train_loader , * args , ** kwargs ) Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Param Type Description train_loader torch.utils.data.DataLoader train set *args arguments to forward to train_batch **kwargs kwargs to forward to train_batch Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" ???+ note \"Train the neural network for one epoch.\" - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `*args` | | arguments to forward to `train_batch` | | `**kwargs` | | kwargs to forward to `train_batch` | \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs ) view ( self ) Overall layout when plotted. Source code in hover/core/neural.py def view ( self ): \"\"\" ???+ note \"Overall layout when plotted.\" \"\"\" return self . _layout_widgets ()","title":"hover.core.neural"},{"location":"pages/reference/core-neural/#hover.core.neural.BaseVectorNet","text":"Abstract transfer learning model defining common signatures. Intended to define crucial interactions with built-in recipes like hover.recipes.active_learning() . Source code in hover/core/neural.py class BaseVectorNet ( Loggable ): \"\"\" ???+ note \"Abstract transfer learning model defining common signatures.\" Intended to define crucial interactions with built-in recipes like `hover.recipes.active_learning()`. \"\"\" @abstractmethod def predict_proba ( self , inps ): pass @abstractmethod def manifold_trajectory ( self , inps , method = \"umap\" , reducer_kwargs = None , spline_kwargs = None ): pass @abstractmethod def prepare_loader ( self , dataset , key , ** kwargs ): pass @abstractmethod def train ( self , train_loader , dev_loader = None , epochs = None , ** kwargs ): pass","title":"BaseVectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet","text":"Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Coupled with: hover.utils.torch_helper.VectorDataset Source code in hover/core/neural.py class VectorNet ( BaseVectorNet ): \"\"\" ???+ note \"Simple transfer learning model: a user-supplied vectorizer followed by a neural net.\" This is a parent class whose children may use different training schemes. Coupled with: - `hover.utils.torch_helper.VectorDataset` \"\"\" DEFAULT_OPTIM_CLS = torch . optim . Adam DEFAULT_OPTIM_LOGLR = 2.0 DEFAULT_OPTIM_KWARGS = { \"lr\" : 0.1 ** DEFAULT_OPTIM_LOGLR , \"betas\" : ( 0.9 , 0.999 )} def __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , example_input = \"\" , ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `architecture` | `class` | a `torch.nn.Module` child class | | `state_dict_path` | `str` | path to a (could-be-empty) `torch` state dict | | `labels` | `list` | list of `str` classification labels | | `backup_state_dict` | `bool` | whether to backup the loaded state dict | | `optimizer_cls` | `subclass of torch.optim.Optimizer` | pytorch optimizer class | | `optimizer_kwargs` | `dict` | pytorch optimizer kwargs | | `verbose` | `int` | logging verbosity level | | `example_input` | any | example input to the vectorizer | \"\"\" assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose self . vectorizer = vectorizer self . example_input = example_input self . architecture = architecture self . setup_label_conversion ( labels ) self . _dynamic_params = {} # set a path to store updated parameters self . nn_update_path = state_dict_path if backup_state_dict and os . path . isfile ( state_dict_path ): state_dict_backup_path = f \" { state_dict_path } . { current_time ( '%Y%m %d %H%M%S' ) } \" copyfile ( state_dict_path , state_dict_backup_path ) # initialize an optimizer object and a dict to hold dynamic parameters optimizer_cls = optimizer_cls or self . __class__ . DEFAULT_OPTIM_CLS optimizer_kwargs = ( optimizer_kwargs or self . __class__ . DEFAULT_OPTIM_KWARGS . copy () ) def callback_reset_nn_optimizer (): \"\"\" Callback function which has access to optimizer init settings. \"\"\" self . nn_optimizer = optimizer_cls ( self . nn . parameters ()) assert isinstance ( self . nn_optimizer , torch . optim . Optimizer ), f \"Expected an optimizer, got { type ( self . nn_optimizer ) } \" self . _dynamic_params [ \"optimizer\" ] = optimizer_kwargs self . _callback_reset_nn_optimizer = callback_reset_nn_optimizer self . setup_nn ( use_existing_state_dict = True ) self . _setup_widgets () def auto_adjust_setup ( self , labels , auto_skip = True ): \"\"\" ???+ note \"Auto-(re)create label encoder/decoder and neural net.\" Intended to be called in and out of the constructor. | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `auto_skip` | `bool` | skip when labels did not change | \"\"\" # sanity check and skip assert isinstance ( labels , list ), f \"Expected a list of labels, got { labels } \" # if the sequence of labels matches label encoder exactly, skip label_match_flag = labels == sorted ( self . label_encoder . keys (), key = lambda k : self . label_encoder [ k ] ) if auto_skip and label_match_flag : return self . setup_label_conversion ( labels ) self . setup_nn ( use_existing_state_dict = False ) self . _good ( f \"adjusted to new list of labels: { labels } \" ) def setup_label_conversion ( self , labels ): \"\"\" ???+ note \"Set up label encoder/decoder and number of classes.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | \"\"\" self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) def setup_nn ( self , use_existing_state_dict = True ): \"\"\" ???+ note \"Set up neural network and optimizers.\" Intended to be called in and out of the constructor. - will try to load parameters from state dict by default - option to override and discard previous state dict - often used when the classification targets have changed | Param | Type | Description | | :------------------------ | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `use_existing_state_dict` | `bool` | whether to use existing state dict | \"\"\" # set up vectorizer and the neural network with appropriate dimensions vec_dim = self . vectorizer ( self . example_input ) . shape [ 0 ] self . nn = self . architecture ( vec_dim , self . num_classes ) self . _callback_reset_nn_optimizer () state_dict_exists = os . path . isfile ( self . nn_update_path ) # if state dict exists, load it (when consistent) or overwrite if state_dict_exists : if use_existing_state_dict : self . load ( self . nn_update_path ) else : self . save ( self . nn_update_path ) self . _good ( f \"reset neural net: in { vec_dim } out { self . num_classes } .\" ) def load ( self , load_path = None ): \"\"\" ???+ note \"Load neural net parameters if possible.\" Can be directed to a custom state dict. | Param | Type | Description | | :---------- | :--------- | :--------------------------- | | `load_path` | `str` | path to a `torch` state dict | \"\"\" load_path = load_path or self . nn_update_path # if the architecture cannot match the state dict, skip the load and warn try : self . nn . load_state_dict ( torch . load ( load_path )) self . _info ( f \"loaded state dict { load_path } .\" ) except Exception as e : self . _warn ( f \"load VectorNet state path failed with { type ( e ) } : { e } \" ) @classmethod def from_module ( cls , model_module , labels , ** kwargs ): \"\"\" ???+ note \"Create a VectorNet model from a loadable module.\" | Param | Type | Description | | :------------- | :--------- | :----------------------------------- | | `model_module` | `module` or `str` | (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable | | `labels` | `list` | list of `str` classification labels | | `**kwargs` | | forwarded to `self.__init__()` constructor | \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ** kwargs , ) return model def save ( self , save_path = None ): \"\"\" ???+ note \"Save the current state dict with authorization to overwrite.\" | Param | Type | Description | | :---------- | :---- | :------------------------------------ | | `save_path` | `str` | option alternative path to state dict | \"\"\" save_path = save_path or self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) verb = \"overwrote\" if os . path . isfile ( save_path ) else \"saved\" self . _info ( f \" { verb } state dict { save_path } .\" ) def _setup_widgets ( self ): \"\"\" ???+ note \"Bokeh widgets for changing hyperparameters through user interaction.\" \"\"\" self . epochs_slider = Slider ( start = 1 , end = 50 , value = 1 , step = 1 , title = \"# epochs\" ) self . loglr_slider = Slider ( title = \"learning rate\" , start = 1.0 , end = 7.0 , value = self . __class__ . DEFAULT_OPTIM_LOGLR , step = 0.1 , format = FuncTickFormatter ( code = \"return Math.pow(0.1, tick).toFixed(8)\" ), ) def update_lr ( attr , old , new ): self . _dynamic_params [ \"optimizer\" ][ \"lr\" ] = 0.1 ** self . loglr_slider . value self . loglr_slider . on_change ( \"value\" , update_lr ) def _layout_widgets ( self ): \"\"\" ???+ note \"Layout of widgets when plotted.\" \"\"\" from bokeh.layouts import row return row ( self . epochs_slider , self . loglr_slider ) def view ( self ): \"\"\" ???+ note \"Overall layout when plotted.\" \"\"\" return self . _layout_widgets () def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Dynamically change parameters of the neural net optimizer.\" - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ]) def predict_proba ( self , inps ): \"\"\" ???+ note \"End-to-end single/multi-piece prediction from inp to class probabilities.\" | Param | Type | Description | | :----- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = not isinstance ( inps , list ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ( np . array ([ self . vectorizer ( _inp ) for _inp in inps ])) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs def manifold_trajectory ( self , inps , method = \"umap\" , reducer_kwargs = None , spline_kwargs = None ): \"\"\" ???+ note \"Compute a propagation trajectory of the dataset manifold through the neural net.\" 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to N-D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines | Param | Type | Description | | :------- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | | `method` | `str` | reduction method: `\"umap\"` or `\"ivis\"` | | `reducer_kwargs` | | kwargs to forward to dimensionality reduction | | `spline_kwargs` | | kwargs to forward to spline calculation | \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline reducer_kwargs = reducer_kwargs or {} spline_kwargs = spline_kwargs or {} # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method , ** reducer_kwargs ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** spline_kwargs ) return traj_arr , seq_arr , disparities def prepare_loader ( self , dataset , key , ** kwargs ): \"\"\" ???+ note \"Create dataloader from `SupervisableDataset` with implied vectorizer(s).\" | Param | Type | Description | | :--------- | :---- | :------------------------- | | `dataset` | `hover.core.dataset.SupervisableDataset` | the dataset to load | | `key` | `str` | \"train\", \"dev\", or \"test\" | | `**kwargs` | | forwarded to `dataset.loader()` | \"\"\" return dataset . loader ( key , self . vectorizer , ** kwargs ) def train ( self , train_loader , dev_loader = None , epochs = None ): \"\"\" ???+ note \"Train the neural network part of the VecNet.\" - intended to be coupled with self.train_batch(). | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | | `epochs` | `int` | number of epochs to train | \"\"\" epochs = epochs or self . epochs_slider . value train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader ) if dev_loader is not None : dev_loader = train_loader acc , conf_mat = self . evaluate ( dev_loader ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" ???+ note \"Train the neural network for one epoch.\" - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `*args` | | arguments to forward to `train_batch` | | `**kwargs` | | kwargs to forward to `train_batch` | \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs ) def train_batch ( self , loaded_input , loaded_output ): \"\"\" ???+ note \"Train the neural network for one batch.\" | Param | Type | Description | | :-------------- | :------------- | :-------------------- | | `loaded_input` | `torch.Tensor` | input tensor | | `loaded_output` | `torch.Tensor` | output tensor | \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = F . cross_entropy ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) def evaluate ( self , dev_loader ): \"\"\" ???+ note \"Evaluate the VecNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose >= 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat","title":"VectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS","text":"Implements Adam algorithm. .. math:: \\begin{aligned} &\\rule{110mm}{0.4pt} \\ &\\textbf{input} : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2 \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)} \\ &\\hspace{13mm} \\lambda \\text{ (weight decay)}, : amsgrad \\ &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ ( first moment)}, v_0\\leftarrow 0 \\text{ (second moment)},: \\widehat{v_0}^{max}\\leftarrow 0\\[-1.ex] &\\rule{110mm}{0.4pt} \\ &\\textbf{for} : t=1 : \\textbf{to} : \\ldots : \\textbf{do} \\ &\\hspace{5mm}g_t \\leftarrow \\nabla_{\\theta} f_t (\\theta_{t-1}) \\ &\\hspace{5mm}\\textbf{if} : \\lambda \\neq 0 \\ &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1} \\ &\\hspace{5mm}m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\ &\\hspace{5mm}v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t \\ &\\hspace{5mm}\\widehat{m_t} \\leftarrow m_t/\\big(1-\\beta_1^t \\big) \\ &\\hspace{5mm}\\widehat{v_t} \\leftarrow v_t/\\big(1-\\beta_2^t \\big) \\ &\\hspace{5mm}\\textbf{if} : amsgrad \\ &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max}, \\widehat{v_t}) \\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big) \\ &\\hspace{5mm}\\textbf{else} \\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big) \\ &\\rule{110mm}{0.4pt} \\[-1.ex] &\\bf{return} : \\theta_t \\[-1.ex] &\\rule{110mm}{0.4pt} \\[-1.ex] \\end{aligned} For further details regarding the algorithm we refer to Adam: A Method for Stochastic Optimization _. Parameters: Name Type Description Default params iterable iterable of parameters to optimize or dicts defining parameter groups required lr float learning rate (default: 1e-3) 0.001 betas Tuple[float, float] coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) (0.9, 0.999) eps float term added to the denominator to improve numerical stability (default: 1e-8) 1e-08 weight_decay float weight decay (L2 penalty) (default: 0) 0 amsgrad boolean whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond _ (default: False) False .. _Adam: A Method for Stochastic Optimization: https://arxiv.org/abs/1412.6980 .. _On the Convergence of Adam and Beyond: https://openreview.net/forum?id=ryQu7f-RZ Source code in hover/core/neural.py class Adam ( Optimizer ): r \"\"\"Implements Adam algorithm. .. math:: \\begin{aligned} &\\rule{110mm}{0.4pt} \\\\ &\\textbf{input} : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2 \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)} \\\\ &\\hspace{13mm} \\lambda \\text{ (weight decay)}, \\: amsgrad \\\\ &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ ( first moment)}, v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex] &\\rule{110mm}{0.4pt} \\\\ &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do} \\\\ &\\hspace{5mm}g_t \\leftarrow \\nabla_{\\theta} f_t (\\theta_{t-1}) \\\\ &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0 \\\\ &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1} \\\\ &\\hspace{5mm}m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ &\\hspace{5mm}v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t \\\\ &\\hspace{5mm}\\widehat{m_t} \\leftarrow m_t/\\big(1-\\beta_1^t \\big) \\\\ &\\hspace{5mm}\\widehat{v_t} \\leftarrow v_t/\\big(1-\\beta_2^t \\big) \\\\ &\\hspace{5mm}\\textbf{if} \\: amsgrad \\\\ &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max}, \\widehat{v_t}) \\\\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big) \\\\ &\\hspace{5mm}\\textbf{else} \\\\ &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/ \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big) \\\\ &\\rule{110mm}{0.4pt} \\\\[-1.ex] &\\bf{return} \\: \\theta_t \\\\[-1.ex] &\\rule{110mm}{0.4pt} \\\\[-1.ex] \\end{aligned} For further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_. Args: params (iterable): iterable of parameters to optimize or dicts defining parameter groups lr (float, optional): learning rate (default: 1e-3) betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8) weight_decay (float, optional): weight decay (L2 penalty) (default: 0) amsgrad (boolean, optional): whether to use the AMSGrad variant of this algorithm from the paper `On the Convergence of Adam and Beyond`_ (default: False) .. _Adam\\: A Method for Stochastic Optimization: https://arxiv.org/abs/1412.6980 .. _On the Convergence of Adam and Beyond: https://openreview.net/forum?id=ryQu7f-RZ \"\"\" def __init__ ( self , params , lr = 1e-3 , betas = ( 0.9 , 0.999 ), eps = 1e-8 , weight_decay = 0 , amsgrad = False ): if not 0.0 <= lr : raise ValueError ( \"Invalid learning rate: {} \" . format ( lr )) if not 0.0 <= eps : raise ValueError ( \"Invalid epsilon value: {} \" . format ( eps )) if not 0.0 <= betas [ 0 ] < 1.0 : raise ValueError ( \"Invalid beta parameter at index 0: {} \" . format ( betas [ 0 ])) if not 0.0 <= betas [ 1 ] < 1.0 : raise ValueError ( \"Invalid beta parameter at index 1: {} \" . format ( betas [ 1 ])) if not 0.0 <= weight_decay : raise ValueError ( \"Invalid weight_decay value: {} \" . format ( weight_decay )) defaults = dict ( lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad ) super ( Adam , self ) . __init__ ( params , defaults ) def __setstate__ ( self , state ): super ( Adam , self ) . __setstate__ ( state ) for group in self . param_groups : group . setdefault ( 'amsgrad' , False ) @torch . no_grad () def step ( self , closure = None ): \"\"\"Performs a single optimization step. Args: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None : with torch . enable_grad (): loss = closure () for group in self . param_groups : params_with_grad = [] grads = [] exp_avgs = [] exp_avg_sqs = [] max_exp_avg_sqs = [] state_steps = [] beta1 , beta2 = group [ 'betas' ] for p in group [ 'params' ]: if p . grad is not None : params_with_grad . append ( p ) if p . grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) grads . append ( p . grad ) state = self . state [ p ] # Lazy state initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) if group [ 'amsgrad' ]: # Maintains max of all exp. moving avg. of sq. grad. values state [ 'max_exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) exp_avgs . append ( state [ 'exp_avg' ]) exp_avg_sqs . append ( state [ 'exp_avg_sq' ]) if group [ 'amsgrad' ]: max_exp_avg_sqs . append ( state [ 'max_exp_avg_sq' ]) # update the steps for each param group update state [ 'step' ] += 1 # record the step after step update state_steps . append ( state [ 'step' ]) F . adam ( params_with_grad , grads , exp_avgs , exp_avg_sqs , max_exp_avg_sqs , state_steps , amsgrad = group [ 'amsgrad' ], beta1 = beta1 , beta2 = beta2 , lr = group [ 'lr' ], weight_decay = group [ 'weight_decay' ], eps = group [ 'eps' ]) return loss","title":"DEFAULT_OPTIM_CLS"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS.step","text":"Performs a single optimization step. Parameters: Name Type Description Default closure callable A closure that reevaluates the model and returns the loss. None Source code in hover/core/neural.py @torch . no_grad () def step ( self , closure = None ): \"\"\"Performs a single optimization step. Args: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None : with torch . enable_grad (): loss = closure () for group in self . param_groups : params_with_grad = [] grads = [] exp_avgs = [] exp_avg_sqs = [] max_exp_avg_sqs = [] state_steps = [] beta1 , beta2 = group [ 'betas' ] for p in group [ 'params' ]: if p . grad is not None : params_with_grad . append ( p ) if p . grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) grads . append ( p . grad ) state = self . state [ p ] # Lazy state initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) if group [ 'amsgrad' ]: # Maintains max of all exp. moving avg. of sq. grad. values state [ 'max_exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) exp_avgs . append ( state [ 'exp_avg' ]) exp_avg_sqs . append ( state [ 'exp_avg_sq' ]) if group [ 'amsgrad' ]: max_exp_avg_sqs . append ( state [ 'max_exp_avg_sq' ]) # update the steps for each param group update state [ 'step' ] += 1 # record the step after step update state_steps . append ( state [ 'step' ]) F . adam ( params_with_grad , grads , exp_avgs , exp_avg_sqs , max_exp_avg_sqs , state_steps , amsgrad = group [ 'amsgrad' ], beta1 = beta1 , beta2 = beta2 , lr = group [ 'lr' ], weight_decay = group [ 'weight_decay' ], eps = group [ 'eps' ]) return loss","title":"step()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.__init__","text":"Create the VectorNet , loading parameters if available. Param Type Description vectorizer callable the feature -> vector function architecture class a torch.nn.Module child class state_dict_path str path to a (could-be-empty) torch state dict labels list list of str classification labels backup_state_dict bool whether to backup the loaded state dict optimizer_cls subclass of torch.optim.Optimizer pytorch optimizer class optimizer_kwargs dict pytorch optimizer kwargs verbose int logging verbosity level example_input any example input to the vectorizer Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , example_input = \"\" , ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `architecture` | `class` | a `torch.nn.Module` child class | | `state_dict_path` | `str` | path to a (could-be-empty) `torch` state dict | | `labels` | `list` | list of `str` classification labels | | `backup_state_dict` | `bool` | whether to backup the loaded state dict | | `optimizer_cls` | `subclass of torch.optim.Optimizer` | pytorch optimizer class | | `optimizer_kwargs` | `dict` | pytorch optimizer kwargs | | `verbose` | `int` | logging verbosity level | | `example_input` | any | example input to the vectorizer | \"\"\" assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose self . vectorizer = vectorizer self . example_input = example_input self . architecture = architecture self . setup_label_conversion ( labels ) self . _dynamic_params = {} # set a path to store updated parameters self . nn_update_path = state_dict_path if backup_state_dict and os . path . isfile ( state_dict_path ): state_dict_backup_path = f \" { state_dict_path } . { current_time ( '%Y%m %d %H%M%S' ) } \" copyfile ( state_dict_path , state_dict_backup_path ) # initialize an optimizer object and a dict to hold dynamic parameters optimizer_cls = optimizer_cls or self . __class__ . DEFAULT_OPTIM_CLS optimizer_kwargs = ( optimizer_kwargs or self . __class__ . DEFAULT_OPTIM_KWARGS . copy () ) def callback_reset_nn_optimizer (): \"\"\" Callback function which has access to optimizer init settings. \"\"\" self . nn_optimizer = optimizer_cls ( self . nn . parameters ()) assert isinstance ( self . nn_optimizer , torch . optim . Optimizer ), f \"Expected an optimizer, got { type ( self . nn_optimizer ) } \" self . _dynamic_params [ \"optimizer\" ] = optimizer_kwargs self . _callback_reset_nn_optimizer = callback_reset_nn_optimizer self . setup_nn ( use_existing_state_dict = True ) self . _setup_widgets ()","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.adjust_optimizer_params","text":"Dynamically change parameters of the neural net optimizer. Intended to be polymorphic in child classes and to be called per epoch. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Dynamically change parameters of the neural net optimizer.\" - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ])","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.auto_adjust_setup","text":"Auto-(re)create label encoder/decoder and neural net. Intended to be called in and out of the constructor. Param Type Description labels list list of str classification labels auto_skip bool skip when labels did not change Source code in hover/core/neural.py def auto_adjust_setup ( self , labels , auto_skip = True ): \"\"\" ???+ note \"Auto-(re)create label encoder/decoder and neural net.\" Intended to be called in and out of the constructor. | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `auto_skip` | `bool` | skip when labels did not change | \"\"\" # sanity check and skip assert isinstance ( labels , list ), f \"Expected a list of labels, got { labels } \" # if the sequence of labels matches label encoder exactly, skip label_match_flag = labels == sorted ( self . label_encoder . keys (), key = lambda k : self . label_encoder [ k ] ) if auto_skip and label_match_flag : return self . setup_label_conversion ( labels ) self . setup_nn ( use_existing_state_dict = False ) self . _good ( f \"adjusted to new list of labels: { labels } \" )","title":"auto_adjust_setup()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.evaluate","text":"Evaluate the VecNet against a dev set. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate ( self , dev_loader ): \"\"\" ???+ note \"Evaluate the VecNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose >= 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat","title":"evaluate()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.from_module","text":"Create a VectorNet model from a loadable module. Param Type Description model_module module or str (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable labels list list of str classification labels **kwargs forwarded to self.__init__() constructor Source code in hover/core/neural.py @classmethod def from_module ( cls , model_module , labels , ** kwargs ): \"\"\" ???+ note \"Create a VectorNet model from a loadable module.\" | Param | Type | Description | | :------------- | :--------- | :----------------------------------- | | `model_module` | `module` or `str` | (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable | | `labels` | `list` | list of `str` classification labels | | `**kwargs` | | forwarded to `self.__init__()` constructor | \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ** kwargs , ) return model","title":"from_module()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.load","text":"Load neural net parameters if possible. Can be directed to a custom state dict. Param Type Description load_path str path to a torch state dict Source code in hover/core/neural.py def load ( self , load_path = None ): \"\"\" ???+ note \"Load neural net parameters if possible.\" Can be directed to a custom state dict. | Param | Type | Description | | :---------- | :--------- | :--------------------------- | | `load_path` | `str` | path to a `torch` state dict | \"\"\" load_path = load_path or self . nn_update_path # if the architecture cannot match the state dict, skip the load and warn try : self . nn . load_state_dict ( torch . load ( load_path )) self . _info ( f \"loaded state dict { load_path } .\" ) except Exception as e : self . _warn ( f \"load VectorNet state path failed with { type ( e ) } : { e } \" )","title":"load()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.manifold_trajectory","text":"Compute a propagation trajectory of the dataset manifold through the neural net. vectorize inps forward propagate, keeping intermediates fit intermediates to N-D manifolds fit manifolds using Procrustes shape analysis fit shapes to trajectory splines Param Type Description inps dynamic (a list of) input features to vectorize method str reduction method: \"umap\" or \"ivis\" reducer_kwargs kwargs to forward to dimensionality reduction spline_kwargs kwargs to forward to spline calculation Source code in hover/core/neural.py def manifold_trajectory ( self , inps , method = \"umap\" , reducer_kwargs = None , spline_kwargs = None ): \"\"\" ???+ note \"Compute a propagation trajectory of the dataset manifold through the neural net.\" 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to N-D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines | Param | Type | Description | | :------- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | | `method` | `str` | reduction method: `\"umap\"` or `\"ivis\"` | | `reducer_kwargs` | | kwargs to forward to dimensionality reduction | | `spline_kwargs` | | kwargs to forward to spline calculation | \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline reducer_kwargs = reducer_kwargs or {} spline_kwargs = spline_kwargs or {} # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method , ** reducer_kwargs ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** spline_kwargs ) return traj_arr , seq_arr , disparities","title":"manifold_trajectory()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.predict_proba","text":"End-to-end single/multi-piece prediction from inp to class probabilities. Param Type Description inps dynamic (a list of) input features to vectorize Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" ???+ note \"End-to-end single/multi-piece prediction from inp to class probabilities.\" | Param | Type | Description | | :----- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = not isinstance ( inps , list ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ( np . array ([ self . vectorizer ( _inp ) for _inp in inps ])) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs","title":"predict_proba()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.prepare_loader","text":"Create dataloader from SupervisableDataset with implied vectorizer(s). Param Type Description dataset hover.core.dataset.SupervisableDataset the dataset to load key str \"train\", \"dev\", or \"test\" **kwargs forwarded to dataset.loader() Source code in hover/core/neural.py def prepare_loader ( self , dataset , key , ** kwargs ): \"\"\" ???+ note \"Create dataloader from `SupervisableDataset` with implied vectorizer(s).\" | Param | Type | Description | | :--------- | :---- | :------------------------- | | `dataset` | `hover.core.dataset.SupervisableDataset` | the dataset to load | | `key` | `str` | \"train\", \"dev\", or \"test\" | | `**kwargs` | | forwarded to `dataset.loader()` | \"\"\" return dataset . loader ( key , self . vectorizer , ** kwargs )","title":"prepare_loader()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.save","text":"Save the current state dict with authorization to overwrite. Param Type Description save_path str option alternative path to state dict Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" ???+ note \"Save the current state dict with authorization to overwrite.\" | Param | Type | Description | | :---------- | :---- | :------------------------------------ | | `save_path` | `str` | option alternative path to state dict | \"\"\" save_path = save_path or self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) verb = \"overwrote\" if os . path . isfile ( save_path ) else \"saved\" self . _info ( f \" { verb } state dict { save_path } .\" )","title":"save()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.setup_label_conversion","text":"Set up label encoder/decoder and number of classes. Param Type Description labels list list of str classification labels Source code in hover/core/neural.py def setup_label_conversion ( self , labels ): \"\"\" ???+ note \"Set up label encoder/decoder and number of classes.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | \"\"\" self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder )","title":"setup_label_conversion()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.setup_nn","text":"Set up neural network and optimizers. Intended to be called in and out of the constructor. will try to load parameters from state dict by default option to override and discard previous state dict often used when the classification targets have changed Param Type Description labels list list of str classification labels use_existing_state_dict bool whether to use existing state dict Source code in hover/core/neural.py def setup_nn ( self , use_existing_state_dict = True ): \"\"\" ???+ note \"Set up neural network and optimizers.\" Intended to be called in and out of the constructor. - will try to load parameters from state dict by default - option to override and discard previous state dict - often used when the classification targets have changed | Param | Type | Description | | :------------------------ | :--------- | :----------------------------------- | | `labels` | `list` | list of `str` classification labels | | `use_existing_state_dict` | `bool` | whether to use existing state dict | \"\"\" # set up vectorizer and the neural network with appropriate dimensions vec_dim = self . vectorizer ( self . example_input ) . shape [ 0 ] self . nn = self . architecture ( vec_dim , self . num_classes ) self . _callback_reset_nn_optimizer () state_dict_exists = os . path . isfile ( self . nn_update_path ) # if state dict exists, load it (when consistent) or overwrite if state_dict_exists : if use_existing_state_dict : self . load ( self . nn_update_path ) else : self . save ( self . nn_update_path ) self . _good ( f \"reset neural net: in { vec_dim } out { self . num_classes } .\" )","title":"setup_nn()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train","text":"Train the neural network part of the VecNet. intended to be coupled with self.train_batch(). Param Type Description train_loader torch.utils.data.DataLoader train set dev_loader torch.utils.data.DataLoader dev set epochs int number of epochs to train Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = None ): \"\"\" ???+ note \"Train the neural network part of the VecNet.\" - intended to be coupled with self.train_batch(). | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | | `epochs` | `int` | number of epochs to train | \"\"\" epochs = epochs or self . epochs_slider . value train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader ) if dev_loader is not None : dev_loader = train_loader acc , conf_mat = self . evaluate ( dev_loader ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_batch","text":"Train the neural network for one batch. Param Type Description loaded_input torch.Tensor input tensor loaded_output torch.Tensor output tensor Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output ): \"\"\" ???+ note \"Train the neural network for one batch.\" | Param | Type | Description | | :-------------- | :------------- | :-------------------- | | `loaded_input` | `torch.Tensor` | input tensor | | `loaded_output` | `torch.Tensor` | output tensor | \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = F . cross_entropy ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , )","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_epoch","text":"Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Param Type Description train_loader torch.utils.data.DataLoader train set *args arguments to forward to train_batch **kwargs kwargs to forward to train_batch Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" ???+ note \"Train the neural network for one epoch.\" - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `*args` | | arguments to forward to `train_batch` | | `**kwargs` | | kwargs to forward to `train_batch` | \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs )","title":"train_epoch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.view","text":"Overall layout when plotted. Source code in hover/core/neural.py def view ( self ): \"\"\" ???+ note \"Overall layout when plotted.\" \"\"\" return self . _layout_widgets ()","title":"view()"},{"location":"pages/reference/core-neural/#hover.core.neural","text":"","title":"hover.core.neural"},{"location":"pages/reference/core-neural/#hover.core.neural.BaseVectorNet","text":"","title":"BaseVectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet","text":"","title":"VectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS","text":"","title":"DEFAULT_OPTIM_CLS"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS.step","text":"","title":"step()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.adjust_optimizer_params","text":"","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.auto_adjust_setup","text":"","title":"auto_adjust_setup()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.evaluate","text":"","title":"evaluate()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.from_module","text":"","title":"from_module()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.load","text":"","title":"load()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.manifold_trajectory","text":"","title":"manifold_trajectory()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.predict_proba","text":"","title":"predict_proba()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.prepare_loader","text":"","title":"prepare_loader()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.save","text":"","title":"save()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.setup_label_conversion","text":"","title":"setup_label_conversion()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.setup_nn","text":"","title":"setup_nn()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train","text":"","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_batch","text":"","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_epoch","text":"","title":"train_epoch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.view","text":"","title":"view()"},{"location":"pages/reference/core-representation/","text":"hover.core.representation.reduction Linker data structures which tie (potentially multiple) dimensionality reducers to arrays. The point is to make it clear which reduction is in reference to which array. Icing on the cake: unify the syntax across different kinds of reducers. DimensionalityReducer ( Loggable ) Source code in hover/core/representation/reduction.py class DimensionalityReducer ( Loggable ): def __init__ ( self , array ): \"\"\" ???+ note \"Link self to the shared input array for reduction methods.\" | Param | Type | Description | | :------ | :----------- | :---------------------------- | | `array` | `np.ndarray` | the input array to fit on | \"\"\" self . reference_array = array @staticmethod def create_reducer ( method , * args , ** kwargs ): \"\"\" ???+ note \"Handle kwarg translation and dynamic imports.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | translated and forwarded | \"\"\" if method == \"umap\" : import umap reducer_cls = umap . UMAP elif method == \"ivis\" : import ivis reducer_cls = ivis . Ivis else : raise ValueError ( \"Expected 'umap' or 'ivis' as reduction method\" ) translated_kwargs = kwargs . copy () for _key , _value in kwargs . items (): _trans_dict = KWARG_TRANSLATOR . get ( _key , {}) if method in _trans_dict : _trans_key = _trans_dict [ method ] translated_kwargs . pop ( _key ) translated_kwargs [ _trans_key ] = _value reducer = reducer_cls ( * args , ** translated_kwargs ) return reducer def fit_transform ( self , method , * args , ** kwargs ): \"\"\" ???+ note \"Fit and transform an array and store the reducer.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | forwarded to the reducer | \"\"\" reducer = DimensionalityReducer . create_reducer ( method , * args , ** kwargs ) embedding = reducer . fit_transform ( self . reference_array ) setattr ( self , method , reducer ) return embedding def transform ( self , array , method ): \"\"\" ???+ note \"Transform an array with a already-fitted reducer.\" | Param | Type | Description | | :--------- | :----------- | :----------------------- | | `array` | `np.ndarray` | the array to transform | | `method` | `str` | `\"umap\"` or `\"ivis\"` | \"\"\" assert method in [ \"umap\" , \"ivis\" ], self . method_error_msg assert isinstance ( array , np . ndarray ), f \"Expected np.ndarray, got { type ( array ) } \" # edge case: array is too small if array . shape [ 0 ] < 1 : return np . array ([]) reducer = getattr ( self , method ) return reducer . transform ( array ) __init__ ( self , array ) special Link self to the shared input array for reduction methods. Param Type Description array np.ndarray the input array to fit on Source code in hover/core/representation/reduction.py def __init__ ( self , array ): \"\"\" ???+ note \"Link self to the shared input array for reduction methods.\" | Param | Type | Description | | :------ | :----------- | :---------------------------- | | `array` | `np.ndarray` | the input array to fit on | \"\"\" self . reference_array = array create_reducer ( method , * args , ** kwargs ) staticmethod Handle kwarg translation and dynamic imports. Param Type Description method str \"umap\" or \"ivis\" *args forwarded to the reducer **kwargs translated and forwarded Source code in hover/core/representation/reduction.py @staticmethod def create_reducer ( method , * args , ** kwargs ): \"\"\" ???+ note \"Handle kwarg translation and dynamic imports.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | translated and forwarded | \"\"\" if method == \"umap\" : import umap reducer_cls = umap . UMAP elif method == \"ivis\" : import ivis reducer_cls = ivis . Ivis else : raise ValueError ( \"Expected 'umap' or 'ivis' as reduction method\" ) translated_kwargs = kwargs . copy () for _key , _value in kwargs . items (): _trans_dict = KWARG_TRANSLATOR . get ( _key , {}) if method in _trans_dict : _trans_key = _trans_dict [ method ] translated_kwargs . pop ( _key ) translated_kwargs [ _trans_key ] = _value reducer = reducer_cls ( * args , ** translated_kwargs ) return reducer fit_transform ( self , method , * args , ** kwargs ) Fit and transform an array and store the reducer. Param Type Description method str \"umap\" or \"ivis\" *args forwarded to the reducer **kwargs forwarded to the reducer Source code in hover/core/representation/reduction.py def fit_transform ( self , method , * args , ** kwargs ): \"\"\" ???+ note \"Fit and transform an array and store the reducer.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | forwarded to the reducer | \"\"\" reducer = DimensionalityReducer . create_reducer ( method , * args , ** kwargs ) embedding = reducer . fit_transform ( self . reference_array ) setattr ( self , method , reducer ) return embedding transform ( self , array , method ) Transform an array with a already-fitted reducer. Param Type Description array np.ndarray the array to transform method str \"umap\" or \"ivis\" Source code in hover/core/representation/reduction.py def transform ( self , array , method ): \"\"\" ???+ note \"Transform an array with a already-fitted reducer.\" | Param | Type | Description | | :--------- | :----------- | :----------------------- | | `array` | `np.ndarray` | the array to transform | | `method` | `str` | `\"umap\"` or `\"ivis\"` | \"\"\" assert method in [ \"umap\" , \"ivis\" ], self . method_error_msg assert isinstance ( array , np . ndarray ), f \"Expected np.ndarray, got { type ( array ) } \" # edge case: array is too small if array . shape [ 0 ] < 1 : return np . array ([]) reducer = getattr ( self , method ) return reducer . transform ( array )","title":"hover.core.representation"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction","text":"Linker data structures which tie (potentially multiple) dimensionality reducers to arrays. The point is to make it clear which reduction is in reference to which array. Icing on the cake: unify the syntax across different kinds of reducers.","title":"reduction"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer","text":"Source code in hover/core/representation/reduction.py class DimensionalityReducer ( Loggable ): def __init__ ( self , array ): \"\"\" ???+ note \"Link self to the shared input array for reduction methods.\" | Param | Type | Description | | :------ | :----------- | :---------------------------- | | `array` | `np.ndarray` | the input array to fit on | \"\"\" self . reference_array = array @staticmethod def create_reducer ( method , * args , ** kwargs ): \"\"\" ???+ note \"Handle kwarg translation and dynamic imports.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | translated and forwarded | \"\"\" if method == \"umap\" : import umap reducer_cls = umap . UMAP elif method == \"ivis\" : import ivis reducer_cls = ivis . Ivis else : raise ValueError ( \"Expected 'umap' or 'ivis' as reduction method\" ) translated_kwargs = kwargs . copy () for _key , _value in kwargs . items (): _trans_dict = KWARG_TRANSLATOR . get ( _key , {}) if method in _trans_dict : _trans_key = _trans_dict [ method ] translated_kwargs . pop ( _key ) translated_kwargs [ _trans_key ] = _value reducer = reducer_cls ( * args , ** translated_kwargs ) return reducer def fit_transform ( self , method , * args , ** kwargs ): \"\"\" ???+ note \"Fit and transform an array and store the reducer.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | forwarded to the reducer | \"\"\" reducer = DimensionalityReducer . create_reducer ( method , * args , ** kwargs ) embedding = reducer . fit_transform ( self . reference_array ) setattr ( self , method , reducer ) return embedding def transform ( self , array , method ): \"\"\" ???+ note \"Transform an array with a already-fitted reducer.\" | Param | Type | Description | | :--------- | :----------- | :----------------------- | | `array` | `np.ndarray` | the array to transform | | `method` | `str` | `\"umap\"` or `\"ivis\"` | \"\"\" assert method in [ \"umap\" , \"ivis\" ], self . method_error_msg assert isinstance ( array , np . ndarray ), f \"Expected np.ndarray, got { type ( array ) } \" # edge case: array is too small if array . shape [ 0 ] < 1 : return np . array ([]) reducer = getattr ( self , method ) return reducer . transform ( array )","title":"DimensionalityReducer"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.__init__","text":"Link self to the shared input array for reduction methods. Param Type Description array np.ndarray the input array to fit on Source code in hover/core/representation/reduction.py def __init__ ( self , array ): \"\"\" ???+ note \"Link self to the shared input array for reduction methods.\" | Param | Type | Description | | :------ | :----------- | :---------------------------- | | `array` | `np.ndarray` | the input array to fit on | \"\"\" self . reference_array = array","title":"__init__()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.create_reducer","text":"Handle kwarg translation and dynamic imports. Param Type Description method str \"umap\" or \"ivis\" *args forwarded to the reducer **kwargs translated and forwarded Source code in hover/core/representation/reduction.py @staticmethod def create_reducer ( method , * args , ** kwargs ): \"\"\" ???+ note \"Handle kwarg translation and dynamic imports.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | translated and forwarded | \"\"\" if method == \"umap\" : import umap reducer_cls = umap . UMAP elif method == \"ivis\" : import ivis reducer_cls = ivis . Ivis else : raise ValueError ( \"Expected 'umap' or 'ivis' as reduction method\" ) translated_kwargs = kwargs . copy () for _key , _value in kwargs . items (): _trans_dict = KWARG_TRANSLATOR . get ( _key , {}) if method in _trans_dict : _trans_key = _trans_dict [ method ] translated_kwargs . pop ( _key ) translated_kwargs [ _trans_key ] = _value reducer = reducer_cls ( * args , ** translated_kwargs ) return reducer","title":"create_reducer()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.fit_transform","text":"Fit and transform an array and store the reducer. Param Type Description method str \"umap\" or \"ivis\" *args forwarded to the reducer **kwargs forwarded to the reducer Source code in hover/core/representation/reduction.py def fit_transform ( self , method , * args , ** kwargs ): \"\"\" ???+ note \"Fit and transform an array and store the reducer.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | forwarded to the reducer | \"\"\" reducer = DimensionalityReducer . create_reducer ( method , * args , ** kwargs ) embedding = reducer . fit_transform ( self . reference_array ) setattr ( self , method , reducer ) return embedding","title":"fit_transform()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.transform","text":"Transform an array with a already-fitted reducer. Param Type Description array np.ndarray the array to transform method str \"umap\" or \"ivis\" Source code in hover/core/representation/reduction.py def transform ( self , array , method ): \"\"\" ???+ note \"Transform an array with a already-fitted reducer.\" | Param | Type | Description | | :--------- | :----------- | :----------------------- | | `array` | `np.ndarray` | the array to transform | | `method` | `str` | `\"umap\"` or `\"ivis\"` | \"\"\" assert method in [ \"umap\" , \"ivis\" ], self . method_error_msg assert isinstance ( array , np . ndarray ), f \"Expected np.ndarray, got { type ( array ) } \" # edge case: array is too small if array . shape [ 0 ] < 1 : return np . array ([]) reducer = getattr ( self , method ) return reducer . transform ( array )","title":"transform()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction","text":"","title":"reduction"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer","text":"","title":"DimensionalityReducer"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.create_reducer","text":"","title":"create_reducer()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.fit_transform","text":"","title":"fit_transform()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.transform","text":"","title":"transform()"},{"location":"pages/reference/recipes/","text":"hover.recipes hover.recipes.stable High-level functions to produce an interactive annotation interface. Stable recipes whose function signatures should almost never change in the future. linked_annotator ( dataset , ** kwargs ) Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout simple_annotator ( dataset , ** kwargs ) Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout hover.recipes.experimental High-level functions to produce an interactive annotation interface. Experimental recipes whose function signatures might change significantly in the future. Use with caution. active_learning ( dataset , vecnet , ** kwargs ) Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vecnet VectorNet model to use in the loop **kwargs forwarded to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search and filter Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vecnet , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vecnet` | `VectorNet` | model to use in the loop | | `**kwargs` | | forwarded to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search and filter | \"\"\" layout , _ = _active_learning ( dataset , vecnet , ** kwargs ) return layout snorkel_crosscheck ( dataset , lf_list , ** kwargs ) Display the dataset for annotation, cross-checking with labeling functions. Param Type Description dataset SupervisableDataset the dataset to link to lf_list list a list of callables decorated by @hover.utils.snorkel_helper.labeling_function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSnorkelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect labeling functions make annotations search and filter Source code in hover/recipes/experimental.py @servable ( title = \"Snorkel Crosscheck\" ) def snorkel_crosscheck ( dataset , lf_list , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, cross-checking with labeling functions.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `lf_list` | `list` | a list of callables decorated by `@hover.utils.snorkel_helper.labeling_function` | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSnorkelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------- | :----------------- | :------------------ | | manage data subsets | inspect labeling functions | make annotations | search and filter | \"\"\" layout , _ = _snorkel_crosscheck ( dataset , lf_list , ** kwargs ) return layout hover.recipes.subroutine Building blocks of high-level recipes. Includes the following: functions for creating individual standard explorers appropriate for a dataset. active_learning_components ( dataset , vecnet , ** kwargs ) Active-learning specific components of a recipe. Param Type Description dataset SupervisableDataset the dataset to link to vecnet VectorNet vecnet to use in the loop **kwargs kwargs to forward to the BokehSoftLabelExplorer Source code in hover/recipes/subroutine.py def active_learning_components ( dataset , vecnet , ** kwargs ): \"\"\" ???+ note \"Active-learning specific components of a recipe.\" | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vecnet` | `VectorNet` | vecnet to use in the loop | | `**kwargs` | | kwargs to forward to the `BokehSoftLabelExplorer` | \"\"\" console = Console () softlabel = standard_softlabel ( dataset , ** kwargs ) feature_key = dataset . __class__ . FEATURE_KEY # patch coordinates for representational similarity analysis # some datasets may have multiple embeddings; use the one with lowest dimension embedding_cols = sorted ( softlabel . find_embedding_fields ()) manifold_dim , _ = re . findall ( r \"\\d+\" , embedding_cols [ 0 ]) manifold_dim = int ( manifold_dim ) manifold_traj_cols = embedding_cols [: manifold_dim ] for _col in manifold_traj_cols : _total_dim , _ = re . findall ( r \"\\d+\" , _col ) _total_dim = int ( _total_dim ) assert ( _total_dim == manifold_dim ), f \"Dim mismatch: { _total_dim } vs. { manifold_dim } \" softlabel . value_patch_by_slider ( _col , f \" { _col } _traj\" , title = \"Manifold trajectory step\" ) # recipe-specific widget model_trainer = Button ( label = \"Train model\" , button_type = \"primary\" ) def retrain_vecnet (): \"\"\" Callback subfunction 1 of 2. \"\"\" model_trainer . disabled = True console . print ( \"Start training... button will be disabled temporarily.\" ) dataset . setup_label_coding () vecnet . auto_adjust_setup ( dataset . classes ) train_loader = vecnet . prepare_loader ( dataset , \"train\" , smoothing_coeff = 0.2 ) if dataset . dfs [ \"dev\" ] . shape [ 0 ] > 0 : dev_loader = vecnet . prepare_loader ( dataset , \"dev\" ) else : dataset . _warn ( \"dev set is empty, borrowing train set for validation.\" ) dev_loader = train_loader _ = vecnet . train ( train_loader , dev_loader ) vecnet . save () console . print ( \"-- 1/2: retrained vecnet\" ) def update_softlabel_plot (): \"\"\" Callback subfunction 2 of 2. \"\"\" # combine inputs and compute outputs of all non-test subsets use_subsets = ( \"raw\" , \"train\" , \"dev\" ) inps = [] for _key in use_subsets : inps . extend ( dataset . dfs [ _key ][ feature_key ] . tolist ()) probs = vecnet . predict_proba ( inps ) labels = [ dataset . label_decoder [ _val ] for _val in probs . argmax ( axis =- 1 )] scores = probs . max ( axis =- 1 ) . tolist () traj_arr , _ , _ = vecnet . manifold_trajectory ( inps , method = \"umap\" , reducer_kwargs = dict ( dimension = manifold_dim ), spline_kwargs = dict ( points_per_step = 5 ), ) offset = 0 for _key in use_subsets : _length = dataset . dfs [ _key ] . shape [ 0 ] # skip subset if empty if _length == 0 : continue _slice = slice ( offset , offset + _length ) dataset . dfs [ _key ][ \"pred_label\" ] = labels [ _slice ] dataset . dfs [ _key ][ \"pred_score\" ] = scores [ _slice ] for i , _col in enumerate ( manifold_traj_cols ): # all steps, selected slice _traj = traj_arr [:, _slice , i ] # selected slice, all steps _traj = list ( np . swapaxes ( _traj , 0 , 1 )) dataset . dfs [ _key ][ f \" { _col } _traj\" ] = _traj offset += _length softlabel . _dynamic_callbacks [ \"adjust_patch_slider\" ]() softlabel . _update_sources () model_trainer . disabled = False console . print ( \"-- 2/2: updated predictions. Training button is re-enabled.\" ) def callback_sequence (): \"\"\" Overall callback function. \"\"\" retrain_vecnet () update_softlabel_plot () model_trainer . on_click ( callback_sequence ) return softlabel , model_trainer get_explorer_class ( task , feature ) Get the right hover.core.explorer class given a task and a feature. Can be useful for dynamically creating explorers without knowing the feature in advance. Param Type Description task str name of the task, which can be \"finder\" , \"annotator\" , \"margin\" , \"softlabel\" , or \"snorkel\" feature str name of the main feature, which can be \"text\" , \"audio\" or \"image\" Usage: # this creates an instance of BokehTextFinder explorer = get_explorer_class ( \"finder\" , \"text\" )( * args , ** kwargs ) Source code in hover/recipes/subroutine.py def get_explorer_class ( task , feature ): \"\"\" ???+ note \"Get the right `hover.core.explorer` class given a task and a feature.\" Can be useful for dynamically creating explorers without knowing the feature in advance. | Param | Type | Description | | :-------- | :---- | :----------------------------------- | | `task` | `str` | name of the task, which can be `\"finder\"`, `\"annotator\"`, `\"margin\"`, `\"softlabel\"`, or `\"snorkel\"` | | `feature` | `str` | name of the main feature, which can be `\"text\"`, `\"audio\"` or `\"image\"` | Usage: ```python # this creates an instance of BokehTextFinder explorer = get_explorer_class(\"finder\", \"text\")(*args, **kwargs) ``` \"\"\" assert task in EXPLORER_CATALOG , f \"Invalid task: { task } \" assert feature in EXPLORER_CATALOG [ task ], f \"Invalid feature: { feature } \" return EXPLORER_CATALOG [ task ][ feature ] recipe_layout ( * components , * , style = 'horizontal' ) Create a recipe-level layout of bokeh objects. Param Type Description *components bokeh objects objects to be plotted style str \"horizontal\" or \"vertical\" Source code in hover/recipes/subroutine.py def recipe_layout ( * components , style = \"horizontal\" ): \"\"\" ???+ note \"Create a recipe-level layout of bokeh objects.\" | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `*components` | `bokeh` objects | objects to be plotted | | `style` | `str` | \"horizontal\" or \"vertical\" | \"\"\" if style == \"horizontal\" : return row ( * components ) elif style == \"vertical\" : return column ( * components ) else : raise ValueError ( f \"Unexpected layout style { style } \" ) standard_annotator ( dataset , ** kwargs ) Set up a BokehDataAnnotator for a SupervisableDataset . The annotator has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset can commit annotations through selections in the \"raw\" subset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataAnnotator Source code in hover/recipes/subroutine.py def standard_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataAnnotator` for a `SupervisableDataset`.\" The annotator has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset - can commit annotations through selections in the \"raw\" subset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataAnnotator` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"annotator\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () annotator = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Annotator: apply labels to selected RAW points\" , ** kwargs , ) annotator . activate_search () annotator . plot () # subscribe for df updates dataset . subscribe_update_push ( annotator , { _k : _k for _k in subsets }) # annotators can commit to a dataset dataset . subscribe_data_commit ( annotator , { \"raw\" : \"raw\" }) # annotators by default link the selection for preview dataset . subscribe_selection_view ( annotator , [ \"raw\" , \"train\" , \"dev\" , \"test\" ]) return annotator standard_finder ( dataset , ** kwargs ) Set up a BokehDataFinder for a SupervisableDataset . The finder has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataFinder Source code in hover/recipes/subroutine.py def standard_finder ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataFinder` for a `SupervisableDataset`.\" The finder has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataFinder` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"finder\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () finder = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Finder: use search for highlight and filter\" , ** kwargs , ) finder . activate_search () finder . plot () # subscribe for df updates dataset . subscribe_update_push ( finder , { _k : _k for _k in subsets }) return finder standard_snorkel ( dataset , ** kwargs ) Set up a BokehSnorkelExplorer for a SupervisableDataset . The snorkel explorer has a few standard interactions with the dataset: read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" subscribe to all updates in those subsets Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehSnorkelExplorer Source code in hover/recipes/subroutine.py def standard_snorkel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSnorkelExplorer` for a `SupervisableDataset`.\" The snorkel explorer has a few standard interactions with the dataset: - read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" - subscribe to all updates in those subsets | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehSnorkelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"snorkel\" , feature ) # first \"static\" version of the plot snorkel = explorer_cls . from_dataset ( dataset , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }, title = \"Snorkel: \u25a1 for correct, x for incorrect, + for missed, o for hit; click on legends to hide or show LF\" , ** kwargs , ) snorkel . activate_search () snorkel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( snorkel , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }) return snorkel standard_softlabel ( dataset , ** kwargs ) Set up a BokehSoftLabelExplorer for a SupervisableDataset . The soft label explorer has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to BokehSoftLabelExplorer Source code in hover/recipes/subroutine.py def standard_softlabel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSoftLabelExplorer` for a `SupervisableDataset`.\" The soft label explorer has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to `BokehSoftLabelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"softlabel\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () softlabel = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, \"pred_label\" , \"pred_score\" , title = \"SoftLabel: inspect predictions and use score range as filter\" , ** kwargs , ) softlabel . activate_search () softlabel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( softlabel , { _k : _k for _k in subsets }) return softlabel","title":"hover.recipes"},{"location":"pages/reference/recipes/#hoverrecipes","text":"","title":"hover.recipes"},{"location":"pages/reference/recipes/#hover.recipes.stable","text":"High-level functions to produce an interactive annotation interface. Stable recipes whose function signatures should almost never change in the future.","title":"stable"},{"location":"pages/reference/recipes/#hover.recipes.stable.linked_annotator","text":"Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout","title":"linked_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable.simple_annotator","text":"Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout","title":"simple_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable","text":"","title":"stable"},{"location":"pages/reference/recipes/#hover.recipes.stable.linked_annotator","text":"","title":"linked_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable.simple_annotator","text":"","title":"simple_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.experimental","text":"High-level functions to produce an interactive annotation interface. Experimental recipes whose function signatures might change significantly in the future. Use with caution.","title":"experimental"},{"location":"pages/reference/recipes/#hover.recipes.experimental.active_learning","text":"Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vecnet VectorNet model to use in the loop **kwargs forwarded to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search and filter Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vecnet , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vecnet` | `VectorNet` | model to use in the loop | | `**kwargs` | | forwarded to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search and filter | \"\"\" layout , _ = _active_learning ( dataset , vecnet , ** kwargs ) return layout","title":"active_learning()"},{"location":"pages/reference/recipes/#hover.recipes.experimental.snorkel_crosscheck","text":"Display the dataset for annotation, cross-checking with labeling functions. Param Type Description dataset SupervisableDataset the dataset to link to lf_list list a list of callables decorated by @hover.utils.snorkel_helper.labeling_function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSnorkelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect labeling functions make annotations search and filter Source code in hover/recipes/experimental.py @servable ( title = \"Snorkel Crosscheck\" ) def snorkel_crosscheck ( dataset , lf_list , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, cross-checking with labeling functions.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `lf_list` | `list` | a list of callables decorated by `@hover.utils.snorkel_helper.labeling_function` | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSnorkelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------- | :----------------- | :------------------ | | manage data subsets | inspect labeling functions | make annotations | search and filter | \"\"\" layout , _ = _snorkel_crosscheck ( dataset , lf_list , ** kwargs ) return layout","title":"snorkel_crosscheck()"},{"location":"pages/reference/recipes/#hover.recipes.experimental","text":"","title":"experimental"},{"location":"pages/reference/recipes/#hover.recipes.experimental.active_learning","text":"","title":"active_learning()"},{"location":"pages/reference/recipes/#hover.recipes.experimental.snorkel_crosscheck","text":"","title":"snorkel_crosscheck()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine","text":"Building blocks of high-level recipes. Includes the following: functions for creating individual standard explorers appropriate for a dataset.","title":"subroutine"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.active_learning_components","text":"Active-learning specific components of a recipe. Param Type Description dataset SupervisableDataset the dataset to link to vecnet VectorNet vecnet to use in the loop **kwargs kwargs to forward to the BokehSoftLabelExplorer Source code in hover/recipes/subroutine.py def active_learning_components ( dataset , vecnet , ** kwargs ): \"\"\" ???+ note \"Active-learning specific components of a recipe.\" | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vecnet` | `VectorNet` | vecnet to use in the loop | | `**kwargs` | | kwargs to forward to the `BokehSoftLabelExplorer` | \"\"\" console = Console () softlabel = standard_softlabel ( dataset , ** kwargs ) feature_key = dataset . __class__ . FEATURE_KEY # patch coordinates for representational similarity analysis # some datasets may have multiple embeddings; use the one with lowest dimension embedding_cols = sorted ( softlabel . find_embedding_fields ()) manifold_dim , _ = re . findall ( r \"\\d+\" , embedding_cols [ 0 ]) manifold_dim = int ( manifold_dim ) manifold_traj_cols = embedding_cols [: manifold_dim ] for _col in manifold_traj_cols : _total_dim , _ = re . findall ( r \"\\d+\" , _col ) _total_dim = int ( _total_dim ) assert ( _total_dim == manifold_dim ), f \"Dim mismatch: { _total_dim } vs. { manifold_dim } \" softlabel . value_patch_by_slider ( _col , f \" { _col } _traj\" , title = \"Manifold trajectory step\" ) # recipe-specific widget model_trainer = Button ( label = \"Train model\" , button_type = \"primary\" ) def retrain_vecnet (): \"\"\" Callback subfunction 1 of 2. \"\"\" model_trainer . disabled = True console . print ( \"Start training... button will be disabled temporarily.\" ) dataset . setup_label_coding () vecnet . auto_adjust_setup ( dataset . classes ) train_loader = vecnet . prepare_loader ( dataset , \"train\" , smoothing_coeff = 0.2 ) if dataset . dfs [ \"dev\" ] . shape [ 0 ] > 0 : dev_loader = vecnet . prepare_loader ( dataset , \"dev\" ) else : dataset . _warn ( \"dev set is empty, borrowing train set for validation.\" ) dev_loader = train_loader _ = vecnet . train ( train_loader , dev_loader ) vecnet . save () console . print ( \"-- 1/2: retrained vecnet\" ) def update_softlabel_plot (): \"\"\" Callback subfunction 2 of 2. \"\"\" # combine inputs and compute outputs of all non-test subsets use_subsets = ( \"raw\" , \"train\" , \"dev\" ) inps = [] for _key in use_subsets : inps . extend ( dataset . dfs [ _key ][ feature_key ] . tolist ()) probs = vecnet . predict_proba ( inps ) labels = [ dataset . label_decoder [ _val ] for _val in probs . argmax ( axis =- 1 )] scores = probs . max ( axis =- 1 ) . tolist () traj_arr , _ , _ = vecnet . manifold_trajectory ( inps , method = \"umap\" , reducer_kwargs = dict ( dimension = manifold_dim ), spline_kwargs = dict ( points_per_step = 5 ), ) offset = 0 for _key in use_subsets : _length = dataset . dfs [ _key ] . shape [ 0 ] # skip subset if empty if _length == 0 : continue _slice = slice ( offset , offset + _length ) dataset . dfs [ _key ][ \"pred_label\" ] = labels [ _slice ] dataset . dfs [ _key ][ \"pred_score\" ] = scores [ _slice ] for i , _col in enumerate ( manifold_traj_cols ): # all steps, selected slice _traj = traj_arr [:, _slice , i ] # selected slice, all steps _traj = list ( np . swapaxes ( _traj , 0 , 1 )) dataset . dfs [ _key ][ f \" { _col } _traj\" ] = _traj offset += _length softlabel . _dynamic_callbacks [ \"adjust_patch_slider\" ]() softlabel . _update_sources () model_trainer . disabled = False console . print ( \"-- 2/2: updated predictions. Training button is re-enabled.\" ) def callback_sequence (): \"\"\" Overall callback function. \"\"\" retrain_vecnet () update_softlabel_plot () model_trainer . on_click ( callback_sequence ) return softlabel , model_trainer","title":"active_learning_components()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.get_explorer_class","text":"Get the right hover.core.explorer class given a task and a feature. Can be useful for dynamically creating explorers without knowing the feature in advance. Param Type Description task str name of the task, which can be \"finder\" , \"annotator\" , \"margin\" , \"softlabel\" , or \"snorkel\" feature str name of the main feature, which can be \"text\" , \"audio\" or \"image\" Usage: # this creates an instance of BokehTextFinder explorer = get_explorer_class ( \"finder\" , \"text\" )( * args , ** kwargs ) Source code in hover/recipes/subroutine.py def get_explorer_class ( task , feature ): \"\"\" ???+ note \"Get the right `hover.core.explorer` class given a task and a feature.\" Can be useful for dynamically creating explorers without knowing the feature in advance. | Param | Type | Description | | :-------- | :---- | :----------------------------------- | | `task` | `str` | name of the task, which can be `\"finder\"`, `\"annotator\"`, `\"margin\"`, `\"softlabel\"`, or `\"snorkel\"` | | `feature` | `str` | name of the main feature, which can be `\"text\"`, `\"audio\"` or `\"image\"` | Usage: ```python # this creates an instance of BokehTextFinder explorer = get_explorer_class(\"finder\", \"text\")(*args, **kwargs) ``` \"\"\" assert task in EXPLORER_CATALOG , f \"Invalid task: { task } \" assert feature in EXPLORER_CATALOG [ task ], f \"Invalid feature: { feature } \" return EXPLORER_CATALOG [ task ][ feature ]","title":"get_explorer_class()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.recipe_layout","text":"Create a recipe-level layout of bokeh objects. Param Type Description *components bokeh objects objects to be plotted style str \"horizontal\" or \"vertical\" Source code in hover/recipes/subroutine.py def recipe_layout ( * components , style = \"horizontal\" ): \"\"\" ???+ note \"Create a recipe-level layout of bokeh objects.\" | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `*components` | `bokeh` objects | objects to be plotted | | `style` | `str` | \"horizontal\" or \"vertical\" | \"\"\" if style == \"horizontal\" : return row ( * components ) elif style == \"vertical\" : return column ( * components ) else : raise ValueError ( f \"Unexpected layout style { style } \" )","title":"recipe_layout()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_annotator","text":"Set up a BokehDataAnnotator for a SupervisableDataset . The annotator has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset can commit annotations through selections in the \"raw\" subset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataAnnotator Source code in hover/recipes/subroutine.py def standard_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataAnnotator` for a `SupervisableDataset`.\" The annotator has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset - can commit annotations through selections in the \"raw\" subset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataAnnotator` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"annotator\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () annotator = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Annotator: apply labels to selected RAW points\" , ** kwargs , ) annotator . activate_search () annotator . plot () # subscribe for df updates dataset . subscribe_update_push ( annotator , { _k : _k for _k in subsets }) # annotators can commit to a dataset dataset . subscribe_data_commit ( annotator , { \"raw\" : \"raw\" }) # annotators by default link the selection for preview dataset . subscribe_selection_view ( annotator , [ \"raw\" , \"train\" , \"dev\" , \"test\" ]) return annotator","title":"standard_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_finder","text":"Set up a BokehDataFinder for a SupervisableDataset . The finder has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataFinder Source code in hover/recipes/subroutine.py def standard_finder ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataFinder` for a `SupervisableDataset`.\" The finder has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataFinder` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"finder\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () finder = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Finder: use search for highlight and filter\" , ** kwargs , ) finder . activate_search () finder . plot () # subscribe for df updates dataset . subscribe_update_push ( finder , { _k : _k for _k in subsets }) return finder","title":"standard_finder()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_snorkel","text":"Set up a BokehSnorkelExplorer for a SupervisableDataset . The snorkel explorer has a few standard interactions with the dataset: read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" subscribe to all updates in those subsets Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehSnorkelExplorer Source code in hover/recipes/subroutine.py def standard_snorkel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSnorkelExplorer` for a `SupervisableDataset`.\" The snorkel explorer has a few standard interactions with the dataset: - read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" - subscribe to all updates in those subsets | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehSnorkelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"snorkel\" , feature ) # first \"static\" version of the plot snorkel = explorer_cls . from_dataset ( dataset , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }, title = \"Snorkel: \u25a1 for correct, x for incorrect, + for missed, o for hit; click on legends to hide or show LF\" , ** kwargs , ) snorkel . activate_search () snorkel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( snorkel , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }) return snorkel","title":"standard_snorkel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_softlabel","text":"Set up a BokehSoftLabelExplorer for a SupervisableDataset . The soft label explorer has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to BokehSoftLabelExplorer Source code in hover/recipes/subroutine.py def standard_softlabel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSoftLabelExplorer` for a `SupervisableDataset`.\" The soft label explorer has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to `BokehSoftLabelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"softlabel\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () softlabel = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, \"pred_label\" , \"pred_score\" , title = \"SoftLabel: inspect predictions and use score range as filter\" , ** kwargs , ) softlabel . activate_search () softlabel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( softlabel , { _k : _k for _k in subsets }) return softlabel","title":"standard_softlabel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine","text":"","title":"subroutine"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.active_learning_components","text":"","title":"active_learning_components()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.get_explorer_class","text":"","title":"get_explorer_class()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.recipe_layout","text":"","title":"recipe_layout()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_annotator","text":"","title":"standard_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_finder","text":"","title":"standard_finder()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_snorkel","text":"","title":"standard_snorkel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_softlabel","text":"","title":"standard_softlabel()"},{"location":"pages/reference/utils-bokeh_helper/","text":"Useful subroutines for working with bokeh in general. auto_label_color ( labels ) Create a label->hex color mapping dict. Source code in hover/utils/bokeh_helper.py def auto_label_color ( labels ): \"\"\" ???+ note \"Create a label->hex color mapping dict.\" \"\"\" use_labels = set ( labels ) use_labels . discard ( module_config . ABSTAIN_DECODED ) use_labels = sorted ( use_labels , reverse = False ) assert len ( use_labels ) <= 20 , \"Too many labels to support (max at 20)\" palette = Category10 [ 10 ] if len ( use_labels ) <= 10 else Category20 [ 20 ] color_dict = { module_config . ABSTAIN_DECODED : \"#dcdcdc\" , # gainsboro hex code ** { _l : _c for _l , _c in zip ( use_labels , palette )}, } return color_dict binder_proxy_app_url ( app_path , port = 5006 ) Find the URL of Bokeh server app in the current Binder session. Intended for visiting a Binder-hosted Bokeh server app. Will NOT work outside of Binder. Source code in hover/utils/bokeh_helper.py def binder_proxy_app_url ( app_path , port = 5006 ): \"\"\" ???+ note \"Find the URL of Bokeh server app in the current Binder session.\" Intended for visiting a Binder-hosted Bokeh server app. Will NOT work outside of Binder. \"\"\" service_url_path = os . environ . get ( \"JUPYTERHUB_SERVICE_PREFIX\" , \"/user/hover-binder/\" ) proxy_url_path = f \"proxy/ { port } / { app_path } \" base_url = \"https://hub.gke2.mybinder.org\" user_url_path = urljoin ( service_url_path , proxy_url_path ) full_url = urljoin ( base_url , user_url_path ) return full_url bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None ) Create a Bokeh hover tooltip from a template. label: whether to expect and show a \"label\" field. text: whether to expect and show a \"text\" field. image: whether to expect and show an \"image\" (url/path) field. audio: whether to expect and show an \"audio\" (url/path) field. coords: whether to show xy-coordinates. index: whether to show indices in the dataset. custom: {display: column} mapping of additional (text) tooltips. Source code in hover/utils/bokeh_helper.py def bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None , ): \"\"\" ???+ note \"Create a Bokeh hover tooltip from a template.\" - label: whether to expect and show a \"label\" field. - text: whether to expect and show a \"text\" field. - image: whether to expect and show an \"image\" (url/path) field. - audio: whether to expect and show an \"audio\" (url/path) field. - coords: whether to show xy-coordinates. - index: whether to show indices in the dataset. - custom: {display: column} mapping of additional (text) tooltips. \"\"\" # initialize mutable default value custom = custom or dict () # prepare encapsulation of a div box and an associated script divbox_prefix = \"\"\"<div class=\"out tooltip\"> \\n \"\"\" divbox_suffix = \"\"\"</div> \\n \"\"\" script_prefix = \"\"\"<script> \\n \"\"\" script_suffix = \"\"\"</script> \\n \"\"\" # dynamically add contents to the div box and the script divbox = divbox_prefix script = script_prefix if label : divbox += \"\"\" <div> <span style=\"font-size: 16px; color: #966;\"> Label: @label </span> </div> \"\"\" if text : divbox += \"\"\" <div style=\"word-wrap: break-word; width: 95%; text-overflow: ellipsis; line-height: 90%\"> <span style=\"font-size: 11px;\"> Text: @text </span> </div> \"\"\" if image : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Image: @image </span> <img src=\"@image\" height=\"60\" alt=\"@image\" width=\"60\" style=\"float: left; margin: 0px 0px 0px 0px;\" border=\"2\" ></img> </div> \"\"\" if audio : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Audio: @audio </span> <audio autoplay preload=\"auto\" src=\"@audio\"> </audio> </div> \"\"\" if coords : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #060;\"> Coordinates: ($x, $y) </span> </div> \"\"\" if index : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #066;\"> Index: [$index] </span> </div> \"\"\" for _key , _field in custom . items (): divbox += f \"\"\" <div> <span style=\"font-size: 12px; color: #606;\"> { _key } : @ { _field } </span> </div> \"\"\" divbox += divbox_suffix script += script_suffix return divbox + script remote_jupyter_proxy_url ( port ) Callable to configure Bokeh's show method when using a proxy (JupyterHub). Intended for rendering a in-notebook Bokeh app. Usage: # show(plot) show ( plot , notebook_url = remote_jupyter_proxy_url ) Source code in hover/utils/bokeh_helper.py def remote_jupyter_proxy_url ( port ): \"\"\" ???+ note \"Callable to configure Bokeh's show method when using a proxy (JupyterHub).\" Intended for rendering a in-notebook Bokeh app. Usage: ```python # show(plot) show(plot, notebook_url=remote_jupyter_proxy_url) ``` \"\"\" # find JupyterHub base (external) url, default to Binder base_url = os . environ . get ( \"JUPYTERHUB_BASE_URL\" , \"https://hub.gke2.mybinder.org\" ) host = urlparse ( base_url ) . netloc if port is None : return host service_url_path = os . environ . get ( \"JUPYTERHUB_SERVICE_PREFIX\" , \"/user/hover-binder/\" ) proxy_url_path = f \"proxy/ { port } \" user_url = urljoin ( base_url , service_url_path ) full_url = urljoin ( user_url , proxy_url_path ) return full_url servable ( title = None ) Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh. Usage: First wrap a function that creates bokeh plot elements: @servable () def dummy ( * args , ** kwargs ): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator ( * args , ** kwargs ) annotator . plot () return annotator . view () Then serve the app in your preferred setting: inline bokeh serve embedded app # in a Jupyter cell from bokeh.io import show , output_notebook output_notebook () show ( dummy ( * args , ** kwargs )) # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc () dummy ( * args , ** kwargs )( doc ) # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app' : dummy ( * args , ** kwargs ), 'my-other-app' : dummy ( * args , ** kwargs ), } server = Server ( app_dict ) server . start () Source code in hover/utils/bokeh_helper.py def servable ( title = None ): \"\"\" ???+ note \"Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh.\" Usage: First wrap a function that creates bokeh plot elements: ```python @servable() def dummy(*args, **kwargs): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator(*args, **kwargs) annotator.plot() return annotator.view() ``` Then serve the app in your preferred setting: === \"inline\" ```python # in a Jupyter cell from bokeh.io import show, output_notebook output_notebook() show(dummy(*args, **kwargs)) ``` === \"bokeh serve\" ```python # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc() dummy(*args, **kwargs)(doc) ``` === \"embedded app\" ```python # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app': dummy(*args, **kwargs), 'my-other-app': dummy(*args, **kwargs), } server = Server(app_dict) server.start() ``` \"\"\" def wrapper ( func ): @wraps ( func ) def wrapped ( * args , ** kwargs ): def handle ( doc ): \"\"\" Note that the handle must create a brand new bokeh model every time it is called. Reference: https://github.com/bokeh/bokeh/issues/8579 \"\"\" spinner = PreText ( text = \"loading...\" ) layout = column ( spinner ) def progress (): \"\"\" If still loading, show some progress. \"\"\" if spinner in layout . children : spinner . text += \".\" def load (): try : bokeh_model = func ( * args , ** kwargs ) layout . children . append ( bokeh_model ) layout . children . pop ( 0 ) except Exception as e : # exception handling message = PreText ( text = f \" { type ( e ) } : { e } \\n { format_exc () } \" ) layout . children . append ( message ) doc . add_root ( layout ) doc . add_periodic_callback ( progress , 5000 ) doc . add_timeout_callback ( load , 500 ) doc . title = title or func . __name__ return handle return wrapped return wrapper show_as_interactive ( obj , ** kwargs ) Wrap a bokeh LayoutDOM as an application to allow Python callbacks. Must have the same signature as bokeh.io.show() [https://docs.bokeh.org/en/latest/docs/reference/io.html#bokeh.io.show]. Source code in hover/utils/bokeh_helper.py def show_as_interactive ( obj , ** kwargs ): \"\"\" ???+ note \"Wrap a bokeh LayoutDOM as an application to allow Python callbacks.\" Must have the same signature as `bokeh.io.show()`[https://docs.bokeh.org/en/latest/docs/reference/io.html#bokeh.io.show]. \"\"\" from bokeh.io import show from bokeh.models.layouts import LayoutDOM assert isinstance ( obj , LayoutDOM ), f \"Expected Bokeh LayoutDOM, got { type ( obj ) } \" def handle ( doc ): doc . add_root ( column ( obj )) return show ( handle , ** kwargs )","title":"hover.utils.bokeh_helper"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.auto_label_color","text":"Create a label->hex color mapping dict. Source code in hover/utils/bokeh_helper.py def auto_label_color ( labels ): \"\"\" ???+ note \"Create a label->hex color mapping dict.\" \"\"\" use_labels = set ( labels ) use_labels . discard ( module_config . ABSTAIN_DECODED ) use_labels = sorted ( use_labels , reverse = False ) assert len ( use_labels ) <= 20 , \"Too many labels to support (max at 20)\" palette = Category10 [ 10 ] if len ( use_labels ) <= 10 else Category20 [ 20 ] color_dict = { module_config . ABSTAIN_DECODED : \"#dcdcdc\" , # gainsboro hex code ** { _l : _c for _l , _c in zip ( use_labels , palette )}, } return color_dict","title":"auto_label_color()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.binder_proxy_app_url","text":"Find the URL of Bokeh server app in the current Binder session. Intended for visiting a Binder-hosted Bokeh server app. Will NOT work outside of Binder. Source code in hover/utils/bokeh_helper.py def binder_proxy_app_url ( app_path , port = 5006 ): \"\"\" ???+ note \"Find the URL of Bokeh server app in the current Binder session.\" Intended for visiting a Binder-hosted Bokeh server app. Will NOT work outside of Binder. \"\"\" service_url_path = os . environ . get ( \"JUPYTERHUB_SERVICE_PREFIX\" , \"/user/hover-binder/\" ) proxy_url_path = f \"proxy/ { port } / { app_path } \" base_url = \"https://hub.gke2.mybinder.org\" user_url_path = urljoin ( service_url_path , proxy_url_path ) full_url = urljoin ( base_url , user_url_path ) return full_url","title":"binder_proxy_app_url()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.bokeh_hover_tooltip","text":"Create a Bokeh hover tooltip from a template. label: whether to expect and show a \"label\" field. text: whether to expect and show a \"text\" field. image: whether to expect and show an \"image\" (url/path) field. audio: whether to expect and show an \"audio\" (url/path) field. coords: whether to show xy-coordinates. index: whether to show indices in the dataset. custom: {display: column} mapping of additional (text) tooltips. Source code in hover/utils/bokeh_helper.py def bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None , ): \"\"\" ???+ note \"Create a Bokeh hover tooltip from a template.\" - label: whether to expect and show a \"label\" field. - text: whether to expect and show a \"text\" field. - image: whether to expect and show an \"image\" (url/path) field. - audio: whether to expect and show an \"audio\" (url/path) field. - coords: whether to show xy-coordinates. - index: whether to show indices in the dataset. - custom: {display: column} mapping of additional (text) tooltips. \"\"\" # initialize mutable default value custom = custom or dict () # prepare encapsulation of a div box and an associated script divbox_prefix = \"\"\"<div class=\"out tooltip\"> \\n \"\"\" divbox_suffix = \"\"\"</div> \\n \"\"\" script_prefix = \"\"\"<script> \\n \"\"\" script_suffix = \"\"\"</script> \\n \"\"\" # dynamically add contents to the div box and the script divbox = divbox_prefix script = script_prefix if label : divbox += \"\"\" <div> <span style=\"font-size: 16px; color: #966;\"> Label: @label </span> </div> \"\"\" if text : divbox += \"\"\" <div style=\"word-wrap: break-word; width: 95%; text-overflow: ellipsis; line-height: 90%\"> <span style=\"font-size: 11px;\"> Text: @text </span> </div> \"\"\" if image : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Image: @image </span> <img src=\"@image\" height=\"60\" alt=\"@image\" width=\"60\" style=\"float: left; margin: 0px 0px 0px 0px;\" border=\"2\" ></img> </div> \"\"\" if audio : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Audio: @audio </span> <audio autoplay preload=\"auto\" src=\"@audio\"> </audio> </div> \"\"\" if coords : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #060;\"> Coordinates: ($x, $y) </span> </div> \"\"\" if index : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #066;\"> Index: [$index] </span> </div> \"\"\" for _key , _field in custom . items (): divbox += f \"\"\" <div> <span style=\"font-size: 12px; color: #606;\"> { _key } : @ { _field } </span> </div> \"\"\" divbox += divbox_suffix script += script_suffix return divbox + script","title":"bokeh_hover_tooltip()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.remote_jupyter_proxy_url","text":"Callable to configure Bokeh's show method when using a proxy (JupyterHub). Intended for rendering a in-notebook Bokeh app. Usage: # show(plot) show ( plot , notebook_url = remote_jupyter_proxy_url ) Source code in hover/utils/bokeh_helper.py def remote_jupyter_proxy_url ( port ): \"\"\" ???+ note \"Callable to configure Bokeh's show method when using a proxy (JupyterHub).\" Intended for rendering a in-notebook Bokeh app. Usage: ```python # show(plot) show(plot, notebook_url=remote_jupyter_proxy_url) ``` \"\"\" # find JupyterHub base (external) url, default to Binder base_url = os . environ . get ( \"JUPYTERHUB_BASE_URL\" , \"https://hub.gke2.mybinder.org\" ) host = urlparse ( base_url ) . netloc if port is None : return host service_url_path = os . environ . get ( \"JUPYTERHUB_SERVICE_PREFIX\" , \"/user/hover-binder/\" ) proxy_url_path = f \"proxy/ { port } \" user_url = urljoin ( base_url , service_url_path ) full_url = urljoin ( user_url , proxy_url_path ) return full_url","title":"remote_jupyter_proxy_url()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.servable","text":"Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh. Usage: First wrap a function that creates bokeh plot elements: @servable () def dummy ( * args , ** kwargs ): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator ( * args , ** kwargs ) annotator . plot () return annotator . view () Then serve the app in your preferred setting: inline bokeh serve embedded app # in a Jupyter cell from bokeh.io import show , output_notebook output_notebook () show ( dummy ( * args , ** kwargs )) # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc () dummy ( * args , ** kwargs )( doc ) # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app' : dummy ( * args , ** kwargs ), 'my-other-app' : dummy ( * args , ** kwargs ), } server = Server ( app_dict ) server . start () Source code in hover/utils/bokeh_helper.py def servable ( title = None ): \"\"\" ???+ note \"Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh.\" Usage: First wrap a function that creates bokeh plot elements: ```python @servable() def dummy(*args, **kwargs): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator(*args, **kwargs) annotator.plot() return annotator.view() ``` Then serve the app in your preferred setting: === \"inline\" ```python # in a Jupyter cell from bokeh.io import show, output_notebook output_notebook() show(dummy(*args, **kwargs)) ``` === \"bokeh serve\" ```python # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc() dummy(*args, **kwargs)(doc) ``` === \"embedded app\" ```python # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app': dummy(*args, **kwargs), 'my-other-app': dummy(*args, **kwargs), } server = Server(app_dict) server.start() ``` \"\"\" def wrapper ( func ): @wraps ( func ) def wrapped ( * args , ** kwargs ): def handle ( doc ): \"\"\" Note that the handle must create a brand new bokeh model every time it is called. Reference: https://github.com/bokeh/bokeh/issues/8579 \"\"\" spinner = PreText ( text = \"loading...\" ) layout = column ( spinner ) def progress (): \"\"\" If still loading, show some progress. \"\"\" if spinner in layout . children : spinner . text += \".\" def load (): try : bokeh_model = func ( * args , ** kwargs ) layout . children . append ( bokeh_model ) layout . children . pop ( 0 ) except Exception as e : # exception handling message = PreText ( text = f \" { type ( e ) } : { e } \\n { format_exc () } \" ) layout . children . append ( message ) doc . add_root ( layout ) doc . add_periodic_callback ( progress , 5000 ) doc . add_timeout_callback ( load , 500 ) doc . title = title or func . __name__ return handle return wrapped return wrapper","title":"servable()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.show_as_interactive","text":"Wrap a bokeh LayoutDOM as an application to allow Python callbacks. Must have the same signature as bokeh.io.show() [https://docs.bokeh.org/en/latest/docs/reference/io.html#bokeh.io.show]. Source code in hover/utils/bokeh_helper.py def show_as_interactive ( obj , ** kwargs ): \"\"\" ???+ note \"Wrap a bokeh LayoutDOM as an application to allow Python callbacks.\" Must have the same signature as `bokeh.io.show()`[https://docs.bokeh.org/en/latest/docs/reference/io.html#bokeh.io.show]. \"\"\" from bokeh.io import show from bokeh.models.layouts import LayoutDOM assert isinstance ( obj , LayoutDOM ), f \"Expected Bokeh LayoutDOM, got { type ( obj ) } \" def handle ( doc ): doc . add_root ( column ( obj )) return show ( handle , ** kwargs )","title":"show_as_interactive()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.auto_label_color","text":"","title":"auto_label_color()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.binder_proxy_app_url","text":"","title":"binder_proxy_app_url()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.bokeh_hover_tooltip","text":"","title":"bokeh_hover_tooltip()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.remote_jupyter_proxy_url","text":"","title":"remote_jupyter_proxy_url()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.servable","text":"","title":"servable()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.show_as_interactive","text":"","title":"show_as_interactive()"},{"location":"pages/reference/utils-snorkel_helper/","text":"labeling_function ( targets , label_encoder = None , ** kwargs ) Hover's flavor of the Snorkel labeling_function decorator. However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. assigns a UUID for easy identification keeps track of LF targets Param Type Description targets list of str labels that the labeling function is intended to create label_encoder dict {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a .snorkel attribute **kwargs forwarded to snorkel 's labeling_function() Source code in hover/utils/snorkel_helper.py def labeling_function ( targets , label_encoder = None , ** kwargs ): \"\"\" ???+ note \"Hover's flavor of the Snorkel labeling_function decorator.\" However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. - assigns a UUID for easy identification - keeps track of LF targets | Param | Type | Description | | :-------------- | :----- | :----------------------------------- | | `targets` | `list` of `str` | labels that the labeling function is intended to create | | `label_encoder` | `dict` | {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a `.snorkel` attribute | | `**kwargs` | | forwarded to `snorkel`'s `labeling_function()` | \"\"\" # lazy import so that the package does not require snorkel # Feb 3, 2022: snorkel's dependency handling is too strict # for other dependencies like NumPy, SciPy, SpaCy, etc. # Let's cite Snorkel and lazy import or copy functions. # DO NOT explicitly depend on Snorkel without confirming # that all builds/tests pass by Anaconda standards, else # we risk having to drop conda support. from snorkel.labeling import ( labeling_function as snorkel_lf , LabelingFunction as SnorkelLF , ) def wrapper ( func ): # set up kwargs for Snorkel's LF # a default name that can be overridden snorkel_kwargs = { \"name\" : func . __name__ } snorkel_kwargs . update ( kwargs ) # return value of hover's decorator lf = SnorkelLF ( f = func , ** snorkel_kwargs ) # additional attributes lf . uuid = uuid . uuid1 () lf . targets = targets [:] # link a snorkel-style labeling function if applicable if label_encoder : lf . label_encoder = label_encoder def snorkel_style_func ( x ): return lf . label_encoder [ func ( x )] lf . snorkel = snorkel_lf ( ** kwargs )( snorkel_style_func ) else : lf . label_encoder = None lf . snorkel = None return lf return wrapper","title":"hover.utils.snorkel_helper"},{"location":"pages/reference/utils-snorkel_helper/#hover.utils.snorkel_helper.labeling_function","text":"Hover's flavor of the Snorkel labeling_function decorator. However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. assigns a UUID for easy identification keeps track of LF targets Param Type Description targets list of str labels that the labeling function is intended to create label_encoder dict {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a .snorkel attribute **kwargs forwarded to snorkel 's labeling_function() Source code in hover/utils/snorkel_helper.py def labeling_function ( targets , label_encoder = None , ** kwargs ): \"\"\" ???+ note \"Hover's flavor of the Snorkel labeling_function decorator.\" However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. - assigns a UUID for easy identification - keeps track of LF targets | Param | Type | Description | | :-------------- | :----- | :----------------------------------- | | `targets` | `list` of `str` | labels that the labeling function is intended to create | | `label_encoder` | `dict` | {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a `.snorkel` attribute | | `**kwargs` | | forwarded to `snorkel`'s `labeling_function()` | \"\"\" # lazy import so that the package does not require snorkel # Feb 3, 2022: snorkel's dependency handling is too strict # for other dependencies like NumPy, SciPy, SpaCy, etc. # Let's cite Snorkel and lazy import or copy functions. # DO NOT explicitly depend on Snorkel without confirming # that all builds/tests pass by Anaconda standards, else # we risk having to drop conda support. from snorkel.labeling import ( labeling_function as snorkel_lf , LabelingFunction as SnorkelLF , ) def wrapper ( func ): # set up kwargs for Snorkel's LF # a default name that can be overridden snorkel_kwargs = { \"name\" : func . __name__ } snorkel_kwargs . update ( kwargs ) # return value of hover's decorator lf = SnorkelLF ( f = func , ** snorkel_kwargs ) # additional attributes lf . uuid = uuid . uuid1 () lf . targets = targets [:] # link a snorkel-style labeling function if applicable if label_encoder : lf . label_encoder = label_encoder def snorkel_style_func ( x ): return lf . label_encoder [ func ( x )] lf . snorkel = snorkel_lf ( ** kwargs )( snorkel_style_func ) else : lf . label_encoder = None lf . snorkel = None return lf return wrapper","title":"labeling_function()"},{"location":"pages/reference/utils-snorkel_helper/#hover.utils.snorkel_helper.labeling_function","text":"","title":"labeling_function()"},{"location":"pages/tutorial/t0-quickstart/","text":"Welcome to the basic use case of hover ! Let's say we want to label some data and call it a day. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: Ingredient 1 / 3: Raw Data Start with a spreadsheet loaded in pandas . We turn it into a SupervisableDataset designed for labeling: from hover.core.dataset import SupervisableTextDataset import pandas as pd example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(example_csv_path).sample(1000) df_raw[\"text\"] = df_raw[\"text\"].astype(str) # data is divided into 4 subsets: \"raw\" / \"train\" / \"dev\" / \"test\" # this example assumes no labeled data available., i.e. only \"raw\" df_raw[\"SUBSET\"] = \"raw\" # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df_raw, feature_key=\"text\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) FAQ What if I have multiple features? feature_key refers to the field that will be vectorized later on, which can be a JSON that encloses multiple features. For example, suppose our data entries look like this: { \"f1\" : \"foo\" , \"f2\" : \"bar\" , \"non_feature\" : \"abc\" } We can put f1 and f2 in a JSON and convert the entries like this: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Can I use audio or image data? Yes! Please check out the \"Guides\" section. Ingredient 2 / 3: Embedding A pre-trained embedding lets us group data points semantically. In particular, let's define a data -> embedding vector function. import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") Tips Caching dataset by itself stores the original features but not the corresponding vectors. To avoid vectorizing the same feature again and again, we could simply do: from functools import cache @cache def vectorizer ( feature ): # put code here If you'd like to limit the size of the cache, something like @lru_cache(maxsize=10000) could help. Check out functools for more options. Vectorizing multiple features Suppose we have multiple features enclosed in a JSON: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Also, suppose we have individual vectorizers likes this: def vectorizer_1 ( feature_1 ): # put code here def vectorizer_2 ( feature_2 ): # put code here Then we can define a composite vectorizer: import json import numpy as np def vectorizer ( feature_json ): data_dict = json . loads ( feature_json ) vectors = [] for field , func in [ ( \"f1\" , vectorizer_1 ), ( \"f2\" , vectorizer_2 ), ]: vectors . append ( func ( data_dict [ field ])) return np . concatenate ( vectors ) Ingredient 3 / 3: 2D Embedding We compute a 2D version of the pre-trained embedding to visualize the whole dataset. Hover has built-in methods for calling umap or ivis . Dependencies (when in your own environment) The libraries for this step are not directly required by hover : for umap: pip install umap-learn for ivis: pip install ivis[cpu] or pip install ivis[gpu] umap-learn is installed in this demo environment. # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) # what we did adds 'embed_2d_0' and 'embed_2d_1' columns to the DataFrames in dataset.dfs dataset.dfs[\"raw\"].head(5) Apply Labels We are ready for the annotation interface! from hover.recipes.stable import simple_annotator interactive_plot = simple_annotator(dataset) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') Tips: annotation interface basics Video guide Text guide There should be a SupervisableDataset board on the left and an BokehDataAnnotator on the right, each with a few buttons. SupervisableDataset BokehDataAnnotator push : push Dataset updates to the bokeh plots. commit : add data entries selected in the Annotator to a specified subset. dedup : deduplicate across subsets by feature (last in gets kept). export : save your data (all subsets) in a specified format. raw / train / dev / test : choose which subsets to display or hide. apply : apply the label input to the selected points in the raw subset only. We've essentially put the data into neighborboods based on the vectorizer, but the quality (homogeneity of labels) of such neighborhoods can vary. hover over any data point to see its tooltip. take advantage of different selection tools to apply labels at appropriate scales. the search widget might turn out useful. note that it does not select points but highlights them. @import url(\"../../../styles/monokai.css\");","title":"Quickstart"},{"location":"pages/tutorial/t0-quickstart/#ingredient-1-3-raw-data","text":"Start with a spreadsheet loaded in pandas . We turn it into a SupervisableDataset designed for labeling: from hover.core.dataset import SupervisableTextDataset import pandas as pd example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(example_csv_path).sample(1000) df_raw[\"text\"] = df_raw[\"text\"].astype(str) # data is divided into 4 subsets: \"raw\" / \"train\" / \"dev\" / \"test\" # this example assumes no labeled data available., i.e. only \"raw\" df_raw[\"SUBSET\"] = \"raw\" # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df_raw, feature_key=\"text\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) FAQ What if I have multiple features? feature_key refers to the field that will be vectorized later on, which can be a JSON that encloses multiple features. For example, suppose our data entries look like this: { \"f1\" : \"foo\" , \"f2\" : \"bar\" , \"non_feature\" : \"abc\" } We can put f1 and f2 in a JSON and convert the entries like this: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Can I use audio or image data? Yes! Please check out the \"Guides\" section.","title":"Ingredient 1 / 3: Raw Data"},{"location":"pages/tutorial/t0-quickstart/#ingredient-2-3-embedding","text":"A pre-trained embedding lets us group data points semantically. In particular, let's define a data -> embedding vector function. import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") Tips Caching dataset by itself stores the original features but not the corresponding vectors. To avoid vectorizing the same feature again and again, we could simply do: from functools import cache @cache def vectorizer ( feature ): # put code here If you'd like to limit the size of the cache, something like @lru_cache(maxsize=10000) could help. Check out functools for more options. Vectorizing multiple features Suppose we have multiple features enclosed in a JSON: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Also, suppose we have individual vectorizers likes this: def vectorizer_1 ( feature_1 ): # put code here def vectorizer_2 ( feature_2 ): # put code here Then we can define a composite vectorizer: import json import numpy as np def vectorizer ( feature_json ): data_dict = json . loads ( feature_json ) vectors = [] for field , func in [ ( \"f1\" , vectorizer_1 ), ( \"f2\" , vectorizer_2 ), ]: vectors . append ( func ( data_dict [ field ])) return np . concatenate ( vectors )","title":"Ingredient 2 / 3: Embedding"},{"location":"pages/tutorial/t0-quickstart/#ingredient-3-3-2d-embedding","text":"We compute a 2D version of the pre-trained embedding to visualize the whole dataset. Hover has built-in methods for calling umap or ivis . Dependencies (when in your own environment) The libraries for this step are not directly required by hover : for umap: pip install umap-learn for ivis: pip install ivis[cpu] or pip install ivis[gpu] umap-learn is installed in this demo environment. # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) # what we did adds 'embed_2d_0' and 'embed_2d_1' columns to the DataFrames in dataset.dfs dataset.dfs[\"raw\"].head(5)","title":"Ingredient 3 / 3: 2D Embedding"},{"location":"pages/tutorial/t0-quickstart/#apply-labels","text":"We are ready for the annotation interface! from hover.recipes.stable import simple_annotator interactive_plot = simple_annotator(dataset) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') Tips: annotation interface basics Video guide Text guide There should be a SupervisableDataset board on the left and an BokehDataAnnotator on the right, each with a few buttons. SupervisableDataset BokehDataAnnotator push : push Dataset updates to the bokeh plots. commit : add data entries selected in the Annotator to a specified subset. dedup : deduplicate across subsets by feature (last in gets kept). export : save your data (all subsets) in a specified format. raw / train / dev / test : choose which subsets to display or hide. apply : apply the label input to the selected points in the raw subset only. We've essentially put the data into neighborboods based on the vectorizer, but the quality (homogeneity of labels) of such neighborhoods can vary. hover over any data point to see its tooltip. take advantage of different selection tools to apply labels at appropriate scales. the search widget might turn out useful. note that it does not select points but highlights them. @import url(\"../../../styles/monokai.css\");","title":"Apply Labels"},{"location":"pages/tutorial/t1-active-learning/","text":"The most common usage of hover is through built-in recipe s like in the quickstart. Let's explore another recipe -- an active learning example. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: Fundamentals Hover recipe s are functions that take a SupervisableDataset and return an annotation interface. The SupervisableDataset is assumed to have some data and embeddings. Recap: Data & Embeddings Let's preprare a dataset with embeddings. This is almost the same as in the quickstart : from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) Recipe-Specific Ingredient Each recipe has different functionalities and potentially different signature. To utilize active learning, we need to specify how to get a model in the loop. hover considers the vectorizer as a \"frozen\" embedding and follows up with a neural network, which infers its own dimensionality from the vectorizer and the output classes. This architecture named VectorNet is the (default) basis of active learning in hover . Custom models It is possible to use a model other than VectorNet or its subclass. You will need to implement the following methods with the same signatures as VectorNet : train save predict_proba prepare_loader manifold_trajectory from hover.core.neural import VectorNet from hover.utils.common_nn import LogisticRegression # Create a model with vectorizer-NN architecture. # model.pt will point to a PyTorch state dict (to be created) # the label classes in the dataset can change, and vecnet can adjust to that vecnet = VectorNet(vectorizer, LogisticRegression, \"model.pt\", dataset.classes) # predict_proba accepts individual strings or list # text -> vector -> class probabilities # if no classes right now, will see an empty list print(vecnet.predict_proba(text)) print(vecnet.predict_proba([text])) Note how the callback dynamically takes dataset.classes , which means the model architecture will adapt when we add classes during annotation. Apply Labels Now we invoke the active_learning recipe. Tips: how recipes work programmatically In general, a recipe is a function taking a SupervisableDataset and other arguments based on its functionality. Here are a few common recipes: active_learning simple_annotator linked_annotator Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vecnet VectorNet model to use in the loop **kwargs forwarded to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search and filter Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vecnet , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vecnet` | `VectorNet` | model to use in the loop | | `**kwargs` | | forwarded to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search and filter | \"\"\" layout , _ = _active_learning ( dataset , vecnet , ** kwargs ) return layout Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout The recipe returns a handle function which bokeh can use to visualize an annotation interface in multiple settings. from hover.recipes.experimental import active_learning interactive_plot = active_learning(dataset, vecnet) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') Tips: annotation interface with multiple plots Video guide: leveraging linked selection Video guide: active learning Text guide: active learning Inspecting model predictions allows us to get an idea of how the current set of annotations will likely teach the model. locate the most valuable samples for further annotation. @import url(\"../../../styles/monokai.css\");","title":"Using Recipes"},{"location":"pages/tutorial/t1-active-learning/#fundamentals","text":"Hover recipe s are functions that take a SupervisableDataset and return an annotation interface. The SupervisableDataset is assumed to have some data and embeddings.","title":"Fundamentals"},{"location":"pages/tutorial/t1-active-learning/#recap-data-embeddings","text":"Let's preprare a dataset with embeddings. This is almost the same as in the quickstart : from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)","title":"Recap: Data &amp; Embeddings"},{"location":"pages/tutorial/t1-active-learning/#recipe-specific-ingredient","text":"Each recipe has different functionalities and potentially different signature. To utilize active learning, we need to specify how to get a model in the loop. hover considers the vectorizer as a \"frozen\" embedding and follows up with a neural network, which infers its own dimensionality from the vectorizer and the output classes. This architecture named VectorNet is the (default) basis of active learning in hover . Custom models It is possible to use a model other than VectorNet or its subclass. You will need to implement the following methods with the same signatures as VectorNet : train save predict_proba prepare_loader manifold_trajectory from hover.core.neural import VectorNet from hover.utils.common_nn import LogisticRegression # Create a model with vectorizer-NN architecture. # model.pt will point to a PyTorch state dict (to be created) # the label classes in the dataset can change, and vecnet can adjust to that vecnet = VectorNet(vectorizer, LogisticRegression, \"model.pt\", dataset.classes) # predict_proba accepts individual strings or list # text -> vector -> class probabilities # if no classes right now, will see an empty list print(vecnet.predict_proba(text)) print(vecnet.predict_proba([text])) Note how the callback dynamically takes dataset.classes , which means the model architecture will adapt when we add classes during annotation.","title":"Recipe-Specific Ingredient"},{"location":"pages/tutorial/t1-active-learning/#apply-labels","text":"Now we invoke the active_learning recipe. Tips: how recipes work programmatically In general, a recipe is a function taking a SupervisableDataset and other arguments based on its functionality. Here are a few common recipes: active_learning simple_annotator linked_annotator Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vecnet VectorNet model to use in the loop **kwargs forwarded to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search and filter Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vecnet , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vecnet` | `VectorNet` | model to use in the loop | | `**kwargs` | | forwarded to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search and filter | \"\"\" layout , _ = _active_learning ( dataset , vecnet , ** kwargs ) return layout Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout The recipe returns a handle function which bokeh can use to visualize an annotation interface in multiple settings. from hover.recipes.experimental import active_learning interactive_plot = active_learning(dataset, vecnet) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') Tips: annotation interface with multiple plots Video guide: leveraging linked selection Video guide: active learning Text guide: active learning Inspecting model predictions allows us to get an idea of how the current set of annotations will likely teach the model. locate the most valuable samples for further annotation. @import url(\"../../../styles/monokai.css\");","title":"Apply Labels"},{"location":"pages/tutorial/t2-bokeh-app/","text":"hover creates a bokeh server app to deliver its annotation interface. This app can be served flexibly based on your needs. @import url(\"../../../styles/monokai.css\"); Prerequisites Suppose that we've already used a recipe to create a handle function like in the quickstart . Recap from the tutorials before the handle is a function which renders plot elements on a bokeh document . Option 1: Jupyter We are probably familiar with this now: from bokeh.io import show , output_notebook output_notebook () show ( handle ) # notebook_url='http://localhost:8888' Pros & Cons This inline Jupyter mode can integrate particularly well with your notebook workflow. For example, when your are (tentatively) done with annotation, the SupervisableDataset can be accessed directly in the notebook, rather than exported to a file and loaded back. The inline mode is highly recommended for local usage. On the contrary, with a remote Jupyter server, it may have trouble displaying the plots. this can be due to failure of loading JavaScript libraries or accessing implicit bokeh server ports. Option 2: Command Line bokeh serve starts an explicit tornado server from the command line: bokeh serve my-app.py # my-app.py # handle = ... from bokeh.io import curdoc doc = curdoc () handle ( doc ) Pros & Cons This is the \"classic\" approach to run a bokeh server. Remote access is simple through parameters specified here . The bokeh plot tools are mobile-friendly too -- this means you can host a server, e.g. an http-enabled cloud virtual machine, and annotate from a tablet. The command line mode is less interactive, since Python objects in the script cannot be accessed on the fly. Option 3: Anywhere in Python It is possible to embed the app in regular Python: from bokeh.server.server import Server server = Server ({ '/my-app' : handle }) server . start () Pros & Cons This embedded mode is a go-to for serving within a greater application. Also note that each command line argument for bokeh serve has a corresponding keyword argument to Server() . For instance, bokeh serve <args> --allow-websocket-origin=* in the command line mirrors Server(*args, allow_websocket_origin='*') in Python. The embedded mode gives you the most control of your server.","title":"Server Options"},{"location":"pages/tutorial/t2-bokeh-app/#prerequisites","text":"Suppose that we've already used a recipe to create a handle function like in the quickstart . Recap from the tutorials before the handle is a function which renders plot elements on a bokeh document .","title":"Prerequisites"},{"location":"pages/tutorial/t2-bokeh-app/#option-1-jupyter","text":"We are probably familiar with this now: from bokeh.io import show , output_notebook output_notebook () show ( handle ) # notebook_url='http://localhost:8888' Pros & Cons This inline Jupyter mode can integrate particularly well with your notebook workflow. For example, when your are (tentatively) done with annotation, the SupervisableDataset can be accessed directly in the notebook, rather than exported to a file and loaded back. The inline mode is highly recommended for local usage. On the contrary, with a remote Jupyter server, it may have trouble displaying the plots. this can be due to failure of loading JavaScript libraries or accessing implicit bokeh server ports.","title":"Option 1: Jupyter"},{"location":"pages/tutorial/t2-bokeh-app/#option-2-command-line","text":"bokeh serve starts an explicit tornado server from the command line: bokeh serve my-app.py # my-app.py # handle = ... from bokeh.io import curdoc doc = curdoc () handle ( doc ) Pros & Cons This is the \"classic\" approach to run a bokeh server. Remote access is simple through parameters specified here . The bokeh plot tools are mobile-friendly too -- this means you can host a server, e.g. an http-enabled cloud virtual machine, and annotate from a tablet. The command line mode is less interactive, since Python objects in the script cannot be accessed on the fly.","title":"Option 2: Command Line"},{"location":"pages/tutorial/t2-bokeh-app/#option-3-anywhere-in-python","text":"It is possible to embed the app in regular Python: from bokeh.server.server import Server server = Server ({ '/my-app' : handle }) server . start () Pros & Cons This embedded mode is a go-to for serving within a greater application. Also note that each command line argument for bokeh serve has a corresponding keyword argument to Server() . For instance, bokeh serve <args> --allow-websocket-origin=* in the command line mirrors Server(*args, allow_websocket_origin='*') in Python. The embedded mode gives you the most control of your server.","title":"Option 3: Anywhere in Python"},{"location":"pages/tutorial/t3-dataset-population-selection/","text":"SupervisableDataset holds your data throughout the labeling process. Let's take a look at its core mechanisms. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: This page addresses single components of hover We are using code snippets to pick out parts of the annotation interface, so that the documentation can explain what they do. Please be aware that this is NOT how one would typically use hover . Typical usage deals with recipes where the individual parts have been tied together. Data Subsets We place unlabeled data and labeled data in different subsets: \"raw\", \"train\", \"dev\", and \"test\". Unlabeled data start from the \"raw\" subset, and can be transferred to other subsets after it gets labeled. SupervisableDataset uses a \"population table\", dataset.pop_table , to show the size of each subset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' show(dataset.pop_table, notebook_url=notebook_url) Transfer Data Between Subsets COMMIT and DEDUP are the mechanisms that hover uses to transfer data between subsets. COMMIT copies selected points (to be discussed later) to a destination subset labeled-raw-only: COMMIT automatically detects which points are in the raw set with a valid label. Other points will not get copied. keep-last: you can commit the same point to the same subset multiple times and the last copy will be kept. This can be useful for revising labels before DEDUP . DEDUP removes duplicates (identified by feature value) across subsets priority rule: test > dev > train > raw, i.e. test set data always gets kept during deduplication FAQ Why does COMMIT only work on the raw subset? Most selections will happen through plots, where different subsets are on top of each other. This means selections can contain both unlabeled and labeled points. Way too often we find ourselves trying to view both the labeled and the unlabeled, but only moving the unlabeled \"raw\" points. So it's handy that COMMIT picks those points only. These mechanisms correspond to buttons in hover 's annotation interface, which you have encountered in the quickstart: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from bokeh.layouts import row, column show(column( row( dataset.data_committer, dataset.dedup_trigger, ), dataset.pop_table, ), notebook_url=notebook_url) Of course, so far we have nothing to move, because there's no data selected. We shall now discuss selections. Selection hover labels data points in bulk, which requires selecting groups of homogeneous data, i.e. semantically similar or going to have the same label. Being able to skim through what you selected gives you confidence about homogeneity. Normally, selection happens through a plot ( explorer ), as we have seen in the quickstart. For the purpose here, we will \"cheat\" and assign the selection programmatically: dataset._callback_update_selection(dataset.dfs[\"raw\"].loc[:10]) show(dataset.sel_table, notebook_url=notebook_url) Edit Data Within a Selection Often the points selected are not perfectly homogeneous, i.e. some outliers belong to a different label from the selected group overall. It would be helpful to EVICT them, and SupervisableDataset has a button for it. Sometimes you may also wish to edit data values on the fly. In hover this is called PATCH , and there also is a button for it. by default, labels can be edited but feature values cannot. Let's plot the forementioned buttons along with the selection table. Toggle any number of rows in the table, then click the button to EVICT or PATCH those rows: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(column( row( dataset.selection_evictor, dataset.selection_patcher, ), dataset.sel_table, ), notebook_url=notebook_url) @import url(\"../../../styles/monokai.css\");","title":"Dataset Mechanisms"},{"location":"pages/tutorial/t3-dataset-population-selection/#data-subsets","text":"We place unlabeled data and labeled data in different subsets: \"raw\", \"train\", \"dev\", and \"test\". Unlabeled data start from the \"raw\" subset, and can be transferred to other subsets after it gets labeled. SupervisableDataset uses a \"population table\", dataset.pop_table , to show the size of each subset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' show(dataset.pop_table, notebook_url=notebook_url)","title":"Data Subsets"},{"location":"pages/tutorial/t3-dataset-population-selection/#transfer-data-between-subsets","text":"COMMIT and DEDUP are the mechanisms that hover uses to transfer data between subsets. COMMIT copies selected points (to be discussed later) to a destination subset labeled-raw-only: COMMIT automatically detects which points are in the raw set with a valid label. Other points will not get copied. keep-last: you can commit the same point to the same subset multiple times and the last copy will be kept. This can be useful for revising labels before DEDUP . DEDUP removes duplicates (identified by feature value) across subsets priority rule: test > dev > train > raw, i.e. test set data always gets kept during deduplication FAQ Why does COMMIT only work on the raw subset? Most selections will happen through plots, where different subsets are on top of each other. This means selections can contain both unlabeled and labeled points. Way too often we find ourselves trying to view both the labeled and the unlabeled, but only moving the unlabeled \"raw\" points. So it's handy that COMMIT picks those points only. These mechanisms correspond to buttons in hover 's annotation interface, which you have encountered in the quickstart: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from bokeh.layouts import row, column show(column( row( dataset.data_committer, dataset.dedup_trigger, ), dataset.pop_table, ), notebook_url=notebook_url) Of course, so far we have nothing to move, because there's no data selected. We shall now discuss selections.","title":"Transfer Data Between Subsets"},{"location":"pages/tutorial/t3-dataset-population-selection/#selection","text":"hover labels data points in bulk, which requires selecting groups of homogeneous data, i.e. semantically similar or going to have the same label. Being able to skim through what you selected gives you confidence about homogeneity. Normally, selection happens through a plot ( explorer ), as we have seen in the quickstart. For the purpose here, we will \"cheat\" and assign the selection programmatically: dataset._callback_update_selection(dataset.dfs[\"raw\"].loc[:10]) show(dataset.sel_table, notebook_url=notebook_url)","title":"Selection"},{"location":"pages/tutorial/t3-dataset-population-selection/#edit-data-within-a-selection","text":"Often the points selected are not perfectly homogeneous, i.e. some outliers belong to a different label from the selected group overall. It would be helpful to EVICT them, and SupervisableDataset has a button for it. Sometimes you may also wish to edit data values on the fly. In hover this is called PATCH , and there also is a button for it. by default, labels can be edited but feature values cannot. Let's plot the forementioned buttons along with the selection table. Toggle any number of rows in the table, then click the button to EVICT or PATCH those rows: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(column( row( dataset.selection_evictor, dataset.selection_patcher, ), dataset.sel_table, ), notebook_url=notebook_url) @import url(\"../../../styles/monokai.css\");","title":"Edit Data Within a Selection"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/","text":"Annotator is an explorer which provides a map of your data colored by labels. Let's walk through its components and how they interact with the dataset . You will find many of these components again in other explorer s. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: This page addresses single components of hover We are using code snippets to pick out parts of the annotation interface, so that the documentation can explain what they do. Please be aware that this is NOT how one would typically use hover . Typical usage deals with recipes where the individual parts have been tied together. Preparation As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) Scatter Plot: Semantically Similar Points are Close Together hover labels data points in bulk, which requires selecting groups of homogeneous data. The core of the annotator is a scatter plot and labeling widgets: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_annotator from bokeh.layouts import row, column annotator = standard_annotator(dataset) show(column( row(annotator.annotator_input, annotator.annotator_apply), annotator.figure, ), notebook_url=notebook_url) Select Points on the Plot On the right of the scatter plot, you can find tap, polygon, and lasso tools which can select data points. View Tooltips with Mouse Hover Embeddings are helpful but rarely perfect. This is why we have tooltips that show the detail of each point on mouse hover, allowing us to inspect points, discover patterns, and come up with new labels on the fly. Show & Hide Subsets Showing labeled subsets can tell you which parts of the data has been explored and which ones have not. With toggle buttons, you can turn on/off the display for any subset. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(annotator.data_key_button_group, notebook_url=notebook_url) Make Consecutive Selections Ever selected multiple (non-adjacent) files in your file system using Ctrl / Command ? Similarly but more powerfully, you can make consecutive selections with a \"keep selecting\" option. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(annotator.selection_option_box, notebook_url=notebook_url) Selection option values: what do they do? Basic set operations on your old & new selection. Quick intro here none : the default, where a new selection B simply replaces the old one A . union : A \u222a B , the new selection gets unioned with the old one. this resembles the Ctrl / Command mentioned above. intersection : A \u2229 B , the new selection gets intersected with the old one. this is particularly useful when going beyond simple 2D plots. difference : A \u2216 B , the new selection gets subtracted from the old one. this is for de-selecting outliers. Change Plot Axes hover supports dynamically choosing which embedding dimensions to use for your 2D plot. This becomes nontrivial, and sometimes very useful, when we have a 3D embedding (or higher): Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=3) annotator = standard_annotator(dataset) show(column( row(annotator.dropdown_x_axis, annotator.dropdown_y_axis), annotator.figure, ), notebook_url=notebook_url) Text Search Widget: Include/Exclude Keywords or regular expressions can be great starting points for identifying a cluster of similar points based on domain expertise. You may specify a positive regular expression to look for and/or a negative one to not look for. The annotator will amplify the sizes of positive-match data points and shrink those of negative matches. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(row(annotator.search_pos, annotator.search_neg), notebook_url=notebook_url) Preview: Use Search for Selection in Finder In a particular kind of plots called finder (see later in the tutorials), the search widget can directly operate on your selection as a filter. The Plot and The Dataset When we apply labels through the annotator plot, it's acutally the dataset behind the plot that gets immediately updated. The plot itself is not in direct sync with the dataset, which is a design choice for performance. Instead, we will use a trigger called PUSH for updating the data entries to the plot. PUSH: Synchronize from Dataset to Plots Below is the full interface of the dataset , where you can find a green \"Push\" button: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(dataset.view(), notebook_url=notebook_url) In a built-in recipe , the \"Push\" button will update the latest data to every explorer linked to the dataset . @import url(\"../../../styles/monokai.css\");","title":"Annotator & Plot Tools"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#preparation","text":"As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)","title":"Preparation"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#scatter-plot-semantically-similar-points-are-close-together","text":"hover labels data points in bulk, which requires selecting groups of homogeneous data. The core of the annotator is a scatter plot and labeling widgets: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_annotator from bokeh.layouts import row, column annotator = standard_annotator(dataset) show(column( row(annotator.annotator_input, annotator.annotator_apply), annotator.figure, ), notebook_url=notebook_url)","title":"Scatter Plot: Semantically Similar Points are Close Together"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#select-points-on-the-plot","text":"On the right of the scatter plot, you can find tap, polygon, and lasso tools which can select data points.","title":"Select Points on the Plot"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#view-tooltips-with-mouse-hover","text":"Embeddings are helpful but rarely perfect. This is why we have tooltips that show the detail of each point on mouse hover, allowing us to inspect points, discover patterns, and come up with new labels on the fly.","title":"View Tooltips with Mouse Hover"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#show-hide-subsets","text":"Showing labeled subsets can tell you which parts of the data has been explored and which ones have not. With toggle buttons, you can turn on/off the display for any subset. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(annotator.data_key_button_group, notebook_url=notebook_url)","title":"Show &amp; Hide Subsets"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#make-consecutive-selections","text":"Ever selected multiple (non-adjacent) files in your file system using Ctrl / Command ? Similarly but more powerfully, you can make consecutive selections with a \"keep selecting\" option. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(annotator.selection_option_box, notebook_url=notebook_url) Selection option values: what do they do? Basic set operations on your old & new selection. Quick intro here none : the default, where a new selection B simply replaces the old one A . union : A \u222a B , the new selection gets unioned with the old one. this resembles the Ctrl / Command mentioned above. intersection : A \u2229 B , the new selection gets intersected with the old one. this is particularly useful when going beyond simple 2D plots. difference : A \u2216 B , the new selection gets subtracted from the old one. this is for de-selecting outliers.","title":"Make Consecutive Selections"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#change-plot-axes","text":"hover supports dynamically choosing which embedding dimensions to use for your 2D plot. This becomes nontrivial, and sometimes very useful, when we have a 3D embedding (or higher): Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=3) annotator = standard_annotator(dataset) show(column( row(annotator.dropdown_x_axis, annotator.dropdown_y_axis), annotator.figure, ), notebook_url=notebook_url)","title":"Change Plot Axes"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#text-search-widget-includeexclude","text":"Keywords or regular expressions can be great starting points for identifying a cluster of similar points based on domain expertise. You may specify a positive regular expression to look for and/or a negative one to not look for. The annotator will amplify the sizes of positive-match data points and shrink those of negative matches. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(row(annotator.search_pos, annotator.search_neg), notebook_url=notebook_url)","title":"Text Search Widget: Include/Exclude"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#preview-use-search-for-selection-in-finder","text":"In a particular kind of plots called finder (see later in the tutorials), the search widget can directly operate on your selection as a filter.","title":"Preview: Use Search for Selection in Finder"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#the-plot-and-the-dataset","text":"When we apply labels through the annotator plot, it's acutally the dataset behind the plot that gets immediately updated. The plot itself is not in direct sync with the dataset, which is a design choice for performance. Instead, we will use a trigger called PUSH for updating the data entries to the plot.","title":"The Plot and The Dataset"},{"location":"pages/tutorial/t4-annotator-dataset-interaction/#push-synchronize-from-dataset-to-plots","text":"Below is the full interface of the dataset , where you can find a green \"Push\" button: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(dataset.view(), notebook_url=notebook_url) In a built-in recipe , the \"Push\" button will update the latest data to every explorer linked to the dataset . @import url(\"../../../styles/monokai.css\");","title":"PUSH: Synchronize from Dataset to Plots"},{"location":"pages/tutorial/t5-finder-filter/","text":"Finder is an explorer focused on search . It can help you select points using a filter based on search results. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: This page addresses single components of hover We are using code snippets to pick out parts of the annotation interface, so that the documentation can explain what they do. Please be aware that this is NOT how one would typically use hover . Typical usage deals with recipes where the individual parts have been tied together. More Angles -> Better Results Explorer s other than annotator are specialized in finding additional insight to help us understand the data. Having them juxtaposed with annotator , we can label more accurately, more confidently, and even faster. Preparation As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) Filter Toggles When we use lasso or polygon select, we are describing a shape. Sometimes that shape is not accurate enough -- we need extra conditions to narrow down the data. Just like annotator , finder has search widgets. But unlike annotator , finder has a filter toggle which can directly intersect what we selected with what meets the search criteria . Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_finder from bokeh.layouts import row, column finder = standard_finder(dataset) show(row( column(finder.search_pos, finder.search_neg), finder.search_filter_box, ), notebook_url=notebook_url) Next to the search widgets is a checkbox. The filter will stay active as long as the checkbox is. How the filter interacts with selection options Selection options apply before filters. hover memorizes your pre-filter selections, so you can keep selecting without having to tweaking the filter toggle. Example: suppose you have previously selected a set of points called A . then you toggled a filter f , giving you A\u2229F where F is the set satisfying f . now, with selection option \"union\", you select a set of points called B . your current selection will be (A \u222a B) \u2229 F , i.e. (A \u2229 F) \u222a (B \u2229 F) . similarly, you would get (A \u2229 B) \u2229 F for \"intersection\" and (A \u2216 B) \u2229 F for \"difference\". if you untoggle the filter now, you selection would be A \u222a B . In the later tutorials, we shall see multiple filters in action together. spoiler: F = F1 \u2229 F2 \u2229 ... and that's it! Stronger Highlight for Search finder also colors data points based on search criteria, making them easier to find. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(column( row(finder.search_pos, finder.search_neg), finder.figure, ), notebook_url=notebook_url) @import url(\"../../../styles/monokai.css\");","title":"Finder & Selection Filter"},{"location":"pages/tutorial/t5-finder-filter/#more-angles-better-results","text":"Explorer s other than annotator are specialized in finding additional insight to help us understand the data. Having them juxtaposed with annotator , we can label more accurately, more confidently, and even faster.","title":"More Angles -&gt; Better Results"},{"location":"pages/tutorial/t5-finder-filter/#preparation","text":"As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)","title":"Preparation"},{"location":"pages/tutorial/t5-finder-filter/#filter-toggles","text":"When we use lasso or polygon select, we are describing a shape. Sometimes that shape is not accurate enough -- we need extra conditions to narrow down the data. Just like annotator , finder has search widgets. But unlike annotator , finder has a filter toggle which can directly intersect what we selected with what meets the search criteria . Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_finder from bokeh.layouts import row, column finder = standard_finder(dataset) show(row( column(finder.search_pos, finder.search_neg), finder.search_filter_box, ), notebook_url=notebook_url) Next to the search widgets is a checkbox. The filter will stay active as long as the checkbox is. How the filter interacts with selection options Selection options apply before filters. hover memorizes your pre-filter selections, so you can keep selecting without having to tweaking the filter toggle. Example: suppose you have previously selected a set of points called A . then you toggled a filter f , giving you A\u2229F where F is the set satisfying f . now, with selection option \"union\", you select a set of points called B . your current selection will be (A \u222a B) \u2229 F , i.e. (A \u2229 F) \u222a (B \u2229 F) . similarly, you would get (A \u2229 B) \u2229 F for \"intersection\" and (A \u2216 B) \u2229 F for \"difference\". if you untoggle the filter now, you selection would be A \u222a B . In the later tutorials, we shall see multiple filters in action together. spoiler: F = F1 \u2229 F2 \u2229 ... and that's it!","title":"Filter Toggles"},{"location":"pages/tutorial/t5-finder-filter/#stronger-highlight-for-search","text":"finder also colors data points based on search criteria, making them easier to find. Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(column( row(finder.search_pos, finder.search_neg), finder.figure, ), notebook_url=notebook_url) @import url(\"../../../styles/monokai.css\");","title":"Stronger Highlight for Search"},{"location":"pages/tutorial/t6-softlabel-joint-filter/","text":"hover filters can stack together. This makes selections incredibly powerful. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: This page addresses single components of hover We are using code snippets to pick out parts of the annotation interface, so that the documentation can explain what they do. Please be aware that this is NOT how one would typically use hover . Typical usage deals with recipes where the individual parts have been tied together. Preparation As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) Soft-Label Explorer Active learning works by predicting labels and scores (i.e. soft labels) and utilizing that prediction. An intuitive way to plot soft labels is to color-code labels and use opacity (\"alpha\" by bokeh terminology) to represent scores. SoftLabelExplorer delivers this functionality: from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_softlabel from bokeh.layouts import row, column softlabel = standard_softlabel(dataset) show(softlabel.figure, notebook_url=notebook_url) Filter Selection by Score Range Similarly to finder , a softlabel plot has its own selection filter. The difference lies in the filter condition: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(softlabel.score_filter, notebook_url=notebook_url) Linked Selections & Joint Filters When we plot multiple explorer s for the same dataset , it makes sense to synchronize selections between those plots. hover recipes take care of this synchronization. This also works with cumulative selections. Consequently, the cumulative toggle is synchronized too. Since each filter is narrowing down the selections we make, joint filters is just set intersection, extended from two sets (original selection + filter) to N sets (original selection + filter A + filter B + ...) The active_learning recipe is built of softlabel + annotator + finder , plus a few widgets for iterating the model-in-loop. In the next tutorial(s), we will see more recipes taking advantage of linked selections and joint filters. Powerful indeed! @import url(\"../../../styles/monokai.css\");","title":"Soft Label & Joint Filters"},{"location":"pages/tutorial/t6-softlabel-joint-filter/#preparation","text":"As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)","title":"Preparation"},{"location":"pages/tutorial/t6-softlabel-joint-filter/#soft-label-explorer","text":"Active learning works by predicting labels and scores (i.e. soft labels) and utilizing that prediction. An intuitive way to plot soft labels is to color-code labels and use opacity (\"alpha\" by bokeh terminology) to represent scores. SoftLabelExplorer delivers this functionality: from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_softlabel from bokeh.layouts import row, column softlabel = standard_softlabel(dataset) show(softlabel.figure, notebook_url=notebook_url)","title":"Soft-Label Explorer"},{"location":"pages/tutorial/t6-softlabel-joint-filter/#filter-selection-by-score-range","text":"Similarly to finder , a softlabel plot has its own selection filter. The difference lies in the filter condition: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(softlabel.score_filter, notebook_url=notebook_url)","title":"Filter Selection by Score Range"},{"location":"pages/tutorial/t6-softlabel-joint-filter/#linked-selections-joint-filters","text":"When we plot multiple explorer s for the same dataset , it makes sense to synchronize selections between those plots. hover recipes take care of this synchronization. This also works with cumulative selections. Consequently, the cumulative toggle is synchronized too. Since each filter is narrowing down the selections we make, joint filters is just set intersection, extended from two sets (original selection + filter) to N sets (original selection + filter A + filter B + ...) The active_learning recipe is built of softlabel + annotator + finder , plus a few widgets for iterating the model-in-loop. In the next tutorial(s), we will see more recipes taking advantage of linked selections and joint filters. Powerful indeed! @import url(\"../../../styles/monokai.css\");","title":"Linked Selections &amp; Joint Filters"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/","text":"Suppose we have some custom functions for labeling or filtering data, which resembles snorkel 's typical scenario. Let's see how these functions can be combined with hover . { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", ref: \"master\", <!-- ref: \"dev\", --> codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: This page addresses single components of hover We are using code snippets to pick out parts of the annotation interface, so that the documentation can explain what they do. Please be aware that this is NOT how one would typically use hover . Typical usage deals with recipes where the individual parts have been tied together. Preparation As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2) Labeling Functions Labeling functions are functions that take a pd.DataFrame row and return a label or abstain . Inside the function one can do many things, but let's start with simple keywords wrapped in regex: About the decorator @labeling_function hover . utils . snorkel_helper . labeling_function ( targets , label_encoder = None , ** kwargs ) Hover's flavor of the Snorkel labeling_function decorator. However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. assigns a UUID for easy identification keeps track of LF targets Param Type Description targets list of str labels that the labeling function is intended to create label_encoder dict {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a .snorkel attribute **kwargs forwarded to snorkel 's labeling_function() Source code in hover/utils/snorkel_helper.py def labeling_function ( targets , label_encoder = None , ** kwargs ): \"\"\" ???+ note \"Hover's flavor of the Snorkel labeling_function decorator.\" However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. - assigns a UUID for easy identification - keeps track of LF targets | Param | Type | Description | | :-------------- | :----- | :----------------------------------- | | `targets` | `list` of `str` | labels that the labeling function is intended to create | | `label_encoder` | `dict` | {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a `.snorkel` attribute | | `**kwargs` | | forwarded to `snorkel`'s `labeling_function()` | \"\"\" # lazy import so that the package does not require snorkel # Feb 3, 2022: snorkel's dependency handling is too strict # for other dependencies like NumPy, SciPy, SpaCy, etc. # Let's cite Snorkel and lazy import or copy functions. # DO NOT explicitly depend on Snorkel without confirming # that all builds/tests pass by Anaconda standards, else # we risk having to drop conda support. from snorkel.labeling import ( labeling_function as snorkel_lf , LabelingFunction as SnorkelLF , ) def wrapper ( func ): # set up kwargs for Snorkel's LF # a default name that can be overridden snorkel_kwargs = { \"name\" : func . __name__ } snorkel_kwargs . update ( kwargs ) # return value of hover's decorator lf = SnorkelLF ( f = func , ** snorkel_kwargs ) # additional attributes lf . uuid = uuid . uuid1 () lf . targets = targets [:] # link a snorkel-style labeling function if applicable if label_encoder : lf . label_encoder = label_encoder def snorkel_style_func ( x ): return lf . label_encoder [ func ( x )] lf . snorkel = snorkel_lf ( ** kwargs )( snorkel_style_func ) else : lf . label_encoder = None lf . snorkel = None return lf return wrapper from hover.utils.snorkel_helper import labeling_function from hover.module_config import ABSTAIN_DECODED as ABSTAIN import re @labeling_function(targets=[\"rec.autos\"]) def auto_keywords(row): flag = re.search( r\"(?i)(diesel|gasoline|automobile|vehicle|drive|driving)\", row.text ) return \"rec.autos\" if flag else ABSTAIN @labeling_function(targets=[\"rec.sport.baseball\"]) def baseball_keywords(row): flag = re.search(r\"(?i)(baseball|stadium|\\ bat\\ |\\ base\\ )\", row.text) return \"rec.sport.baseball\" if flag else ABSTAIN @labeling_function(targets=[\"sci.crypt\"]) def crypt_keywords(row): flag = re.search(r\"(?i)(crypt|math|encode|decode|key)\", row.text) return \"sci.crypt\" if flag else ABSTAIN @labeling_function(targets=[\"talk.politics.guns\"]) def guns_keywords(row): flag = re.search(r\"(?i)(gun|rifle|ammunition|violence|shoot)\", row.text) return \"talk.politics.guns\" if flag else ABSTAIN @labeling_function(targets=[\"misc.forsale\"]) def forsale_keywords(row): flag = re.search(r\"(?i)(sale|deal|price|discount)\", row.text) return \"misc.forsale\" if flag else ABSTAIN LABELING_FUNCTIONS = [ auto_keywords, baseball_keywords, crypt_keywords, guns_keywords, forsale_keywords, ] # we will come back to this block later on # LABELING_FUNCTIONS.pop(-1) Using a Function to Apply Labels Hover's SnorkelExplorer (short as snorkel ) can take the labeling functions above and apply them on areas of data that you choose. The widget below is responsible for labeling: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_snorkel snorkel_plot = standard_snorkel(dataset) snorkel_plot.subscribed_lf_list = LABELING_FUNCTIONS show(snorkel_plot.lf_apply_trigger, notebook_url=notebook_url) Using a Function to Apply Filters Any function that labels is also a function that filters. The filter condition is \"keep if did not abstain\" . The widget below handles filtering: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(snorkel_plot.lf_filter_trigger, notebook_url=notebook_url) Unlike the toggled filters for finder and softlabel , filtering with functions is on a per-click basis. In other words, this particular filtration doesn't persist when you select another area. Dynamic List of Functions Python lists are mutable, and we are going to take advantage of that for improvising and editing labeling functions on the fly. Run the block below and open the resulting URL to launch a recipe. labeling functions are evaluated against the dev set. hence you are advised to send the labels produced by these functions to the train set, not the dev set. come back and edit the list of labeling functions in-place in one of the code cells above. then go to the launched app and refresh the functions! from hover.recipes.experimental import snorkel_crosscheck interactive_plot = snorkel_crosscheck(dataset, LABELING_FUNCTIONS) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') What's really cool is that in your local environment, this update-and-refresh operation can be done all in a notebook. So now you can interactively evaluate and revise labeling functions visually assign specific data regions to apply those functions which makes labeling functions significantly more accurate and applicable. @import url(\"../../../styles/monokai.css\");","title":"Custom Labeling Functions"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#preparation","text":"As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)","title":"Preparation"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#labeling-functions","text":"Labeling functions are functions that take a pd.DataFrame row and return a label or abstain . Inside the function one can do many things, but let's start with simple keywords wrapped in regex: About the decorator @labeling_function","title":"Labeling Functions"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#hover.utils.snorkel_helper.labeling_function","text":"Hover's flavor of the Snorkel labeling_function decorator. However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. assigns a UUID for easy identification keeps track of LF targets Param Type Description targets list of str labels that the labeling function is intended to create label_encoder dict {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a .snorkel attribute **kwargs forwarded to snorkel 's labeling_function() Source code in hover/utils/snorkel_helper.py def labeling_function ( targets , label_encoder = None , ** kwargs ): \"\"\" ???+ note \"Hover's flavor of the Snorkel labeling_function decorator.\" However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. - assigns a UUID for easy identification - keeps track of LF targets | Param | Type | Description | | :-------------- | :----- | :----------------------------------- | | `targets` | `list` of `str` | labels that the labeling function is intended to create | | `label_encoder` | `dict` | {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a `.snorkel` attribute | | `**kwargs` | | forwarded to `snorkel`'s `labeling_function()` | \"\"\" # lazy import so that the package does not require snorkel # Feb 3, 2022: snorkel's dependency handling is too strict # for other dependencies like NumPy, SciPy, SpaCy, etc. # Let's cite Snorkel and lazy import or copy functions. # DO NOT explicitly depend on Snorkel without confirming # that all builds/tests pass by Anaconda standards, else # we risk having to drop conda support. from snorkel.labeling import ( labeling_function as snorkel_lf , LabelingFunction as SnorkelLF , ) def wrapper ( func ): # set up kwargs for Snorkel's LF # a default name that can be overridden snorkel_kwargs = { \"name\" : func . __name__ } snorkel_kwargs . update ( kwargs ) # return value of hover's decorator lf = SnorkelLF ( f = func , ** snorkel_kwargs ) # additional attributes lf . uuid = uuid . uuid1 () lf . targets = targets [:] # link a snorkel-style labeling function if applicable if label_encoder : lf . label_encoder = label_encoder def snorkel_style_func ( x ): return lf . label_encoder [ func ( x )] lf . snorkel = snorkel_lf ( ** kwargs )( snorkel_style_func ) else : lf . label_encoder = None lf . snorkel = None return lf return wrapper","title":"labeling_function()"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#hover.utils.snorkel_helper.labeling_function","text":"from hover.utils.snorkel_helper import labeling_function from hover.module_config import ABSTAIN_DECODED as ABSTAIN import re @labeling_function(targets=[\"rec.autos\"]) def auto_keywords(row): flag = re.search( r\"(?i)(diesel|gasoline|automobile|vehicle|drive|driving)\", row.text ) return \"rec.autos\" if flag else ABSTAIN @labeling_function(targets=[\"rec.sport.baseball\"]) def baseball_keywords(row): flag = re.search(r\"(?i)(baseball|stadium|\\ bat\\ |\\ base\\ )\", row.text) return \"rec.sport.baseball\" if flag else ABSTAIN @labeling_function(targets=[\"sci.crypt\"]) def crypt_keywords(row): flag = re.search(r\"(?i)(crypt|math|encode|decode|key)\", row.text) return \"sci.crypt\" if flag else ABSTAIN @labeling_function(targets=[\"talk.politics.guns\"]) def guns_keywords(row): flag = re.search(r\"(?i)(gun|rifle|ammunition|violence|shoot)\", row.text) return \"talk.politics.guns\" if flag else ABSTAIN @labeling_function(targets=[\"misc.forsale\"]) def forsale_keywords(row): flag = re.search(r\"(?i)(sale|deal|price|discount)\", row.text) return \"misc.forsale\" if flag else ABSTAIN LABELING_FUNCTIONS = [ auto_keywords, baseball_keywords, crypt_keywords, guns_keywords, forsale_keywords, ] # we will come back to this block later on # LABELING_FUNCTIONS.pop(-1)","title":"labeling_function()"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#using-a-function-to-apply-labels","text":"Hover's SnorkelExplorer (short as snorkel ) can take the labeling functions above and apply them on areas of data that you choose. The widget below is responsible for labeling: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. from local_lib.binder_helper import remote_jupyter_proxy_url from bokeh.io import show, output_notebook output_notebook() # special configuration for this remotely hosted tutorial notebook_url = remote_jupyter_proxy_url # normally your would skip notebook_url or use Jupyter address # notebook_url = 'localhost:8888' from hover.recipes.subroutine import standard_snorkel snorkel_plot = standard_snorkel(dataset) snorkel_plot.subscribed_lf_list = LABELING_FUNCTIONS show(snorkel_plot.lf_apply_trigger, notebook_url=notebook_url)","title":"Using a Function to Apply Labels"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#using-a-function-to-apply-filters","text":"Any function that labels is also a function that filters. The filter condition is \"keep if did not abstain\" . The widget below handles filtering: Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application. show(snorkel_plot.lf_filter_trigger, notebook_url=notebook_url) Unlike the toggled filters for finder and softlabel , filtering with functions is on a per-click basis. In other words, this particular filtration doesn't persist when you select another area.","title":"Using a Function to Apply Filters"},{"location":"pages/tutorial/t7-snorkel-improvise-rules/#dynamic-list-of-functions","text":"Python lists are mutable, and we are going to take advantage of that for improvising and editing labeling functions on the fly. Run the block below and open the resulting URL to launch a recipe. labeling functions are evaluated against the dev set. hence you are advised to send the labels produced by these functions to the train set, not the dev set. come back and edit the list of labeling functions in-place in one of the code cells above. then go to the launched app and refresh the functions! from hover.recipes.experimental import snorkel_crosscheck interactive_plot = snorkel_crosscheck(dataset, LABELING_FUNCTIONS) # ---------- SERVER MODE: for this documentation page ---------- # because this tutorial is remotely hosted, we need explicit serving to expose the plot to you from local_lib.binder_helper import binder_proxy_app_url from bokeh.server.server import Server server = Server({'/my-app': interactive_plot}, port=5007, allow_websocket_origin=['*'], use_xheaders=True) server.start() # visit this URL printed in cell output to see the interactive plot; locally you would just do \"https://localhost:5007/my-app\" binder_proxy_app_url('my-app', port=5007) # ---------- NOTEBOOK MODE: for your actual Jupyter environment --------- # if you'd like, this code will render the entire plot in Jupyter # from bokeh.io import show, output_notebook # output_notebook() # show(interactive_plot, notebook_url='https://localhost:8888') What's really cool is that in your local environment, this update-and-refresh operation can be done all in a notebook. So now you can interactively evaluate and revise labeling functions visually assign specific data regions to apply those functions which makes labeling functions significantly more accurate and applicable. @import url(\"../../../styles/monokai.css\");","title":"Dynamic List of Functions"},{"location":"snippets/markdown/binder-kernel/","text":"Running Python right here Think of this page as almost a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel:","title":"Binder kernel"},{"location":"snippets/markdown/component-tutorial/","text":"This page addresses single components of hover We are using code snippets to pick out parts of the annotation interface, so that the documentation can explain what they do. Please be aware that this is NOT how one would typically use hover . Typical usage deals with recipes where the individual parts have been tied together.","title":"Component tutorial"},{"location":"snippets/markdown/dataset-prep/","text":"As always, start with a ready-for-plot dataset: from hover.core.dataset import SupervisableTextDataset import pandas as pd raw_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" train_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_train.csv\" # for fast, low-memory demonstration purpose, sample the data df_raw = pd.read_csv(raw_csv_path).sample(400) df_raw[\"SUBSET\"] = \"raw\" df_train = pd.read_csv(train_csv_path).sample(400) df_train[\"SUBSET\"] = \"train\" df_dev = pd.read_csv(train_csv_path).sample(100) df_dev[\"SUBSET\"] = \"dev\" df_test = pd.read_csv(train_csv_path).sample(100) df_test[\"SUBSET\"] = \"test\" # build overall dataframe and ensure feature type df = pd.concat([df_raw, df_train, df_dev, df_test]) df[\"text\"] = df[\"text\"].astype(str) # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df, feature_key=\"text\", label_key=\"label\") import spacy import re from functools import lru_cache # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array @lru_cache(maxsize=int(1e+4)) def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html reducer = dataset.compute_nd_embedding(vectorizer, \"umap\", dimension=2)","title":"Dataset prep"},{"location":"snippets/markdown/jupyterlab-js-issue/","text":"Showcase widgets here are not interactive Plotted widgets on this page are not interactive, but only for illustration. Widgets will be interactive when you actually use them (in your local environment or server apps like in the quickstart). be sure to use a whole recipe rather than individual widgets. if you really want to plot interactive widgets on their own, try from hover.utils.bokeh_helper import show_as_interactive as show instead of from bokeh.io import show . this works in your own environment but still not on the documentation page. show_as_interactive is a simple tweak of bokeh.io.show by turning standalone LayoutDOM to an application.","title":"Jupyterlab js issue"},{"location":"snippets/markdown/tutorial-required/","text":"This page assumes that you have know the basics i.e. simple usage of dataset and annotator . Please visit the quickstart tutorial if you haven't done so.","title":"Tutorial required"},{"location":"snippets/markdown/wrappy-cache/","text":"Caching and reading from disk This guide uses @wrappy.memoize in place of @functools.lru_cache for caching. The benefit is that wrappy.memoize can persist the cache to disk, speeding up code across sessions. Cached values for this guide have been pre-computed, making it much master to run the guide.","title":"Wrappy cache"}]}