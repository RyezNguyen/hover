{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hover - 0.5.0 Documentation Explore and mark areas on a map of your data. Zoom in, zoom out, hover.. and have fun! hover makes data annotation blazing fast by putting together embedding + visualization + callbacks . A 2D-embedded view of your dataset with tooltips See the whole picture while able to inspect every detail. A variety of tools to help you label data on the fly. Works both in Jupyter and as an web app. Balance between speed and precision Come up with labels as you inspect the data, no need to pre-define them. Utilize active learning directly in the annotation interface. Leverage multiple views and filter conditions of the data. Check out @phurwicz/hover-binder for a list of demo apps. Project News Dec 8, 2021 We are working on updates to make hover more extensible. An example would be the usage of VectorNet . Apr 30, 2021 0.5.0 is now available. Check out the changelog for details :partying_face:. Some tl-dr for the impatient: you can now filter selected data with search criteria, or soft label scores, or both! active learning now includes an interpolation between input and output manifolds, helping you explore decision boundaries and their formation. Quickstart Example script with walkthrough Example on Binder Install Python: 3.7+ OS: Mac & Linux To get the latest release version: pip install hover Feel free to open an issue if you would like conda or conda-forge support. Resources Binder repo Changelog Documentation Tutorials Remarks Shoutouts Thanks to Bokeh because hover would not exist without linked plots and callbacks, or be nearly as good without embeddable server apps. Thanks to Philip Vollet for sharing hover with the community even when it was really green. Contributing All feedbacks are welcome :hugs: Especially what you find frustrating and want fixed! Developer ./requirements-dev.txt lists recommended packages for development. You are encouraged, but not required, to use pre-commit hooks.","title":"Home"},{"location":"#hover-050-documentation","text":"Explore and mark areas on a map of your data. Zoom in, zoom out, hover.. and have fun! hover makes data annotation blazing fast by putting together embedding + visualization + callbacks . A 2D-embedded view of your dataset with tooltips See the whole picture while able to inspect every detail. A variety of tools to help you label data on the fly. Works both in Jupyter and as an web app. Balance between speed and precision Come up with labels as you inspect the data, no need to pre-define them. Utilize active learning directly in the annotation interface. Leverage multiple views and filter conditions of the data. Check out @phurwicz/hover-binder for a list of demo apps.","title":"Hover - 0.5.0 Documentation"},{"location":"#project-news","text":"Dec 8, 2021 We are working on updates to make hover more extensible. An example would be the usage of VectorNet . Apr 30, 2021 0.5.0 is now available. Check out the changelog for details :partying_face:. Some tl-dr for the impatient: you can now filter selected data with search criteria, or soft label scores, or both! active learning now includes an interpolation between input and output manifolds, helping you explore decision boundaries and their formation.","title":"Project News"},{"location":"#quickstart","text":"Example script with walkthrough Example on Binder","title":"Quickstart"},{"location":"#install","text":"Python: 3.7+ OS: Mac & Linux To get the latest release version: pip install hover Feel free to open an issue if you would like conda or conda-forge support.","title":"Install"},{"location":"#resources","text":"Binder repo Changelog Documentation Tutorials","title":"Resources"},{"location":"#remarks","text":"","title":"Remarks"},{"location":"#shoutouts","text":"Thanks to Bokeh because hover would not exist without linked plots and callbacks, or be nearly as good without embeddable server apps. Thanks to Philip Vollet for sharing hover with the community even when it was really green.","title":"Shoutouts"},{"location":"#contributing","text":"All feedbacks are welcome :hugs: Especially what you find frustrating and want fixed!","title":"Contributing"},{"location":"#developer","text":"./requirements-dev.txt lists recommended packages for development. You are encouraged, but not required, to use pre-commit hooks.","title":"Developer"},{"location":"pages/reference/core-dataset/","text":"Dataset classes which extend beyond DataFrames. When we supervise a collection of data, these operations need to be simple: managing raw / train / dev / test subsets transferring data points between subsets pulling updates from annotation interfaces pushing updates to annotation interfaces getting a 2D embedding loading data for training models SupervisableDataset Feature-agnostic class for a dataset open to supervision. Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that the DataFrame form supports most kinds of operations; the list-of-dicts form could be useful for manipulations outside the scope of pandas; synchronization between the two forms should be called sparingly. __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = 'feature' , label_key = 'label' ) special Create (1) dictl and df forms and (2) the mapping between categorical and string labels. Param Type Description raw_dictl list list of dicts holding the to-be-supervised raw data train_dictl list list of dicts holding any supervised train data dev_dictl list list of dicts holding any supervised dev data test_dictl list list of dicts holding any supervised test data feature_key str the key for the feature in each piece of data label_key str the key for the **str** label in supervised data Source code in hover/core/dataset.py def __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ note \"Create (1) dictl and df forms and (2) the mapping between categorical and string labels.\" | Param | Type | Description | | :------------ | :----- | :----------------------------------- | | `raw_dictl` | `list` | list of dicts holding the **to-be-supervised** raw data | | `train_dictl` | `list` | list of dicts holding any **supervised** train data | | `dev_dictl` | `list` | list of dicts holding any **supervised** dev data | | `test_dictl` | `list` | list of dicts holding any **supervised** test data | | `feature_key` | `str` | the key for the feature in each piece of data | | `label_key` | `str` | the key for the `**str**` label in supervised data | \"\"\" self . _info ( \"Initializing...\" ) def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] self . dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } self . synchronize_dictl_to_df () self . df_deduplicate () self . synchronize_df_to_dictl () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_file_export () self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . setup_sel_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _good ( f \" { self . __class__ . __name__ } : finished initialization.\" ) compute_2d_embedding ( self , vectorizer , method , ** kwargs ) Get embeddings in the xy-plane and return the dimensionality reducer. Reference: DimensionalityReducer Param Type Description vectorizer callable the feature -> vector function method str arg for DimensionalityReducer **kwargs kwargs for DimensionalityReducer Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ note \"Get embeddings in the xy-plane and return the dimensionality reducer.\" Reference: [`DimensionalityReducer`](https://github.com/phurwicz/hover/blob/main/hover/core/representation/reduction.py) | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" from hover.core.representation.reduction import DimensionalityReducer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () trans_arr = np . array ([ vectorizer ( _inp ) for _inp in tqdm ( feature_inp )]) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] self . dfs [ _key ][ \"x\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 0 ] ) self . dfs [ _key ][ \"y\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 1 ] ) start_idx += _length return reducer copy ( self , use_df = True ) Create another instance, copying over the data entries. Param Type Description use_df bool whether to use the df or dictl form Source code in hover/core/dataset.py def copy ( self , use_df = True ): \"\"\" ???+ note \"Create another instance, copying over the data entries.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `use_df` | `bool` | whether to use the df or dictl form | \"\"\" if use_df : self . synchronize_df_to_dictl () return self . __class__ ( raw_dictl = self . dictls [ \"raw\" ], train_dictl = self . dictls [ \"train\" ], dev_dictl = self . dictls [ \"dev\" ], test_dictl = self . dictls [ \"test\" ], feature_key = self . __class__ . FEATURE_KEY , label_key = \"label\" , ) df_deduplicate ( self ) Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" ???+ note \"Cross-deduplicate data entries by feature between subsets.\" \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" ) from_pandas ( df , ** kwargs ) classmethod Import from a pandas DataFrame. Param Type Description df DataFrame with a \"SUBSET\" field dividing subsets Source code in hover/core/dataset.py @classmethod def from_pandas ( cls , df , ** kwargs ): \"\"\" ???+ note \"Import from a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `df` | `DataFrame` | with a \"SUBSET\" field dividing subsets | \"\"\" SUBSETS = cls . SCRATCH_SUBSETS + cls . PUBLIC_SUBSETS + cls . PRIVATE_SUBSETS if DATASET_SUBSET_FIELD not in df . columns : raise ValueError ( f \"Expecting column ' { DATASET_SUBSET_FIELD } ' in the DataFrame which takes values from { SUBSETS } \" ) dictls = {} for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _sub_df = df [ df [ DATASET_SUBSET_FIELD ] == _subset ] dictls [ _subset ] = _sub_df . to_dict ( orient = \"records\" ) return cls ( raw_dictl = dictls [ \"raw\" ], train_dictl = dictls [ \"train\" ], dev_dictl = dictls [ \"dev\" ], test_dictl = dictls [ \"test\" ], ** kwargs , ) loader ( self , key , * vectorizers , * , batch_size = 64 , smoothing_coeff = 0.0 ) Prepare a torch Dataloader for training or evaluation. Param Type Description key str subset of data, e.g. \"train\" vectorizers callable (s) the feature -> vector function(s) batch_size int size per batch smoothing_coeff float portion of probability to equally split between classes Source code in hover/core/dataset.py def loader ( self , key , * vectorizers , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ note \"Prepare a torch `Dataloader` for training or evaluation.\" | Param | Type | Description | | :------------ | :------------ | :--------------------------------- | | `key` | `str` | subset of data, e.g. `\"train\"` | | `vectorizers` | `callable`(s) | the feature -> vector function(s) | | `batch_size` | `int` | size per batch | | `smoothing_coeff` | `float` | portion of probability to equally split between classes | \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import ( VectorDataset , MultiVectorDataset , one_hot , label_smoothing , ) # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) # prepare output vectors labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) # prepare input vectors assert len ( vectorizers ) > 0 , \"Expected at least one vectorizer\" multi_flag = len ( vectorizers ) > 1 features = df [ self . __class__ . FEATURE_KEY ] . tolist () input_vector_lists = [] for _vec_func in vectorizers : self . _info ( f \"Preparing { key } input vectors...\" ) _input_vecs = [ _vec_func ( _f ) for _f in tqdm ( features )] input_vector_lists . append ( _input_vecs ) self . _info ( f \"Preparing { key } data loader...\" ) if multi_flag : assert len ( input_vector_lists ) > 1 , \"Expected multiple lists of vectors\" loader = MultiVectorDataset ( input_vector_lists , output_vectors ) . loader ( batch_size = batch_size ) else : assert len ( input_vector_lists ) == 1 , \"Expected only one list of vectors\" input_vectors = input_vector_lists [ 0 ] loader = VectorDataset ( input_vectors , output_vectors ) . loader ( batch_size = batch_size ) self . _good ( f \"Prepared { key } loader with { len ( features ) } examples; { len ( vectorizers ) } vectors per feature, batch size { batch_size } \" ) return loader setup_label_coding ( self , verbose = True , debug = False ) Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add \"ABSTAIN\" as a no-label placeholder which gets ignored categorically. Param Type Description verbose bool whether to log verbosely debug bool whether to enable label validation Source code in hover/core/dataset.py def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ note \"Auto-determine labels in the dataset, then create encoder/decoder in lexical order.\" Add `\"ABSTAIN\"` as a no-label placeholder which gets ignored categorically. | Param | Type | Description | | :-------- | :----- | :--------------------------------- | | `verbose` | `bool` | whether to log verbosely | | `debug` | `bool` | whether to enable label validation | \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels () setup_pop_table ( self , ** kwargs ) Set up a bokeh DataTable widget for monitoring subset data populations. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for monitoring subset data populations.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], TableColumn ( field = \"color\" , title = \"color\" , formatter = HTMLTemplateFormatter ( template = COLOR_GLYPH_TEMPLATE ), ), ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] color_dict = auto_label_color ( self . classes ) eff_colors = [ color_dict [ _label ] for _label in eff_labels ] pop_data = dict ( color = eff_colors , label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Population updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . dedup_trigger . on_click ( update_population ) # store the callback so that it can be referenced by other methods self . _callback_update_population = update_population setup_sel_table ( self , ** kwargs ) Set up a bokeh DataTable widget for viewing selected data points. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_sel_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for viewing selected data points.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" def auto_columns ( df ): return [ TableColumn ( field = _col , title = _col ) for _col in df . columns ] sel_source = ColumnDataSource ( dict ()) sel_columns = auto_columns ( self . dfs [ \"train\" ]) self . sel_table = DataTable ( source = sel_source , columns = sel_columns , ** kwargs ) def update_selection ( selected_df ): \"\"\" Callback function. \"\"\" # push results to bokeh data source self . sel_table . columns = auto_columns ( selected_df ) sel_source . data = selected_df . to_dict ( orient = \"list\" ) self . _good ( f \"Selection table: latest selection with { selected_df . shape [ 0 ] } entries.\" ) self . _callback_update_selection = update_selection setup_widgets ( self ) Create bokeh widgets for interactive data management. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" ???+ note \"Create `bokeh` widgets for interactive data management.\" \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_viewer = Button ( label = \"View Selected\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. - PUSH shall be blocked until DEDUP is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. - COMMIT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget () subscribe_data_commit ( self , explorer , subset_mapping ) Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | \"\"\" def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return # take selected slice, ignoring ABSTAIN'ed rows # CAUTION: applying selected_idx from explorer.source to self.df # this assumes that the source and the df have consistent entries. # Consider this: # keep_cols = self.dfs[sub_k].columns # sel_slice = explorer.dfs[sub_v].iloc[selected_idx][keep_cols] sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) # chain another callback self . _callback_update_population () self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" ) subscribe_selection_view ( self , explorer , subsets ) Enable viewing groups of data entries, specified by a selection in an explorer. Param Type Description explorer BokehBaseExplorer the explorer to register subsets list subset selections to consider Source code in hover/core/dataset.py def subscribe_selection_view ( self , explorer , subsets ): \"\"\" ???+ note \"Enable viewing groups of data entries, specified by a selection in an explorer.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subsets` | `list` | subset selections to consider | \"\"\" assert ( isinstance ( subsets , list ) and len ( subsets ) > 0 ), \"Expected a non-empty list of subsets\" def callback_view (): sel_slices = [] for subset in subsets : selected_idx = explorer . sources [ subset ] . selected . indices sub_slice = explorer . dfs [ subset ] . iloc [ selected_idx ] sel_slices . append ( sub_slice ) selected = pd . concat ( sel_slices , axis = 0 ) # replace this with an actual display (and analysis) self . _callback_update_selection ( selected ) self . selection_viewer . on_click ( callback_view ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to selection view: { subsets } \" ) subscribe_update_push ( self , explorer , subset_mapping ) Enable pushing updated DataFrames to explorers that depend on them. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable pushing updated DataFrames to explorers that depend on them.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" # local import to avoid import cycles from hover.core.explorer.base import BokehBaseExplorer assert isinstance ( explorer , BokehBaseExplorer ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" ) synchronize_df_to_dictl ( self ) Re-make lists of dictionaries from dataframes. Source code in hover/core/dataset.py def synchronize_df_to_dictl ( self ): \"\"\" ???+ note \"Re-make lists of dictionaries from dataframes.\" \"\"\" self . dictls = dict () for _key , _df in self . dfs . items (): self . dictls [ _key ] = _df . to_dict ( orient = \"records\" ) synchronize_dictl_to_df ( self ) Re-make dataframes from lists of dictionaries. Source code in hover/core/dataset.py def synchronize_dictl_to_df ( self ): \"\"\" ???+ note \"Re-make dataframes from lists of dictionaries.\" \"\"\" self . dfs = dict () for _key , _dictl in self . dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df to_pandas ( self , use_df = True ) Export to a pandas DataFrame. Param Type Description use_df bool whether to use the df or dictl form Source code in hover/core/dataset.py def to_pandas ( self , use_df = True ): \"\"\" ???+ note \"Export to a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `use_df` | `bool` | whether to use the df or dictl form | \"\"\" if not use_df : self . synchronize_dictl_to_df () dfs = [] for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _df = self . dfs [ _subset ] . copy () _df [ DATASET_SUBSET_FIELD ] = _subset dfs . append ( _df ) return pd . concat ( dfs , axis = 0 ) validate_labels ( self , raise_exception = True ) Assert that every label is in the encoder. Param Type Description raise_exception bool whether to raise errors when failed Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" ???+ note \"Assert that every label is in the encoder.\" | Param | Type | Description | | :---------------- | :----- | :---------------------------------- | | `raise_exception` | `bool` | whether to raise errors when failed | \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : x in self . label_encoder ) _invalid_indices = np . where ( _mask is False )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ({ self . dfs [ _key ] . loc [ _invalid_indices ]}) if raise_exception : raise ValueError ( \"invalid labels\" ) view ( self ) Defines the layout of bokeh objects when visualized. Source code in hover/core/dataset.py def view ( self ): \"\"\" ???+ note \"Defines the layout of `bokeh` objects when visualized.\" \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , row ( self . update_pusher , self . data_committer , self . dedup_trigger , self . selection_viewer , self . file_exporter , ), self . pop_table , self . sel_table , ) SupervisableImageDataset Can add text-specific methods. SupervisableTextDataset Can add text-specific methods.","title":"hover.core.dataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset","text":"Feature-agnostic class for a dataset open to supervision. Keeping a DataFrame form and a list-of-dicts (\"dictl\") form, with the intention that the DataFrame form supports most kinds of operations; the list-of-dicts form could be useful for manipulations outside the scope of pandas; synchronization between the two forms should be called sparingly.","title":"SupervisableDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.__init__","text":"Create (1) dictl and df forms and (2) the mapping between categorical and string labels. Param Type Description raw_dictl list list of dicts holding the to-be-supervised raw data train_dictl list list of dicts holding any supervised train data dev_dictl list list of dicts holding any supervised dev data test_dictl list list of dicts holding any supervised test data feature_key str the key for the feature in each piece of data label_key str the key for the **str** label in supervised data Source code in hover/core/dataset.py def __init__ ( self , raw_dictl , train_dictl = None , dev_dictl = None , test_dictl = None , feature_key = \"feature\" , label_key = \"label\" , ): \"\"\" ???+ note \"Create (1) dictl and df forms and (2) the mapping between categorical and string labels.\" | Param | Type | Description | | :------------ | :----- | :----------------------------------- | | `raw_dictl` | `list` | list of dicts holding the **to-be-supervised** raw data | | `train_dictl` | `list` | list of dicts holding any **supervised** train data | | `dev_dictl` | `list` | list of dicts holding any **supervised** dev data | | `test_dictl` | `list` | list of dicts holding any **supervised** test data | | `feature_key` | `str` | the key for the feature in each piece of data | | `label_key` | `str` | the key for the `**str**` label in supervised data | \"\"\" self . _info ( \"Initializing...\" ) def dictl_transform ( dictl , labels = True ): \"\"\" Burner function to transform the input list of dictionaries into standard format. \"\"\" # edge case when dictl is empty or None if not dictl : return [] # transform the feature and possibly the label key_transform = { feature_key : self . __class__ . FEATURE_KEY } if labels : key_transform [ label_key ] = \"label\" def burner ( d ): \"\"\" Burner function to transform a single dict. \"\"\" if labels : assert label_key in d , f \"Expected dict key { label_key } \" trans_d = { key_transform . get ( _k , _k ): _v for _k , _v in d . items ()} if not labels : trans_d [ \"label\" ] = module_config . ABSTAIN_DECODED return trans_d return [ burner ( _d ) for _d in dictl ] self . dictls = { \"raw\" : dictl_transform ( raw_dictl , labels = False ), \"train\" : dictl_transform ( train_dictl ), \"dev\" : dictl_transform ( dev_dictl ), \"test\" : dictl_transform ( test_dictl ), } self . synchronize_dictl_to_df () self . df_deduplicate () self . synchronize_df_to_dictl () self . setup_widgets () # self.setup_label_coding() # redundant if setup_pop_table() immediately calls this again self . setup_file_export () self . setup_pop_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . setup_sel_table ( width_policy = \"fit\" , height_policy = \"fit\" ) self . _good ( f \" { self . __class__ . __name__ } : finished initialization.\" )","title":"__init__()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_2d_embedding","text":"Get embeddings in the xy-plane and return the dimensionality reducer. Reference: DimensionalityReducer Param Type Description vectorizer callable the feature -> vector function method str arg for DimensionalityReducer **kwargs kwargs for DimensionalityReducer Source code in hover/core/dataset.py def compute_2d_embedding ( self , vectorizer , method , ** kwargs ): \"\"\" ???+ note \"Get embeddings in the xy-plane and return the dimensionality reducer.\" Reference: [`DimensionalityReducer`](https://github.com/phurwicz/hover/blob/main/hover/core/representation/reduction.py) | Param | Type | Description | | :----------- | :--------- | :--------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `method` | `str` | arg for `DimensionalityReducer` | | `**kwargs` | | kwargs for `DimensionalityReducer` | \"\"\" from hover.core.representation.reduction import DimensionalityReducer # prepare input vectors to manifold learning fit_subset = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS ] trans_subset = [ * self . __class__ . PRIVATE_SUBSETS ] assert not set ( fit_subset ) . intersection ( set ( trans_subset )), \"Unexpected overlap\" # compute vectors and keep track which where to slice the array for fitting feature_inp = [] for _key in fit_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () fit_num = len ( feature_inp ) for _key in trans_subset : feature_inp += self . dfs [ _key ][ self . __class__ . FEATURE_KEY ] . tolist () trans_arr = np . array ([ vectorizer ( _inp ) for _inp in tqdm ( feature_inp )]) # initialize and fit manifold learning reducer using specified subarray self . _info ( f \"Fit-transforming { method . upper () } on { fit_num } samples...\" ) reducer = DimensionalityReducer ( trans_arr [: fit_num ]) fit_embedding = reducer . fit_transform ( method , ** kwargs ) # compute embedding of the whole dataset self . _info ( f \"Transforming { method . upper () } on { trans_arr . shape [ 0 ] - fit_num } samples...\" ) trans_embedding = reducer . transform ( trans_arr [ fit_num :], method ) # assign x and y coordinates to dataset start_idx = 0 for _subset , _embedding in [ ( fit_subset , fit_embedding ), ( trans_subset , trans_embedding ), ]: # edge case: embedding is too small if _embedding . shape [ 0 ] < 1 : for _key in _subset : assert ( self . dfs [ _key ] . shape [ 0 ] == 0 ), \"Expected empty df due to empty embedding\" continue for _key in _subset : _length = self . dfs [ _key ] . shape [ 0 ] self . dfs [ _key ][ \"x\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 0 ] ) self . dfs [ _key ][ \"y\" ] = pd . Series ( _embedding [ start_idx : ( start_idx + _length ), 1 ] ) start_idx += _length return reducer","title":"compute_2d_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.copy","text":"Create another instance, copying over the data entries. Param Type Description use_df bool whether to use the df or dictl form Source code in hover/core/dataset.py def copy ( self , use_df = True ): \"\"\" ???+ note \"Create another instance, copying over the data entries.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `use_df` | `bool` | whether to use the df or dictl form | \"\"\" if use_df : self . synchronize_df_to_dictl () return self . __class__ ( raw_dictl = self . dictls [ \"raw\" ], train_dictl = self . dictls [ \"train\" ], dev_dictl = self . dictls [ \"dev\" ], test_dictl = self . dictls [ \"test\" ], feature_key = self . __class__ . FEATURE_KEY , label_key = \"label\" , )","title":"copy()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.df_deduplicate","text":"Cross-deduplicate data entries by feature between subsets. Source code in hover/core/dataset.py def df_deduplicate ( self ): \"\"\" ???+ note \"Cross-deduplicate data entries by feature between subsets.\" \"\"\" self . _info ( \"Deduplicating...\" ) # for data entry accounting before , after = dict (), dict () # deduplicating rule: entries that come LATER are of higher priority ordered_subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] # keep track of which df has which columns and which rows came from which subset columns = dict () for _key in ordered_subsets : before [ _key ] = self . dfs [ _key ] . shape [ 0 ] columns [ _key ] = self . dfs [ _key ] . columns self . dfs [ _key ][ \"__subset\" ] = _key # concatenate in order and deduplicate overall_df = pd . concat ( [ self . dfs [ _key ] for _key in ordered_subsets ], axis = 0 , sort = False ) overall_df . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) overall_df . reset_index ( drop = True , inplace = True ) # cut up slices for _key in ordered_subsets : self . dfs [ _key ] = overall_df [ overall_df [ \"__subset\" ] == _key ] . reset_index ( drop = True , inplace = False )[ columns [ _key ]] after [ _key ] = self . dfs [ _key ] . shape [ 0 ] self . _info ( f \"--subset { _key } rows: { before [ _key ] } -> { after [ _key ] } .\" )","title":"df_deduplicate()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.from_pandas","text":"Import from a pandas DataFrame. Param Type Description df DataFrame with a \"SUBSET\" field dividing subsets Source code in hover/core/dataset.py @classmethod def from_pandas ( cls , df , ** kwargs ): \"\"\" ???+ note \"Import from a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `df` | `DataFrame` | with a \"SUBSET\" field dividing subsets | \"\"\" SUBSETS = cls . SCRATCH_SUBSETS + cls . PUBLIC_SUBSETS + cls . PRIVATE_SUBSETS if DATASET_SUBSET_FIELD not in df . columns : raise ValueError ( f \"Expecting column ' { DATASET_SUBSET_FIELD } ' in the DataFrame which takes values from { SUBSETS } \" ) dictls = {} for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _sub_df = df [ df [ DATASET_SUBSET_FIELD ] == _subset ] dictls [ _subset ] = _sub_df . to_dict ( orient = \"records\" ) return cls ( raw_dictl = dictls [ \"raw\" ], train_dictl = dictls [ \"train\" ], dev_dictl = dictls [ \"dev\" ], test_dictl = dictls [ \"test\" ], ** kwargs , )","title":"from_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.loader","text":"Prepare a torch Dataloader for training or evaluation. Param Type Description key str subset of data, e.g. \"train\" vectorizers callable (s) the feature -> vector function(s) batch_size int size per batch smoothing_coeff float portion of probability to equally split between classes Source code in hover/core/dataset.py def loader ( self , key , * vectorizers , batch_size = 64 , smoothing_coeff = 0.0 ): \"\"\" ???+ note \"Prepare a torch `Dataloader` for training or evaluation.\" | Param | Type | Description | | :------------ | :------------ | :--------------------------------- | | `key` | `str` | subset of data, e.g. `\"train\"` | | `vectorizers` | `callable`(s) | the feature -> vector function(s) | | `batch_size` | `int` | size per batch | | `smoothing_coeff` | `float` | portion of probability to equally split between classes | \"\"\" # lazy import: missing torch should not break the rest of the class from hover.utils.torch_helper import ( VectorDataset , MultiVectorDataset , one_hot , label_smoothing , ) # take the slice that has a meaningful label df = self . dfs [ key ][ self . dfs [ key ][ \"label\" ] != module_config . ABSTAIN_DECODED ] # edge case: valid slice is too small if df . shape [ 0 ] < 1 : raise ValueError ( f \"Subset { key } has too few samples ( { df . shape [ 0 ] } )\" ) batch_size = min ( batch_size , df . shape [ 0 ]) # prepare output vectors labels = df [ \"label\" ] . apply ( lambda x : self . label_encoder [ x ]) . tolist () output_vectors = one_hot ( labels , num_classes = len ( self . classes )) if smoothing_coeff > 0.0 : output_vectors = label_smoothing ( output_vectors , coefficient = smoothing_coeff ) # prepare input vectors assert len ( vectorizers ) > 0 , \"Expected at least one vectorizer\" multi_flag = len ( vectorizers ) > 1 features = df [ self . __class__ . FEATURE_KEY ] . tolist () input_vector_lists = [] for _vec_func in vectorizers : self . _info ( f \"Preparing { key } input vectors...\" ) _input_vecs = [ _vec_func ( _f ) for _f in tqdm ( features )] input_vector_lists . append ( _input_vecs ) self . _info ( f \"Preparing { key } data loader...\" ) if multi_flag : assert len ( input_vector_lists ) > 1 , \"Expected multiple lists of vectors\" loader = MultiVectorDataset ( input_vector_lists , output_vectors ) . loader ( batch_size = batch_size ) else : assert len ( input_vector_lists ) == 1 , \"Expected only one list of vectors\" input_vectors = input_vector_lists [ 0 ] loader = VectorDataset ( input_vectors , output_vectors ) . loader ( batch_size = batch_size ) self . _good ( f \"Prepared { key } loader with { len ( features ) } examples; { len ( vectorizers ) } vectors per feature, batch size { batch_size } \" ) return loader","title":"loader()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_label_coding","text":"Auto-determine labels in the dataset, then create encoder/decoder in lexical order. Add \"ABSTAIN\" as a no-label placeholder which gets ignored categorically. Param Type Description verbose bool whether to log verbosely debug bool whether to enable label validation Source code in hover/core/dataset.py def setup_label_coding ( self , verbose = True , debug = False ): \"\"\" ???+ note \"Auto-determine labels in the dataset, then create encoder/decoder in lexical order.\" Add `\"ABSTAIN\"` as a no-label placeholder which gets ignored categorically. | Param | Type | Description | | :-------- | :----- | :--------------------------------- | | `verbose` | `bool` | whether to log verbosely | | `debug` | `bool` | whether to enable label validation | \"\"\" all_labels = set () for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _df = self . dfs [ _key ] _found_labels = set ( _df [ \"label\" ] . tolist ()) all_labels = all_labels . union ( _found_labels ) # exclude ABSTAIN from self.classes, but include it in the encoding all_labels . discard ( module_config . ABSTAIN_DECODED ) self . classes = sorted ( all_labels ) self . label_encoder = { ** { _label : _i for _i , _label in enumerate ( self . classes )}, module_config . ABSTAIN_DECODED : module_config . ABSTAIN_ENCODED , } self . label_decoder = { _v : _k for _k , _v in self . label_encoder . items ()} if verbose : self . _good ( f \"Set up label encoder/decoder with { len ( self . classes ) } classes.\" ) if debug : self . validate_labels ()","title":"setup_label_coding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_pop_table","text":"Set up a bokeh DataTable widget for monitoring subset data populations. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_pop_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for monitoring subset data populations.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" subsets = [ * self . __class__ . SCRATCH_SUBSETS , * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS , ] pop_source = ColumnDataSource ( dict ()) pop_columns = [ TableColumn ( field = \"label\" , title = \"label\" ), * [ TableColumn ( field = f \"count_ { _subset } \" , title = _subset ) for _subset in subsets ], TableColumn ( field = \"color\" , title = \"color\" , formatter = HTMLTemplateFormatter ( template = COLOR_GLYPH_TEMPLATE ), ), ] self . pop_table = DataTable ( source = pop_source , columns = pop_columns , ** kwargs ) def update_population (): \"\"\" Callback function. \"\"\" # make sure that the label coding is correct self . setup_label_coding () # re-compute label population eff_labels = [ module_config . ABSTAIN_DECODED , * self . classes ] color_dict = auto_label_color ( self . classes ) eff_colors = [ color_dict [ _label ] for _label in eff_labels ] pop_data = dict ( color = eff_colors , label = eff_labels ) for _subset in subsets : _subpop = self . dfs [ _subset ][ \"label\" ] . value_counts () pop_data [ f \"count_ { _subset } \" ] = [ _subpop . get ( _label , 0 ) for _label in eff_labels ] # push results to bokeh data source pop_source . data = pop_data self . _good ( f \"Population updater: latest population with { len ( self . classes ) } classes.\" ) update_population () self . dedup_trigger . on_click ( update_population ) # store the callback so that it can be referenced by other methods self . _callback_update_population = update_population","title":"setup_pop_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_sel_table","text":"Set up a bokeh DataTable widget for viewing selected data points. Param Type Description **kwargs forwarded to the DataTable Source code in hover/core/dataset.py def setup_sel_table ( self , ** kwargs ): \"\"\" ???+ note \"Set up a bokeh `DataTable` widget for viewing selected data points.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to the `DataTable` | \"\"\" def auto_columns ( df ): return [ TableColumn ( field = _col , title = _col ) for _col in df . columns ] sel_source = ColumnDataSource ( dict ()) sel_columns = auto_columns ( self . dfs [ \"train\" ]) self . sel_table = DataTable ( source = sel_source , columns = sel_columns , ** kwargs ) def update_selection ( selected_df ): \"\"\" Callback function. \"\"\" # push results to bokeh data source self . sel_table . columns = auto_columns ( selected_df ) sel_source . data = selected_df . to_dict ( orient = \"list\" ) self . _good ( f \"Selection table: latest selection with { selected_df . shape [ 0 ] } entries.\" ) self . _callback_update_selection = update_selection","title":"setup_sel_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_widgets","text":"Create bokeh widgets for interactive data management. Source code in hover/core/dataset.py def setup_widgets ( self ): \"\"\" ???+ note \"Create `bokeh` widgets for interactive data management.\" \"\"\" self . update_pusher = Button ( label = \"Push\" , button_type = \"success\" , height_policy = \"fit\" , width_policy = \"min\" ) self . data_committer = Dropdown ( label = \"Commit\" , button_type = \"warning\" , menu = [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ], height_policy = \"fit\" , width_policy = \"min\" , ) self . dedup_trigger = Button ( label = \"Dedup\" , button_type = \"warning\" , height_policy = \"fit\" , width_policy = \"min\" , ) self . selection_viewer = Button ( label = \"View Selected\" , button_type = \"primary\" , height_policy = \"fit\" , width_policy = \"min\" , ) def commit_base_callback (): \"\"\" COMMIT creates cross-duplicates between subsets. - PUSH shall be blocked until DEDUP is executed. \"\"\" self . dedup_trigger . disabled = False self . update_pusher . disabled = True def dedup_base_callback (): \"\"\" DEDUP re-creates dfs with different indices than before. - COMMIT shall be blocked until PUSH is executed. \"\"\" self . update_pusher . disabled = False self . data_committer . disabled = True self . df_deduplicate () def push_base_callback (): \"\"\" PUSH enforces df consistency with all linked explorers. - DEDUP could be blocked because it stays trivial until COMMIT is executed. \"\"\" self . data_committer . disabled = False self . dedup_trigger . disabled = True self . update_pusher . on_click ( push_base_callback ) self . data_committer . on_click ( commit_base_callback ) self . dedup_trigger . on_click ( dedup_base_callback ) self . help_div = dataset_help_widget ()","title":"setup_widgets()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_data_commit","text":"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Source code in hover/core/dataset.py def subscribe_data_commit ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable committing data across subsets, specified by a selection in an explorer and a dropdown widget of the dataset.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | \"\"\" def callback_commit ( event ): for sub_k , sub_v in subset_mapping . items (): sub_to = event . item selected_idx = explorer . sources [ sub_v ] . selected . indices if not selected_idx : self . _warn ( f \"Attempting data commit: did not select any data points in subset { sub_v } .\" ) return # take selected slice, ignoring ABSTAIN'ed rows # CAUTION: applying selected_idx from explorer.source to self.df # this assumes that the source and the df have consistent entries. # Consider this: # keep_cols = self.dfs[sub_k].columns # sel_slice = explorer.dfs[sub_v].iloc[selected_idx][keep_cols] sel_slice = self . dfs [ sub_k ] . iloc [ selected_idx ] valid_slice = sel_slice [ sel_slice [ \"label\" ] != module_config . ABSTAIN_DECODED ] # concat to the end and do some accounting size_before = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] = pd . concat ( [ self . dfs [ sub_to ], valid_slice ], axis = 0 , sort = False , ignore_index = True , ) size_mid = self . dfs [ sub_to ] . shape [ 0 ] self . dfs [ sub_to ] . drop_duplicates ( subset = [ self . __class__ . FEATURE_KEY ], keep = \"last\" , inplace = True ) size_after = self . dfs [ sub_to ] . shape [ 0 ] self . _info ( f \"Committed { valid_slice . shape [ 0 ] } (valid out of { sel_slice . shape [ 0 ] } selected) entries from { sub_k } to { sub_to } ( { size_before } -> { size_after } with { size_mid - size_after } overwrites).\" ) # chain another callback self . _callback_update_population () self . data_committer . on_click ( callback_commit ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset commits: { subset_mapping } \" )","title":"subscribe_data_commit()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_selection_view","text":"Enable viewing groups of data entries, specified by a selection in an explorer. Param Type Description explorer BokehBaseExplorer the explorer to register subsets list subset selections to consider Source code in hover/core/dataset.py def subscribe_selection_view ( self , explorer , subsets ): \"\"\" ???+ note \"Enable viewing groups of data entries, specified by a selection in an explorer.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subsets` | `list` | subset selections to consider | \"\"\" assert ( isinstance ( subsets , list ) and len ( subsets ) > 0 ), \"Expected a non-empty list of subsets\" def callback_view (): sel_slices = [] for subset in subsets : selected_idx = explorer . sources [ subset ] . selected . indices sub_slice = explorer . dfs [ subset ] . iloc [ selected_idx ] sel_slices . append ( sub_slice ) selected = pd . concat ( sel_slices , axis = 0 ) # replace this with an actual display (and analysis) self . _callback_update_selection ( selected ) self . selection_viewer . on_click ( callback_view ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to selection view: { subsets } \" )","title":"subscribe_selection_view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_update_push","text":"Enable pushing updated DataFrames to explorers that depend on them. Param Type Description explorer BokehBaseExplorer the explorer to register subset_mapping dict dataset -> explorer subset mapping Note: the reason we need this is due to self.dfs[key] = ... -like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their self.dfs references. Source code in hover/core/dataset.py def subscribe_update_push ( self , explorer , subset_mapping ): \"\"\" ???+ note \"Enable pushing updated DataFrames to explorers that depend on them.\" | Param | Type | Description | | :--------------- | :----- | :------------------------------------- | | `explorer` | `BokehBaseExplorer` | the explorer to register | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | Note: the reason we need this is due to `self.dfs[key] = ...`-like assignments. If DF operations were all in-place, then the explorers could directly access the updates through their `self.dfs` references. \"\"\" # local import to avoid import cycles from hover.core.explorer.base import BokehBaseExplorer assert isinstance ( explorer , BokehBaseExplorer ) def callback_push (): df_dict = { _v : self . dfs [ _k ] for _k , _v in subset_mapping . items ()} explorer . _setup_dfs ( df_dict ) explorer . _update_sources () self . update_pusher . on_click ( callback_push ) self . _good ( f \"Subscribed { explorer . __class__ . __name__ } to dataset pushes: { subset_mapping } \" )","title":"subscribe_update_push()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_df_to_dictl","text":"Re-make lists of dictionaries from dataframes. Source code in hover/core/dataset.py def synchronize_df_to_dictl ( self ): \"\"\" ???+ note \"Re-make lists of dictionaries from dataframes.\" \"\"\" self . dictls = dict () for _key , _df in self . dfs . items (): self . dictls [ _key ] = _df . to_dict ( orient = \"records\" )","title":"synchronize_df_to_dictl()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_dictl_to_df","text":"Re-make dataframes from lists of dictionaries. Source code in hover/core/dataset.py def synchronize_dictl_to_df ( self ): \"\"\" ???+ note \"Re-make dataframes from lists of dictionaries.\" \"\"\" self . dfs = dict () for _key , _dictl in self . dictls . items (): if _dictl : _df = pd . DataFrame ( _dictl ) assert self . __class__ . FEATURE_KEY in _df . columns assert \"label\" in _df . columns else : _df = pd . DataFrame ( columns = [ self . __class__ . FEATURE_KEY , \"label\" ]) self . dfs [ _key ] = _df","title":"synchronize_dictl_to_df()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.to_pandas","text":"Export to a pandas DataFrame. Param Type Description use_df bool whether to use the df or dictl form Source code in hover/core/dataset.py def to_pandas ( self , use_df = True ): \"\"\" ???+ note \"Export to a pandas DataFrame.\" | Param | Type | Description | | :------- | :----- | :----------------------------------- | | `use_df` | `bool` | whether to use the df or dictl form | \"\"\" if not use_df : self . synchronize_dictl_to_df () dfs = [] for _subset in [ \"raw\" , \"train\" , \"dev\" , \"test\" ]: _df = self . dfs [ _subset ] . copy () _df [ DATASET_SUBSET_FIELD ] = _subset dfs . append ( _df ) return pd . concat ( dfs , axis = 0 )","title":"to_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.validate_labels","text":"Assert that every label is in the encoder. Param Type Description raise_exception bool whether to raise errors when failed Source code in hover/core/dataset.py def validate_labels ( self , raise_exception = True ): \"\"\" ???+ note \"Assert that every label is in the encoder.\" | Param | Type | Description | | :---------------- | :----- | :---------------------------------- | | `raise_exception` | `bool` | whether to raise errors when failed | \"\"\" for _key in [ * self . __class__ . PUBLIC_SUBSETS , * self . __class__ . PRIVATE_SUBSETS ]: _invalid_indices = None assert \"label\" in self . dfs [ _key ] . columns _mask = self . dfs [ _key ][ \"label\" ] . apply ( lambda x : x in self . label_encoder ) _invalid_indices = np . where ( _mask is False )[ 0 ] . tolist () if _invalid_indices : self . _fail ( f \"Subset { _key } has invalid labels:\" ) self . _print ({ self . dfs [ _key ] . loc [ _invalid_indices ]}) if raise_exception : raise ValueError ( \"invalid labels\" )","title":"validate_labels()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.view","text":"Defines the layout of bokeh objects when visualized. Source code in hover/core/dataset.py def view ( self ): \"\"\" ???+ note \"Defines the layout of `bokeh` objects when visualized.\" \"\"\" # local import to avoid naming confusion/conflicts from bokeh.layouts import row , column return column ( self . help_div , row ( self . update_pusher , self . data_committer , self . dedup_trigger , self . selection_viewer , self . file_exporter , ), self . pop_table , self . sel_table , )","title":"view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableImageDataset","text":"Can add text-specific methods.","title":"SupervisableImageDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableTextDataset","text":"Can add text-specific methods.","title":"SupervisableTextDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset","text":"","title":"hover.core.dataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset","text":"","title":"SupervisableDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.compute_2d_embedding","text":"","title":"compute_2d_embedding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.copy","text":"","title":"copy()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.df_deduplicate","text":"","title":"df_deduplicate()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.from_pandas","text":"","title":"from_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.loader","text":"","title":"loader()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_label_coding","text":"","title":"setup_label_coding()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_pop_table","text":"","title":"setup_pop_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_sel_table","text":"","title":"setup_sel_table()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.setup_widgets","text":"","title":"setup_widgets()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_data_commit","text":"","title":"subscribe_data_commit()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_selection_view","text":"","title":"subscribe_selection_view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.subscribe_update_push","text":"","title":"subscribe_update_push()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_df_to_dictl","text":"","title":"synchronize_df_to_dictl()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.synchronize_dictl_to_df","text":"","title":"synchronize_dictl_to_df()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.to_pandas","text":"","title":"to_pandas()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.validate_labels","text":"","title":"validate_labels()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableDataset.view","text":"","title":"view()"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableImageDataset","text":"","title":"SupervisableImageDataset"},{"location":"pages/reference/core-dataset/#hover.core.dataset.SupervisableTextDataset","text":"","title":"SupervisableTextDataset"},{"location":"pages/reference/core-explorer-base/","text":"Base class(es) for ALL explorer implementations. BokehBaseExplorer Base class for visually exploring data with Bokeh . Assumes: in supplied dataframes (always) xy coordinates in x and y columns; (always) an index for the rows; (always) classification label (or ABSTAIN) in a label column. Does not assume: a specific form of data; what the map serves to do. __init__ ( self , df_dict , ** kwargs ) special Constructor shared by all child classes. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults create widgets that child classes can override create data sources the correspond to class-specific data subsets. activate builtin search callbacks depending on the child class. initialize a figure under the settings above Source code in hover/core/explorer/base.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Constructor shared by all child classes.\" | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | 1. settle the figure settings by using child class defaults & kwargs overrides 2. settle the glyph settings by using child class defaults 3. create widgets that child classes can override 4. create data sources the correspond to class-specific data subsets. 5. activate builtin search callbacks depending on the child class. 6. initialize a figure under the settings above \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip ( kwargs . pop ( \"tooltips\" , \"\" )), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . figure = figure ( ** self . figure_kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_dfs ( df_dict ) self . _setup_sources () self . _setup_widgets () self . _activate_search_builtin () activate_search ( self , subset , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Left to child classes that have a specific feature format. Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/base.py @abstractmethod def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ note \"Left to child classes that have a specific feature format.\" | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" pass auto_color_mapping ( self ) Find all labels and an appropriate color for each. Source code in hover/core/explorer/base.py def auto_color_mapping ( self ): \"\"\" ???+ note \"Find all labels and an appropriate color for each.\" \"\"\" from hover.utils.bokeh_helper import auto_label_color labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) return auto_label_color ( labels ) from_dataset ( dataset , subset_mapping , * args , ** kwargs ) classmethod Alternative constructor from a SupervisableDataset . Param Type Description dataset SupervisableDataset dataset with DataFrame s subset_mapping dict dataset -> explorer subset mapping *args forwarded to the constructor **kwargs forwarded to the constructor Source code in hover/core/explorer/base.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" ???+ note \"Alternative constructor from a `SupervisableDataset`.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `dataset` | `SupervisableDataset` | dataset with `DataFrame`s | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | | `*args` | | forwarded to the constructor | | `**kwargs` | | forwarded to the constructor | \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} return cls ( df_dict , * args , ** kwargs ) link_selection ( self , key , other , other_key ) Synchronize the selected indices between specified sources. Param Type Description key str the key of the subset to link other BokehBaseExplorer the other explorer other_key str the key of the other subset Source code in hover/core/explorer/base.py def link_selection ( self , key , other , other_key ): \"\"\" ???+ note \"Synchronize the selected indices between specified sources.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `key` | `str` | the key of the subset to link | | `other` | `BokehBaseExplorer` | the other explorer | | `other_key` | `str` | the key of the other subset | \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] # deprecated: use js_link to sync attributes # sl.selected.js_link(\"indices\", sr.selected, \"indices\") # sr.selected.js_link(\"indices\", sl.selected, \"indices\") def left_to_right ( attr , old , new ): sr . selected . indices = sl . selected . indices [:] def right_to_left ( attr , old , new ): sl . selected . indices = sr . selected . indices [:] sl . selected . on_change ( \"indices\" , left_to_right ) sr . selected . on_change ( \"indices\" , right_to_left ) # link last manual selections (pointing to the same set) self . _last_selections [ key ] . union ( other . _last_selections [ other_key ]) # link selection filter functions (pointing to the same set) self . _selection_filters [ key ] . data . update ( other . _selection_filters [ other_key ] . data ) self . _selection_filters [ key ] . union ( other . _selection_filters [ other_key ]) link_xy_range ( self , other ) Synchronize plotting ranges on the xy-plane. Param Type Description other BokehBaseExplorer the other explorer Source code in hover/core/explorer/base.py def link_xy_range ( self , other ): \"\"\" ???+ note \"Synchronize plotting ranges on the xy-plane.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr ) plot ( self , * args , ** kwargs ) Plot something onto the figure. Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | *args | | left to child classes | | **kwargs | | left to child classes | Source code in hover/core/explorer/base.py @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot something onto the figure.\" Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | `*args` | | left to child classes | | `**kwargs` | | left to child classes | \"\"\" pass value_patch ( self , col_original , col_patch , ** kwargs ) Allow source values to be dynamically patched through a slider. Param Type Description col_original str column of values before the patch col_patch str column of list of values to use as patches **kwargs forwarded to the slider Reference Source code in hover/core/explorer/base.py def value_patch ( self , col_original , col_patch , ** kwargs ): \"\"\" ???+ note \"Allow source values to be dynamically patched through a slider.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `col_original` | `str` | column of values before the patch | | `col_patch` | `str` | column of list of values to use as patches | | `**kwargs` | | forwarded to the slider | [Reference](https://github.com/bokeh/bokeh/blob/2.3.0/examples/howto/patch_app.py) \"\"\" # add a patch slider to widgets, if none exist if \"patch_slider\" not in self . _dynamic_widgets : slider = Slider ( start = 0 , end = 1 , value = 0 , step = 1 , ** kwargs ) slider . disabled = True self . _dynamic_widgets [ \"patch_slider\" ] = slider else : slider = self . _dynamic_widgets [ \"patch_slider\" ] # create a slider-adjusting callback exposed to the outside def adjust_slider (): \"\"\" Infer slider length from the number of patch values. \"\"\" num_patches = None for _key , _df in self . dfs . items (): assert ( col_patch in _df . columns ), f \"Subset { _key } expecting column { col_patch } among columns, got { _df . columns } \" # find all array lengths; note that the data subset can be empty _num_patches_seen = _df [ col_patch ] . apply ( len ) . values assert ( len ( set ( _num_patches_seen )) <= 1 ), f \"Expecting consistent number of patches, got { _num_patches_seen } \" _num_patches = _num_patches_seen [ 0 ] if _df . shape [ 0 ] > 0 else None # if a previous subset has implied the number of patches, run a consistency check if num_patches is None : num_patches = _num_patches else : assert ( num_patches == _num_patches ), f \"Conflicting number of patches: { num_patches } vs { _num_patches } \" assert num_patches >= 2 , f \"Expecting at least 2 patches, got { num_patches } \" slider . end = num_patches - 1 slider . disabled = False self . _dynamic_callbacks [ \"adjust_patch_slider\" ] = adjust_slider # create the callback for patching values def update_patch ( attr , old , new ): for _key , _df in self . dfs . items (): # calculate the patch corresponding to slider value _value = [ _arr [ new ] for _arr in _df [ col_patch ] . values ] _slice = slice ( _df . shape [ 0 ]) _patch = { col_original : [( _slice , _value )]} self . sources [ _key ] . patch ( _patch ) slider . on_change ( \"value\" , update_patch ) self . _good ( f \"Patching { col_original } using { col_patch } \" ) view ( self ) Define the high-level visual layout of the whole explorer. Source code in hover/core/explorer/base.py def view ( self ): \"\"\" ???+ note \"Define the high-level visual layout of the whole explorer.\" \"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure )","title":".base"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer","text":"Base class for visually exploring data with Bokeh . Assumes: in supplied dataframes (always) xy coordinates in x and y columns; (always) an index for the rows; (always) classification label (or ABSTAIN) in a label column. Does not assume: a specific form of data; what the map serves to do.","title":"BokehBaseExplorer"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.__init__","text":"Constructor shared by all child classes. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure settle the figure settings by using child class defaults & kwargs overrides settle the glyph settings by using child class defaults create widgets that child classes can override create data sources the correspond to class-specific data subsets. activate builtin search callbacks depending on the child class. initialize a figure under the settings above Source code in hover/core/explorer/base.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Constructor shared by all child classes.\" | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | 1. settle the figure settings by using child class defaults & kwargs overrides 2. settle the glyph settings by using child class defaults 3. create widgets that child classes can override 4. create data sources the correspond to class-specific data subsets. 5. activate builtin search callbacks depending on the child class. 6. initialize a figure under the settings above \"\"\" self . figure_kwargs = { \"tools\" : STANDARD_PLOT_TOOLS , \"tooltips\" : self . _build_tooltip ( kwargs . pop ( \"tooltips\" , \"\" )), # bokeh recommends webgl for scalability \"output_backend\" : \"webgl\" , } self . figure_kwargs . update ( kwargs ) self . figure = figure ( ** self . figure_kwargs ) self . glyph_kwargs = { _key : _dict [ \"constant\" ] . copy () for _key , _dict in self . __class__ . SUBSET_GLYPH_KWARGS . items () } self . _setup_dfs ( df_dict ) self . _setup_sources () self . _setup_widgets () self . _activate_search_builtin ()","title":"__init__()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.activate_search","text":"Left to child classes that have a specific feature format. Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/base.py @abstractmethod def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ note \"Left to child classes that have a specific feature format.\" | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" pass","title":"activate_search()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.auto_color_mapping","text":"Find all labels and an appropriate color for each. Source code in hover/core/explorer/base.py def auto_color_mapping ( self ): \"\"\" ???+ note \"Find all labels and an appropriate color for each.\" \"\"\" from hover.utils.bokeh_helper import auto_label_color labels = set () for _key in self . dfs . keys (): labels = labels . union ( set ( self . dfs [ _key ][ \"label\" ] . values )) return auto_label_color ( labels )","title":"auto_color_mapping()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.from_dataset","text":"Alternative constructor from a SupervisableDataset . Param Type Description dataset SupervisableDataset dataset with DataFrame s subset_mapping dict dataset -> explorer subset mapping *args forwarded to the constructor **kwargs forwarded to the constructor Source code in hover/core/explorer/base.py @classmethod def from_dataset ( cls , dataset , subset_mapping , * args , ** kwargs ): \"\"\" ???+ note \"Alternative constructor from a `SupervisableDataset`.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `dataset` | `SupervisableDataset` | dataset with `DataFrame`s | | `subset_mapping` | `dict` | `dataset` -> `explorer` subset mapping | | `*args` | | forwarded to the constructor | | `**kwargs` | | forwarded to the constructor | \"\"\" # local import to avoid import cycles from hover.core.dataset import SupervisableDataset assert isinstance ( dataset , SupervisableDataset ) df_dict = { _v : dataset . dfs [ _k ] for _k , _v in subset_mapping . items ()} return cls ( df_dict , * args , ** kwargs )","title":"from_dataset()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_selection","text":"Synchronize the selected indices between specified sources. Param Type Description key str the key of the subset to link other BokehBaseExplorer the other explorer other_key str the key of the other subset Source code in hover/core/explorer/base.py def link_selection ( self , key , other , other_key ): \"\"\" ???+ note \"Synchronize the selected indices between specified sources.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `key` | `str` | the key of the subset to link | | `other` | `BokehBaseExplorer` | the other explorer | | `other_key` | `str` | the key of the other subset | \"\"\" self . _prelink_check ( other ) # link selection in a bidirectional manner sl , sr = self . sources [ key ], other . sources [ other_key ] # deprecated: use js_link to sync attributes # sl.selected.js_link(\"indices\", sr.selected, \"indices\") # sr.selected.js_link(\"indices\", sl.selected, \"indices\") def left_to_right ( attr , old , new ): sr . selected . indices = sl . selected . indices [:] def right_to_left ( attr , old , new ): sl . selected . indices = sr . selected . indices [:] sl . selected . on_change ( \"indices\" , left_to_right ) sr . selected . on_change ( \"indices\" , right_to_left ) # link last manual selections (pointing to the same set) self . _last_selections [ key ] . union ( other . _last_selections [ other_key ]) # link selection filter functions (pointing to the same set) self . _selection_filters [ key ] . data . update ( other . _selection_filters [ other_key ] . data ) self . _selection_filters [ key ] . union ( other . _selection_filters [ other_key ])","title":"link_selection()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_xy_range","text":"Synchronize plotting ranges on the xy-plane. Param Type Description other BokehBaseExplorer the other explorer Source code in hover/core/explorer/base.py def link_xy_range ( self , other ): \"\"\" ???+ note \"Synchronize plotting ranges on the xy-plane.\" | Param | Type | Description | | :------ | :------ | :----------------------------- | | `other` | `BokehBaseExplorer` | the other explorer | \"\"\" self . _prelink_check ( other ) # link coordinate ranges in a bidirectional manner for _attr in [ \"start\" , \"end\" ]: self . figure . x_range . js_link ( _attr , other . figure . x_range , _attr ) self . figure . y_range . js_link ( _attr , other . figure . y_range , _attr ) other . figure . x_range . js_link ( _attr , self . figure . x_range , _attr ) other . figure . y_range . js_link ( _attr , self . figure . y_range , _attr )","title":"link_xy_range()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.plot","text":"Plot something onto the figure. Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | *args | | left to child classes | | **kwargs | | left to child classes | Source code in hover/core/explorer/base.py @abstractmethod def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot something onto the figure.\" Implemented in child classes based on their functionalities. | Param | Type | Description | | :--------- | :---- | :-------------------- | | `*args` | | left to child classes | | `**kwargs` | | left to child classes | \"\"\" pass","title":"plot()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.value_patch","text":"Allow source values to be dynamically patched through a slider. Param Type Description col_original str column of values before the patch col_patch str column of list of values to use as patches **kwargs forwarded to the slider Reference Source code in hover/core/explorer/base.py def value_patch ( self , col_original , col_patch , ** kwargs ): \"\"\" ???+ note \"Allow source values to be dynamically patched through a slider.\" | Param | Type | Description | | :--------------- | :----- | :--------------------------- | | `col_original` | `str` | column of values before the patch | | `col_patch` | `str` | column of list of values to use as patches | | `**kwargs` | | forwarded to the slider | [Reference](https://github.com/bokeh/bokeh/blob/2.3.0/examples/howto/patch_app.py) \"\"\" # add a patch slider to widgets, if none exist if \"patch_slider\" not in self . _dynamic_widgets : slider = Slider ( start = 0 , end = 1 , value = 0 , step = 1 , ** kwargs ) slider . disabled = True self . _dynamic_widgets [ \"patch_slider\" ] = slider else : slider = self . _dynamic_widgets [ \"patch_slider\" ] # create a slider-adjusting callback exposed to the outside def adjust_slider (): \"\"\" Infer slider length from the number of patch values. \"\"\" num_patches = None for _key , _df in self . dfs . items (): assert ( col_patch in _df . columns ), f \"Subset { _key } expecting column { col_patch } among columns, got { _df . columns } \" # find all array lengths; note that the data subset can be empty _num_patches_seen = _df [ col_patch ] . apply ( len ) . values assert ( len ( set ( _num_patches_seen )) <= 1 ), f \"Expecting consistent number of patches, got { _num_patches_seen } \" _num_patches = _num_patches_seen [ 0 ] if _df . shape [ 0 ] > 0 else None # if a previous subset has implied the number of patches, run a consistency check if num_patches is None : num_patches = _num_patches else : assert ( num_patches == _num_patches ), f \"Conflicting number of patches: { num_patches } vs { _num_patches } \" assert num_patches >= 2 , f \"Expecting at least 2 patches, got { num_patches } \" slider . end = num_patches - 1 slider . disabled = False self . _dynamic_callbacks [ \"adjust_patch_slider\" ] = adjust_slider # create the callback for patching values def update_patch ( attr , old , new ): for _key , _df in self . dfs . items (): # calculate the patch corresponding to slider value _value = [ _arr [ new ] for _arr in _df [ col_patch ] . values ] _slice = slice ( _df . shape [ 0 ]) _patch = { col_original : [( _slice , _value )]} self . sources [ _key ] . patch ( _patch ) slider . on_change ( \"value\" , update_patch ) self . _good ( f \"Patching { col_original } using { col_patch } \" )","title":"value_patch()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.view","text":"Define the high-level visual layout of the whole explorer. Source code in hover/core/explorer/base.py def view ( self ): \"\"\" ???+ note \"Define the high-level visual layout of the whole explorer.\" \"\"\" from bokeh.layouts import column return column ( self . _layout_widgets (), self . figure )","title":"view()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base","text":"","title":"hover.core.explorer.base"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer","text":"","title":"BokehBaseExplorer"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.activate_search","text":"","title":"activate_search()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.auto_color_mapping","text":"","title":"auto_color_mapping()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.from_dataset","text":"","title":"from_dataset()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_selection","text":"","title":"link_selection()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.link_xy_range","text":"","title":"link_xy_range()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.value_patch","text":"","title":"value_patch()"},{"location":"pages/reference/core-explorer-base/#hover.core.explorer.base.BokehBaseExplorer.view","text":"","title":"view()"},{"location":"pages/reference/core-explorer-feature/","text":"Intermediate classes based on the main feature. BokehForAudio BokehBaseExplorer with audio (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) audio urls in an audio column Does not assume: what the explorer serves to do. activate_search ( self , subset , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Help wanted Trivial implementation until we figure out how to search audios. Create an issue if you have an idea :) Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/feature.py def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ help \"Help wanted\" Trivial implementation until we figure out how to search audios. [Create an issue](https://github.com/phurwicz/hover/issues/new) if you have an idea :) | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs BokehForImage BokehBaseExplorer with image (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) image urls in an image column Does not assume: what the explorer serves to do. activate_search ( self , subset , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Help wanted Trivial implementation until we figure out how to search images. Create an issue if you have an idea :) Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/feature.py def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ help \"Help wanted\" Trivial implementation until we figure out how to search images. [Create an issue](https://github.com/phurwicz/hover/issues/new) if you have an idea :) | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs BokehForText BokehBaseExplorer with text ( str ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) text data in a text column Does not assume: what the explorer serves to do. activate_search ( self , subset , kwargs , altered_param = ( 'size' , 10 , 5 , 7 )) Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/feature.py def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ note \"Enables string/regex search-and-highlight mechanism.\" Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () param_key , param_pos , param_neg , param_default = altered_param num_points = len ( self . sources [ subset ] . data [ \"text\" ]) self . sources [ subset ] . add ([ param_default ] * num_points , f \" { param_key } \" ) self . _extra_source_cols [ subset ][ param_key ] = param_default updated_kwargs [ param_key ] = param_key def search_response ( attr , old , new ): pos_regex , neg_regex = self . search_pos . value , self . search_neg . value def regex_score ( text ): score = 0 if len ( pos_regex ) > 0 : score += 1 if re . search ( pos_regex , text ) else - 2 if len ( neg_regex ) > 0 : score += - 2 if re . search ( neg_regex , text ) else 1 return score def score_to_param ( score ): if score > 0 : return param_pos elif score == 0 : return param_default else : return param_neg patch_slice = slice ( len ( self . sources [ subset ] . data [ \"text\" ])) search_scores = list ( map ( regex_score , self . sources [ subset ] . data [ \"text\" ])) search_params = list ( map ( score_to_param , search_scores )) self . sources [ subset ] . patch ( { SEARCH_SCORE_FIELD : [( patch_slice , search_scores )]} ) self . sources [ subset ] . patch ({ param_key : [( patch_slice , search_params )]}) return # js_callback = CustomJS( # args={ # \"source\": self.sources[subset], # \"key_pos\": self.search_pos, # \"key_neg\": self.search_neg, # \"param_pos\": param_pos, # \"param_neg\": param_neg, # \"param_default\": param_default, # }, # code=f\"\"\" # const data = source.data; # const text = data['text']; # var highlight_arr = data['{param_key}']; # var score_arr = data['{SEARCH_SCORE_FIELD}']; # \"\"\" # + \"\"\" # var search_pos = key_pos.value; # var search_neg = key_neg.value; # var valid_pos = (search_pos.length > 0); # var valid_neg = (search_neg.length > 0); # # function searchScore(candidate) # { # var score = 0; # if (valid_pos) { # if (candidate.search(search_pos) >= 0) { # score += 1; # } else { # score -= 2; # } # }; # if (valid_neg) { # if (candidate.search(search_neg) < 0) { # score += 1; # } else { # score -= 2; # } # }; # return score; # } # # function scoreToAttr(score) # { # // return attribute # if (score > 0) { # return param_pos; # } else if (score < 0) { # return param_neg; # } else {return param_default;} # } # # function toRegex(search_key) { # var match = search_key.match(new RegExp('^/(.*?)/([gimy]*)$')); # if (match) { # return new RegExp(match[1], match[2]); # } else { # return search_key; # } # } # # // convert search input to regex # if (valid_pos) {search_pos = toRegex(search_pos);} # if (valid_neg) {search_neg = toRegex(search_neg);} # # // search, store scores, and set highlight # for (var i = 0; i < highlight_arr.length; i++) { # var score = searchScore(text[i]); # score_arr[i] = score; # highlight_arr[i] = scoreToAttr(score); # } # # source.change.emit() # \"\"\", # ) # assign dynamic callback self . _dynamic_callbacks [ \"search_response\" ][ subset ] = search_response return updated_kwargs","title":".feature"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForAudio","text":"BokehBaseExplorer with audio (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) audio urls in an audio column Does not assume: what the explorer serves to do.","title":"BokehForAudio"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForAudio.activate_search","text":"Help wanted Trivial implementation until we figure out how to search audios. Create an issue if you have an idea :) Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/feature.py def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ help \"Help wanted\" Trivial implementation until we figure out how to search audios. [Create an issue](https://github.com/phurwicz/hover/issues/new) if you have an idea :) | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs","title":"activate_search()"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForImage","text":"BokehBaseExplorer with image (path like \"http://\" or \"file:///\" ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) image urls in an image column Does not assume: what the explorer serves to do.","title":"BokehForImage"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForImage.activate_search","text":"Help wanted Trivial implementation until we figure out how to search images. Create an issue if you have an idea :) Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/feature.py def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ help \"Help wanted\" Trivial implementation until we figure out how to search images. [Create an issue](https://github.com/phurwicz/hover/issues/new) if you have an idea :) | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" self . _warn ( \"no search highlight available.\" ) return kwargs","title":"activate_search()"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForText","text":"BokehBaseExplorer with text ( str ) as the main feature. Assumes on top of its parent class: in supplied dataframes (always) text data in a text column Does not assume: what the explorer serves to do.","title":"BokehForText"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForText.activate_search","text":"Enables string/regex search-and-highlight mechanism. Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. Param Type Description subset str the subset to activate search on kwargs bool kwargs for the plot to add to altered_param tuple (attribute, positive, negative, default) Source code in hover/core/explorer/feature.py def activate_search ( self , subset , kwargs , altered_param = ( \"size\" , 10 , 5 , 7 )): \"\"\" ???+ note \"Enables string/regex search-and-highlight mechanism.\" Modifies the plotting source in-place. Using a JS callback (instead of Python) so that it also works in standalone HTML. | Param | Type | Description | | :-------------- | :------ | :--------------------------- | | `subset` | `str` | the subset to activate search on | | `kwargs` | `bool` | kwargs for the plot to add to | | `altered_param` | `tuple` | (attribute, positive, negative, default) | \"\"\" assert isinstance ( kwargs , dict ) updated_kwargs = kwargs . copy () param_key , param_pos , param_neg , param_default = altered_param num_points = len ( self . sources [ subset ] . data [ \"text\" ]) self . sources [ subset ] . add ([ param_default ] * num_points , f \" { param_key } \" ) self . _extra_source_cols [ subset ][ param_key ] = param_default updated_kwargs [ param_key ] = param_key def search_response ( attr , old , new ): pos_regex , neg_regex = self . search_pos . value , self . search_neg . value def regex_score ( text ): score = 0 if len ( pos_regex ) > 0 : score += 1 if re . search ( pos_regex , text ) else - 2 if len ( neg_regex ) > 0 : score += - 2 if re . search ( neg_regex , text ) else 1 return score def score_to_param ( score ): if score > 0 : return param_pos elif score == 0 : return param_default else : return param_neg patch_slice = slice ( len ( self . sources [ subset ] . data [ \"text\" ])) search_scores = list ( map ( regex_score , self . sources [ subset ] . data [ \"text\" ])) search_params = list ( map ( score_to_param , search_scores )) self . sources [ subset ] . patch ( { SEARCH_SCORE_FIELD : [( patch_slice , search_scores )]} ) self . sources [ subset ] . patch ({ param_key : [( patch_slice , search_params )]}) return # js_callback = CustomJS( # args={ # \"source\": self.sources[subset], # \"key_pos\": self.search_pos, # \"key_neg\": self.search_neg, # \"param_pos\": param_pos, # \"param_neg\": param_neg, # \"param_default\": param_default, # }, # code=f\"\"\" # const data = source.data; # const text = data['text']; # var highlight_arr = data['{param_key}']; # var score_arr = data['{SEARCH_SCORE_FIELD}']; # \"\"\" # + \"\"\" # var search_pos = key_pos.value; # var search_neg = key_neg.value; # var valid_pos = (search_pos.length > 0); # var valid_neg = (search_neg.length > 0); # # function searchScore(candidate) # { # var score = 0; # if (valid_pos) { # if (candidate.search(search_pos) >= 0) { # score += 1; # } else { # score -= 2; # } # }; # if (valid_neg) { # if (candidate.search(search_neg) < 0) { # score += 1; # } else { # score -= 2; # } # }; # return score; # } # # function scoreToAttr(score) # { # // return attribute # if (score > 0) { # return param_pos; # } else if (score < 0) { # return param_neg; # } else {return param_default;} # } # # function toRegex(search_key) { # var match = search_key.match(new RegExp('^/(.*?)/([gimy]*)$')); # if (match) { # return new RegExp(match[1], match[2]); # } else { # return search_key; # } # } # # // convert search input to regex # if (valid_pos) {search_pos = toRegex(search_pos);} # if (valid_neg) {search_neg = toRegex(search_neg);} # # // search, store scores, and set highlight # for (var i = 0; i < highlight_arr.length; i++) { # var score = searchScore(text[i]); # score_arr[i] = score; # highlight_arr[i] = scoreToAttr(score); # } # # source.change.emit() # \"\"\", # ) # assign dynamic callback self . _dynamic_callbacks [ \"search_response\" ][ subset ] = search_response return updated_kwargs","title":"activate_search()"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature","text":"","title":"hover.core.explorer.feature"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForAudio","text":"","title":"BokehForAudio"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForAudio.activate_search","text":"","title":"activate_search()"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForImage","text":"","title":"BokehForImage"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForImage.activate_search","text":"","title":"activate_search()"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForText","text":"","title":"BokehForText"},{"location":"pages/reference/core-explorer-feature/#hover.core.explorer.feature.BokehForText.activate_search","text":"","title":"activate_search()"},{"location":"pages/reference/core-explorer-functionality/","text":"Intermediate classes based on the functionality. BokehDataAnnotator Annoate data points via callbacks on the buttons. Features: alter values in the 'label' column through the widgets. plot ( self ) Re-plot all data points with the new labels. Overrides the parent method. Determines the label -> color mapping dynamically. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Re-plot all data points with the new labels.\" Overrides the parent method. Determines the label -> color mapping dynamically. \"\"\" for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , color = SOURCE_COLOR_FIELD , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) BokehDataFinder Plot data points in grey ('gainsboro') and highlight search positives in coral. Features: the search widgets will highlight the results through a change of color, which gives arguably the best focus. plot ( self ) Plot all data points. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Plot all data points.\" \"\"\" for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" ) BokehMarginExplorer Plot data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios. __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ) special Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col_a and label_col_b for \"label margins\". Param Type Description df_dict dict str -> DataFrame mapping label_col_a str column for label set A label_col_b str column for label set B **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col_a and label_col_b for \"label margins\". | Param | Type | Description | | :------------ | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col_a` | `str` | column for label set A | | `label_col_b` | `str` | column for label set B | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs ) plot ( self , label , ** kwargs ) Plot the margins about a single label. Param Type Description label the label to plot about **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , label , ** kwargs ): \"\"\" ???+ note \"Plot the margins about a single label.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `label` | | the label to plot about | | `**kwargs` | | forwarded to plotting markers | \"\"\" for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( \"x\" , \"y\" , name = _key , source = _source , view = _view , ** eff_kwargs ) BokehSnorkelExplorer Plot data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set. __init__ ( self , df_dict , ** kwargs ) special Additional construtor Set up a list to keep track of plotted labeling functions. a palette for plotting labeling function predictions. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" Set up - a list to keep track of plotted labeling functions. - a palette for plotting labeling function predictions. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" super () . __init__ ( df_dict , ** kwargs ) # initialize a list to keep track of plotted LFs self . lfs = [] self . palette = Category20 [ 20 ] plot ( self , * args , ** kwargs ) Plot the raw subset in the background. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot the raw subset in the background.\" \"\"\" self . figure . circle ( \"x\" , \"y\" , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" ) plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( 'C' , 'I' , 'M' ), ** kwargs ) Plot about a single labeling function. Param Type Description lf callable labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw np.ndarray predictions, in decoded str , on the \"raw\" set L_labeled np.ndarray predictions, in decoded str , on the \"labeled\" set include tuple of str \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot **kwargs forwarded to plotting markers lf: labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw: . L_labeled: . include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer/functionality.py def plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" ???+ note \"Plot about a single labeling function.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `L_raw` | `np.ndarray` | predictions, in decoded `str`, on the `\"raw\"` set | | `L_labeled` | `np.ndarray` | predictions, in decoded `str`, on the `\"labeled\"` set | | `include` | `tuple` of `str` | \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot | | `**kwargs` | | forwarded to plotting markers | - lf: labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` - L_raw: . - L_labeled: . - include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # keep track of added LF self . lfs . append ( lf ) # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings legend_label = f \" { ', ' . join ( lf . targets ) } | { lf . name } \" color = self . palette [ len ( self . lfs ) - 1 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create correct/incorrect/missed/hit subsets to_plot = [] if \"C\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_correct ( L_labeled ), \"marker\" : self . figure . square , \"kwargs\" : labeled_glyph_kwargs , } ) if \"I\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_incorrect ( L_labeled ), \"marker\" : self . figure . x , \"kwargs\" : labeled_glyph_kwargs , } ) if \"M\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_missed ( L_labeled , lf . targets ), \"marker\" : self . figure . cross , \"kwargs\" : labeled_glyph_kwargs , } ) if \"H\" in include : to_plot . append ( { \"name\" : \"raw\" , \"view\" : self . _view_hit ( L_raw ), \"marker\" : self . figure . circle , \"kwargs\" : raw_glyph_kwargs , } ) # plot created subsets for _dict in to_plot : _name = _dict [ \"name\" ] _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _kwargs = _dict [ \"kwargs\" ] _marker ( \"x\" , \"y\" , source = _view . source , view = _view , name = _name , ** _kwargs ) BokehSoftLabelExplorer Plot data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios. __init__ ( self , df_dict , label_col , score_col , ** kwargs ) special Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col and score_col for \"soft predictions\". Param Type Description df_dict dict str -> DataFrame mapping label_col str column for the soft label score_col str column for the soft score **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col and score_col for \"soft predictions\". | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col` | `str` | column for the soft label | | `score_col` | `str` | column for the soft score | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs ) plot ( self , ** kwargs ) Plot all data points, setting color alpha based on the soft score. Param Type Description **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , ** kwargs ): \"\"\" ???+ note \"Plot all data points, setting color alpha based on the soft score.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to plotting markers | \"\"\" for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"color\" : SOURCE_COLOR_FIELD , \"fill_alpha\" : SOURCE_ALPHA_FIELD , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":".functionality"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator","text":"Annoate data points via callbacks on the buttons. Features: alter values in the 'label' column through the widgets.","title":"BokehDataAnnotator"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator.plot","text":"Re-plot all data points with the new labels. Overrides the parent method. Determines the label -> color mapping dynamically. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Re-plot all data points with the new labels.\" Overrides the parent method. Determines the label -> color mapping dynamically. \"\"\" for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , color = SOURCE_COLOR_FIELD , source = _source , ** self . glyph_kwargs [ _key ], ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder","text":"Plot data points in grey ('gainsboro') and highlight search positives in coral. Features: the search widgets will highlight the results through a change of color, which gives arguably the best focus.","title":"BokehDataFinder"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder.plot","text":"Plot all data points. Source code in hover/core/explorer/functionality.py def plot ( self ): \"\"\" ???+ note \"Plot all data points.\" \"\"\" for _key , _source in self . sources . items (): self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** self . glyph_kwargs [ _key ] ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer","text":"Plot data points along with two versions of labels. Could be useful for A/B tests. Features: can choose to only plot the margins about specific labels. currently not considering multi-label scenarios.","title":"BokehMarginExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.__init__","text":"Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col_a and label_col_b for \"label margins\". Param Type Description df_dict dict str -> DataFrame mapping label_col_a str column for label set A label_col_b str column for label set B **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col_a , label_col_b , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col_a and label_col_b for \"label margins\". | Param | Type | Description | | :------------ | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col_a` | `str` | column for label set A | | `label_col_b` | `str` | column for label set B | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" self . label_col_a = label_col_a self . label_col_b = label_col_b super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.plot","text":"Plot the margins about a single label. Param Type Description label the label to plot about **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , label , ** kwargs ): \"\"\" ???+ note \"Plot the margins about a single label.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `label` | | the label to plot about | | `**kwargs` | | forwarded to plotting markers | \"\"\" for _key , _source in self . sources . items (): # prepare plot settings eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( kwargs ) eff_kwargs [ \"legend_label\" ] = f \" { label } \" # create agreement/increment/decrement subsets col_a_pos = np . where ( self . dfs [ _key ][ self . label_col_a ] == label )[ 0 ] . tolist () col_a_neg = np . where ( self . dfs [ _key ][ self . label_col_a ] != label )[ 0 ] . tolist () col_b_pos = np . where ( self . dfs [ _key ][ self . label_col_b ] == label )[ 0 ] . tolist () col_b_neg = np . where ( self . dfs [ _key ][ self . label_col_b ] != label )[ 0 ] . tolist () agreement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_pos )] ) increment_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_neg ), IndexFilter ( col_b_pos )] ) decrement_view = CDSView ( source = _source , filters = [ IndexFilter ( col_a_pos ), IndexFilter ( col_b_neg )] ) to_plot = [ { \"view\" : agreement_view , \"marker\" : self . figure . square }, { \"view\" : increment_view , \"marker\" : self . figure . x }, { \"view\" : decrement_view , \"marker\" : self . figure . cross }, ] # plot created subsets for _dict in to_plot : _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _marker ( \"x\" , \"y\" , name = _key , source = _source , view = _view , ** eff_kwargs )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer","text":"Plot data points along with labeling function (LF) outputs. Features: each labeling function corresponds to its own line_color. uses a different marker for each type of predictions: square for 'correct', x for 'incorrect', cross for 'missed', circle for 'hit'. 'correct': the LF made a correct prediction on a point in the 'labeled' set. 'incorrect': the LF made an incorrect prediction on a point in the 'labeled' set. 'missed': the LF is capable of predicting the target class, but did not make such prediction on the particular point. 'hit': the LF made a prediction on a point in the 'raw' set.","title":"BokehSnorkelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.__init__","text":"Additional construtor Set up a list to keep track of plotted labeling functions. a palette for plotting labeling function predictions. Param Type Description df_dict dict str -> DataFrame mapping **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" Set up - a list to keep track of plotted labeling functions. - a palette for plotting labeling function predictions. | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" super () . __init__ ( df_dict , ** kwargs ) # initialize a list to keep track of plotted LFs self . lfs = [] self . palette = Category20 [ 20 ]","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot","text":"Plot the raw subset in the background. Source code in hover/core/explorer/functionality.py def plot ( self , * args , ** kwargs ): \"\"\" ???+ note \"Plot the raw subset in the background.\" \"\"\" self . figure . circle ( \"x\" , \"y\" , name = \"raw\" , source = self . sources [ \"raw\" ], ** self . glyph_kwargs [ \"raw\" ] ) self . _good ( f \"Plotted subset raw with { self . dfs [ 'raw' ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_lf","text":"Plot about a single labeling function. Param Type Description lf callable labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw np.ndarray predictions, in decoded str , on the \"raw\" set L_labeled np.ndarray predictions, in decoded str , on the \"labeled\" set include tuple of str \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot **kwargs forwarded to plotting markers lf: labeling function decorated by @labeling_function() from hover.utils.snorkel_helper L_raw: . L_labeled: . include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). Source code in hover/core/explorer/functionality.py def plot_lf ( self , lf , L_raw = None , L_labeled = None , include = ( \"C\" , \"I\" , \"M\" ), ** kwargs ): \"\"\" ???+ note \"Plot about a single labeling function.\" | Param | Type | Description | | :---------- | :--------------- | :--------------------------- | | `lf` | `callable` | labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` | | `L_raw` | `np.ndarray` | predictions, in decoded `str`, on the `\"raw\"` set | | `L_labeled` | `np.ndarray` | predictions, in decoded `str`, on the `\"labeled\"` set | | `include` | `tuple` of `str` | \"C\" for correct, \"I\" for incorrect, \"M\" for missed\", \"H\" for hit: types of predictions to make visible in the plot | | `**kwargs` | | forwarded to plotting markers | - lf: labeling function decorated by `@labeling_function()` from `hover.utils.snorkel_helper` - L_raw: . - L_labeled: . - include: subsets to show, which can be correct(C)/incorrect(I)/missed(M)/hit(H). \"\"\" # keep track of added LF self . lfs . append ( lf ) # calculate predicted labels if not provided if L_raw is None : L_raw = self . dfs [ \"raw\" ] . apply ( lf , axis = 1 ) . values if L_labeled is None : L_labeled = self . dfs [ \"labeled\" ] . apply ( lf , axis = 1 ) . values # prepare plot settings legend_label = f \" { ', ' . join ( lf . targets ) } | { lf . name } \" color = self . palette [ len ( self . lfs ) - 1 ] raw_glyph_kwargs = self . glyph_kwargs [ \"raw\" ] . copy () raw_glyph_kwargs [ \"legend_label\" ] = legend_label raw_glyph_kwargs [ \"color\" ] = color raw_glyph_kwargs . update ( kwargs ) labeled_glyph_kwargs = self . glyph_kwargs [ \"labeled\" ] . copy () labeled_glyph_kwargs [ \"legend_label\" ] = legend_label labeled_glyph_kwargs [ \"color\" ] = color labeled_glyph_kwargs . update ( kwargs ) # create correct/incorrect/missed/hit subsets to_plot = [] if \"C\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_correct ( L_labeled ), \"marker\" : self . figure . square , \"kwargs\" : labeled_glyph_kwargs , } ) if \"I\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_incorrect ( L_labeled ), \"marker\" : self . figure . x , \"kwargs\" : labeled_glyph_kwargs , } ) if \"M\" in include : to_plot . append ( { \"name\" : \"labeled\" , \"view\" : self . _view_missed ( L_labeled , lf . targets ), \"marker\" : self . figure . cross , \"kwargs\" : labeled_glyph_kwargs , } ) if \"H\" in include : to_plot . append ( { \"name\" : \"raw\" , \"view\" : self . _view_hit ( L_raw ), \"marker\" : self . figure . circle , \"kwargs\" : raw_glyph_kwargs , } ) # plot created subsets for _dict in to_plot : _name = _dict [ \"name\" ] _view = _dict [ \"view\" ] _marker = _dict [ \"marker\" ] _kwargs = _dict [ \"kwargs\" ] _marker ( \"x\" , \"y\" , source = _view . source , view = _view , name = _name , ** _kwargs )","title":"plot_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer","text":"Plot data points according to their labels and confidence scores. Features: the predicted label will correspond to fill_color. the confidence score, assumed to be a float between 0.0 and 1.0, will be reflected through fill_alpha. currently not considering multi-label scenarios.","title":"BokehSoftLabelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.__init__","text":"Additional construtor On top of the requirements of the parent class, the input dataframe should contain: label_col and score_col for \"soft predictions\". Param Type Description df_dict dict str -> DataFrame mapping label_col str column for the soft label score_col str column for the soft score **kwargs forwarded to bokeh.plotting.figure Source code in hover/core/explorer/functionality.py def __init__ ( self , df_dict , label_col , score_col , ** kwargs ): \"\"\" ???+ note \"Additional construtor\" On top of the requirements of the parent class, the input dataframe should contain: - label_col and score_col for \"soft predictions\". | Param | Type | Description | | :---------- | :----- | :--------------------------- | | `df_dict` | `dict` | `str` -> `DataFrame` mapping | | `label_col` | `str` | column for the soft label | | `score_col` | `str` | column for the soft score | | `**kwargs` | | forwarded to `bokeh.plotting.figure` | \"\"\" assert label_col != \"label\" , \"'label' field is reserved\" self . label_col = label_col self . score_col = score_col super () . __init__ ( df_dict , ** kwargs )","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.plot","text":"Plot all data points, setting color alpha based on the soft score. Param Type Description **kwargs forwarded to plotting markers Source code in hover/core/explorer/functionality.py def plot ( self , ** kwargs ): \"\"\" ???+ note \"Plot all data points, setting color alpha based on the soft score.\" | Param | Type | Description | | :--------- | :----- | :--------------------------- | | `**kwargs` | | forwarded to plotting markers | \"\"\" for _key , _source in self . sources . items (): # prepare plot settings preset_kwargs = { \"color\" : SOURCE_COLOR_FIELD , \"fill_alpha\" : SOURCE_ALPHA_FIELD , } eff_kwargs = self . glyph_kwargs [ _key ] . copy () eff_kwargs . update ( preset_kwargs ) eff_kwargs . update ( kwargs ) self . figure . circle ( \"x\" , \"y\" , name = _key , source = _source , ** eff_kwargs ) self . _good ( f \"Plotted subset { _key } with { self . dfs [ _key ] . shape [ 0 ] } points\" )","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality","text":"","title":"hover.core.explorer.functionality"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator","text":"","title":"BokehDataAnnotator"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataAnnotator.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder","text":"","title":"BokehDataFinder"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehDataFinder.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer","text":"","title":"BokehMarginExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehMarginExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer","text":"","title":"BokehSnorkelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSnorkelExplorer.plot_lf","text":"","title":"plot_lf()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer","text":"","title":"BokehSoftLabelExplorer"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-explorer-functionality/#hover.core.explorer.functionality.BokehSoftLabelExplorer.plot","text":"","title":"plot()"},{"location":"pages/reference/core-explorer-specialization/","text":"Child classes which are functionality -by- feature products. This could resemble template specialization in C++. BokehAudioAnnotator The audio flavor of BokehDataAnnotator .\" BokehAudioFinder The audio flavor of BokehDataFinder .\" BokehAudioMargin The audio flavor of BokehMarginExplorer .\" BokehAudioSnorkel The audio flavor of BokehSnorkelExplorer .\" BokehAudioSoftLabel The audio flavor of BokehSoftLabelExplorer .\" BokehImageAnnotator The image flavor of BokehDataAnnotator .\" BokehImageFinder The image flavor of BokehDataFinder .\" BokehImageMargin The image flavor of BokehMarginExplorer .\" BokehImageSnorkel The image flavor of BokehSnorkelExplorer .\" BokehImageSoftLabel The image flavor of BokehSoftLabelExplorer .\" BokehTextAnnotator The text flavor of BokehDataAnnotator .\" BokehTextFinder The text flavor of BokehDataFinder .\" BokehTextMargin The text flavor of BokehMarginExplorer .\" BokehTextSnorkel The text flavor of BokehSnorkelExplorer .\" BokehTextSoftLabel The text flavor of BokehSoftLabelExplorer .\"","title":".specialization"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioAnnotator","text":"The audio flavor of BokehDataAnnotator .\"","title":"BokehAudioAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioFinder","text":"The audio flavor of BokehDataFinder .\"","title":"BokehAudioFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioMargin","text":"The audio flavor of BokehMarginExplorer .\"","title":"BokehAudioMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSnorkel","text":"The audio flavor of BokehSnorkelExplorer .\"","title":"BokehAudioSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSoftLabel","text":"The audio flavor of BokehSoftLabelExplorer .\"","title":"BokehAudioSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageAnnotator","text":"The image flavor of BokehDataAnnotator .\"","title":"BokehImageAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageFinder","text":"The image flavor of BokehDataFinder .\"","title":"BokehImageFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageMargin","text":"The image flavor of BokehMarginExplorer .\"","title":"BokehImageMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSnorkel","text":"The image flavor of BokehSnorkelExplorer .\"","title":"BokehImageSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSoftLabel","text":"The image flavor of BokehSoftLabelExplorer .\"","title":"BokehImageSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextAnnotator","text":"The text flavor of BokehDataAnnotator .\"","title":"BokehTextAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextFinder","text":"The text flavor of BokehDataFinder .\"","title":"BokehTextFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextMargin","text":"The text flavor of BokehMarginExplorer .\"","title":"BokehTextMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSnorkel","text":"The text flavor of BokehSnorkelExplorer .\"","title":"BokehTextSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSoftLabel","text":"The text flavor of BokehSoftLabelExplorer .\"","title":"BokehTextSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization","text":"","title":"hover.core.explorer.specialization"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioAnnotator","text":"","title":"BokehAudioAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioFinder","text":"","title":"BokehAudioFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioMargin","text":"","title":"BokehAudioMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSnorkel","text":"","title":"BokehAudioSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehAudioSoftLabel","text":"","title":"BokehAudioSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageAnnotator","text":"","title":"BokehImageAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageFinder","text":"","title":"BokehImageFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageMargin","text":"","title":"BokehImageMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSnorkel","text":"","title":"BokehImageSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehImageSoftLabel","text":"","title":"BokehImageSoftLabel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextAnnotator","text":"","title":"BokehTextAnnotator"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextFinder","text":"","title":"BokehTextFinder"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextMargin","text":"","title":"BokehTextMargin"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSnorkel","text":"","title":"BokehTextSnorkel"},{"location":"pages/reference/core-explorer-specialization/#hover.core.explorer.specialization.BokehTextSoftLabel","text":"","title":"BokehTextSoftLabel"},{"location":"pages/reference/core-neural/","text":"Neural network components. torch -based template classes for implementing neural nets that work the most smoothly with hover MultiVectorNet Ensemble transfer learning model: multiple jointly-trained VectorNet's. Note that the VectorNets can have different vectorizers. Consequently, when training the nets, they expect multiple vectors per input. Coupled with: hover.utils.torch_helper.MultiVectorDataset DEFAULT_ADJACENCY_FUNC ( info_dict , acc_bar = 0.5 ) Everyone node points at the most different member that is not itself. Triggers if accuracies are high enough. Source code in hover/core/neural.py def disagreement_priority ( info_dict , acc_bar = 0.5 ): \"\"\" Everyone node points at the most different member that is not itself. Triggers if accuracies are high enough. \"\"\" refs = [] acc_list = info_dict [ \"accuracy\" ] disagree_dict = info_dict [ \"disagreement_rate\" ] for i in range ( 0 , len ( acc_list )): top_candidates = sorted ( disagree_dict [ i ] . keys (), key = lambda j : disagree_dict [ i ][ j ], reverse = True ) candidate = top_candidates [ 0 ] if top_candidates [ 0 ] != i else top_candidates [ 1 ] if acc_list [ i ] > acc_bar and acc_list [ candidate ] > acc_bar : refs . append ([ candidate ]) else : refs . append ([ i ]) return refs __init__ ( self , vector_nets , verbose = 0 ) special Create the VectorNet , loading parameters if available. Param Type Description vector_nets list list of VectorNet instances verbose int logging verbosity level Source code in hover/core/neural.py def __init__ ( self , vector_nets , verbose = 0 ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :-------------- | :----- | :--------------------------- | | `vector_nets` | `list` | list of VectorNet instances | | `verbose` | `int` | logging verbosity level | \"\"\" self . vector_nets = vector_nets self . _dynamic_params = dict () assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose self . _warn ( \"this class is in preview and is not sufficiently tested. Use with caution.\" ) adjust_optimizer_params ( self ) Adjust all optimizer params. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Adjust all optimizer params.\" \"\"\" for _net , _dict in zip ( self . vector_nets , self . _dynamic_params [ \"optimizer\" ]): _net . _dynamic_params [ \"optimizer\" ] = _dict . copy () _net . adjust_optimizer_params () evaluate_ensemble ( self , dev_loader ) Evaluate against a dev set, adding up logits from all VectorNets. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate_ensemble ( self , dev_loader ): \"\"\" ???+ note \"Evaluate against a dev set, adding up logits from all VectorNets.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" for _net in self . vector_nets : _net . nn . eval () true = [] pred = [] for loaded_input_list , loaded_output , _idx in dev_loader : net_inp_pairs = zip ( self . vector_nets , loaded_input_list ) _output_tensor = loaded_output . float () _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) _logits_sum = None for _net , _inp in net_inp_pairs : _logits = _net . nn ( _inp . float ()) . detach () if _logits_sum is None : _logits_sum = _logits . clone () else : _logits_sum = _logits_sum . add ( _logits . clone ()) _pred_batch = F . softmax ( _logits_sum , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Ensemble Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat evaluate_individual ( self , dev_loader ) Evaluate each VectorNet against a dev set. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate_individual ( self , dev_loader ): \"\"\" ???+ note \"Evaluate each VectorNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" for _net in self . vector_nets : _net . nn . eval () true = [] pred_list = [[] for _net in self . vector_nets ] for loaded_input_list , loaded_output , _idx in dev_loader : net_inp_pairs = zip ( self . vector_nets , loaded_input_list ) _output_tensor = loaded_output . float () _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) for i , ( _net , _inp ) in enumerate ( net_inp_pairs ): _logits = _net . nn ( _inp . float ()) _probs = F . softmax ( _logits , dim = 1 ) _pred_batch = _probs . argmax ( dim = 1 ) . detach () . numpy () pred_list [ i ] . append ( _pred_batch ) true = np . concatenate ( true ) pred_list = [ np . concatenate ( _pred ) for _pred in pred_list ] accuracy_list = [ classification_accuracy ( true , _pred ) for _pred in pred_list ] conf_mat_list = [ confusion_matrix ( true , _pred ) for _pred in pred_list ] disagree_rate = prediction_disagreement ( pred_list , reduce = True ) if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"\" . join ( [ \"|M {0} : Acc {1:.3f} \" . format ( i , _acc ) for i , _acc in enumerate ( accuracy_list ) ] ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy_list , conf_mat_list , disagree_rate train ( self , train_loader , params_per_epoch , dev_loader = None ) Train multiple VectorNet's jointly. Param Type Description train_loader torch.utils.data.DataLoader train set params_per_epoch list of dict updates to dynamic params dev_loader torch.utils.data.DataLoader dev set Example for params_per_epoch: def get_params(warmup_epochs=5, coteach_epochs=10, forget_rate=0.3): for i in range(warmup_epochs): yield {\"forget_rate\": 0.0, \"optimizer\": [{\"lr\": 0.1, \"momentum\": 0.9}] * 4} for i in range(coteach_epochs): yield {\"forget_rate\": forget_rate, \"optimizer\": [{\"lr\": 0.05, \"momentum\": 0.7}] * 4} params_per_epoch = get_params() Source code in hover/core/neural.py def train ( self , train_loader , params_per_epoch , dev_loader = None ): \"\"\" ???+ note \"Train multiple VectorNet's jointly.\" | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `params_per_epoch` | `list` of `dict` | updates to dynamic params | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | Example for params_per_epoch: ``` def get_params(warmup_epochs=5, coteach_epochs=10, forget_rate=0.3): for i in range(warmup_epochs): yield {\"forget_rate\": 0.0, \"optimizer\": [{\"lr\": 0.1, \"momentum\": 0.9}] * 4} for i in range(coteach_epochs): yield {\"forget_rate\": forget_rate, \"optimizer\": [{\"lr\": 0.05, \"momentum\": 0.7}] * 4} params_per_epoch = get_params() ``` \"\"\" train_info = [] for epoch_idx , param_dict in enumerate ( params_per_epoch ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . _dynamic_params . update ( param_dict ) self . train_epoch ( train_loader ) if dev_loader is None : dev_loader = train_loader acc_list , conf_list , disagree_rate = self . evaluate_individual ( dev_loader ) adj_func = self . _dynamic_params . get ( \"adjacency_function\" , self . __class__ . DEFAULT_ADJACENCY_FUNC , ) # keep training information and re-pick model teachers info_dict = { \"accuracy\" : acc_list , \"confusion_matrix\" : conf_list , \"disagreement_rate\" : disagree_rate , } train_info . append ( info_dict ) self . _dynamic_params [ \"tail_head_teachers\" ] = adj_func ( info_dict ) return train_info train_batch ( self , loaded_input_list , loaded_output ) Train all neural networks for one batch. Param Type Description loaded_input_list list of torch.Tensor input tensors loaded_output torch.Tensor output tensor verbose int verbosity for logging Source code in hover/core/neural.py def train_batch ( self , loaded_input_list , loaded_output ): \"\"\" ???+ note \"Train all neural networks for one batch.\" | Param | Type | Description | | :------------------ | :------------- | :---------------------- | | `loaded_input_list` | `list` of `torch.Tensor` | input tensors | | `loaded_output` | `torch.Tensor` | output tensor | | `verbose` | `int` | verbosity for logging | \"\"\" forget_rate = self . _dynamic_params [ \"forget_rate\" ] tail_head_teachers = self . _dynamic_params . get ( \"tail_head_teachers\" , [[ i ] for i , _net in enumerate ( self . vector_nets )] ) frozen_indices = self . _dynamic_params . get ( \"frozen\" , []) # determine which nets to enable train mode for i , _net in enumerate ( self . vector_nets ): if i not in frozen_indices : _net . nn . train () # compute logits output_tensor = loaded_output . float () net_inp_pairs = zip ( self . vector_nets , loaded_input_list ) logits_list = [ _net . nn ( _inp . float ()) for _net , _inp in net_inp_pairs ] loss_list = loss_coteaching_graph ( logits_list , output_tensor , tail_head_teachers , forget_rate , ) for i , ( _net , _loss ) in enumerate ( zip ( self . vector_nets , loss_list )): if i not in frozen_indices : _net . nn_optimizer . zero_grad () _loss . backward () _net . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"\" . join ( [ \"|M {0} : L {1:.3f} \" . format ( i , _loss ) for i , _loss in enumerate ( loss_list ) ] ) print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) VectorNet Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Coupled with: hover.utils.torch_helper.VectorDataset DEFAULT_OPTIM_CLS Implements Adam algorithm. It has been proposed in Adam: A Method for Stochastic Optimization _. Parameters: Name Type Description Default params iterable iterable of parameters to optimize or dicts defining parameter groups required lr float learning rate (default: 1e-3) required betas Tuple[float, float] coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) required eps float term added to the denominator to improve numerical stability (default: 1e-8) required weight_decay float weight decay (L2 penalty) (default: 0) required amsgrad boolean whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond _ (default: False) required .. _Adam: A Method for Stochastic Optimization: https://arxiv.org/abs/1412.6980 .. _On the Convergence of Adam and Beyond: https://openreview.net/forum?id=ryQu7f-RZ step ( self , closure = None ) Performs a single optimization step. Parameters: Name Type Description Default closure callable A closure that reevaluates the model and returns the loss. None Source code in hover/core/neural.py @torch . no_grad () def step ( self , closure = None ): \"\"\"Performs a single optimization step. Arguments: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None : with torch . enable_grad (): loss = closure () for group in self . param_groups : for p in group [ 'params' ]: if p . grad is None : continue grad = p . grad if grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) amsgrad = group [ 'amsgrad' ] state = self . state [ p ] # State initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) if amsgrad : # Maintains max of all exp. moving avg. of sq. grad. values state [ 'max_exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) exp_avg , exp_avg_sq = state [ 'exp_avg' ], state [ 'exp_avg_sq' ] if amsgrad : max_exp_avg_sq = state [ 'max_exp_avg_sq' ] beta1 , beta2 = group [ 'betas' ] state [ 'step' ] += 1 bias_correction1 = 1 - beta1 ** state [ 'step' ] bias_correction2 = 1 - beta2 ** state [ 'step' ] if group [ 'weight_decay' ] != 0 : grad = grad . add ( p , alpha = group [ 'weight_decay' ]) # Decay the first and second moment running average coefficient exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad , value = 1 - beta2 ) if amsgrad : # Maintains the maximum of all 2nd moment running avg. till now torch . max ( max_exp_avg_sq , exp_avg_sq , out = max_exp_avg_sq ) # Use the max. for normalizing running avg. of gradient denom = ( max_exp_avg_sq . sqrt () / math . sqrt ( bias_correction2 )) . add_ ( group [ 'eps' ]) else : denom = ( exp_avg_sq . sqrt () / math . sqrt ( bias_correction2 )) . add_ ( group [ 'eps' ]) step_size = group [ 'lr' ] / bias_correction1 p . addcdiv_ ( exp_avg , denom , value =- step_size ) return loss __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 ) special Create the VectorNet , loading parameters if available. Param Type Description vectorizer callable the feature -> vector function architecture class a torch.nn.Module child class state_dict_path str path to a (could-be-empty) torch state dict labels list list of str classification labels backup_state_dict bool whether to backup the loaded state dict optimizer_cls subclass of torch.optim.Optimizer pytorch optimizer class optimizer_kwargs dict pytorch optimizer kwargs verbose int logging verbosity level Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `architecture` | `class` | a `torch.nn.Module` child class | | `state_dict_path` | `str` | path to a (could-be-empty) `torch` state dict | | `labels` | `list` | list of `str` classification labels | | `backup_state_dict` | `bool` | whether to backup the loaded state dict | | `optimizer_cls` | `subclass of torch.optim.Optimizer` | pytorch optimizer class | | `optimizer_kwargs` | `dict` | pytorch optimizer kwargs | | `verbose` | `int` | logging verbosity level | \"\"\" assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose # set up label conversion self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) # set up vectorizer and the neural network with appropriate dimensions self . vectorizer = vectorizer vec_dim = self . vectorizer ( \"\" ) . shape [ 0 ] self . nn = architecture ( vec_dim , self . num_classes ) # if a state dict exists, load it and create a backup copy import os if os . path . isfile ( state_dict_path ): from shutil import copyfile try : self . nn . load_state_dict ( torch . load ( state_dict_path )) except Exception as e : self . _warn ( f \"Load VectorNet state path failed with { type ( e ) } : { e } \" ) if backup_state_dict : state_dict_backup_path = ( f \" { state_dict_path } . { current_time ( '%Y%m %d %H%M%S' ) } \" ) copyfile ( state_dict_path , state_dict_backup_path ) # set a path to store updated parameters self . nn_update_path = state_dict_path # initialize an optimizer object and a dict to hold dynamic parameters optimizer_cls = optimizer_cls or self . __class__ . DEFAULT_OPTIM_CLS optimizer_kwargs = ( optimizer_kwargs or self . __class__ . DEFAULT_OPTIM_KWARGS . copy () ) self . nn_optimizer = optimizer_cls ( self . nn . parameters ()) assert isinstance ( self . nn_optimizer , torch . optim . Optimizer ), f \"Expected an optimizer, got { type ( self . nn_optimizer ) } \" self . _dynamic_params = { \"optimizer\" : optimizer_kwargs } adjust_optimizer_params ( self ) Dynamically change parameters of the neural net optimizer. Intended to be polymorphic in child classes and to be called per epoch. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Dynamically change parameters of the neural net optimizer.\" - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ]) evaluate ( self , dev_loader ) Evaluate the VecNet against a dev set. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate ( self , dev_loader ): \"\"\" ???+ note \"Evaluate the VecNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat from_module ( model_module , labels ) classmethod Create a VectorNet model from a loadable module. Param Type Description model_module module or str (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable labels list list of str classification labels Source code in hover/core/neural.py @classmethod def from_module ( cls , model_module , labels ): \"\"\" ???+ note \"Create a VectorNet model from a loadable module.\" | Param | Type | Description | | :------------- | :--------- | :----------------------------------- | | `model_module` | `module` or `str` | (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable | | `labels` | `list` | list of `str` classification labels | \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ) return model manifold_trajectory ( self , inps , method = 'umap' , ** kwargs ) Compute a propagation trajectory of the dataset manifold through the neural net. vectorize inps forward propagate, keeping intermediates fit intermediates to 2D manifolds fit manifolds using Procrustes shape analysis fit shapes to trajectory splines Param Type Description inps dynamic (a list of) input features to vectorize method str reduction method: \"umap\" or \"ivis\" **kwargs kwargs to forward to dimensionality reduction Source code in hover/core/neural.py def manifold_trajectory ( self , inps , method = \"umap\" , ** kwargs ): \"\"\" ???+ note \"Compute a propagation trajectory of the dataset manifold through the neural net.\" 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to 2D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines | Param | Type | Description | | :------- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | | `method` | `str` | reduction method: `\"umap\"` or `\"ivis\"` | | `**kwargs` | | kwargs to forward to dimensionality reduction | \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** kwargs ) return traj_arr , seq_arr , disparities predict_proba ( self , inps ) End-to-end single/multi-piece prediction from inp to class probabilities. Param Type Description inps dynamic (a list of) input features to vectorize Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" ???+ note \"End-to-end single/multi-piece prediction from inp to class probabilities.\" | Param | Type | Description | | :----- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = not isinstance ( inps , list ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs save ( self , save_path = None ) Save the current state dict with authorization to overwrite. Param Type Description save_path str option alternative path to state dict Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" ???+ note \"Save the current state dict with authorization to overwrite.\" | Param | Type | Description | | :---------- | :---- | :------------------------------------ | | `save_path` | `str` | option alternative path to state dict | \"\"\" if save_path is None : save_path = self . nn_update_path torch . save ( self . nn . state_dict (), save_path ) train ( self , train_loader , dev_loader = None , epochs = 1 ) Train the neural network part of the VecNet. This method is a vanilla template and is intended to be overridden in child classes. Also intended to be coupled with self.train_batch(). Param Type Description train_loader torch.utils.data.DataLoader train set dev_loader torch.utils.data.DataLoader dev set epochs int number of epochs to train Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = 1 ): \"\"\" ???+ note \"Train the neural network part of the VecNet.\" - This method is a vanilla template and is intended to be overridden in child classes. - Also intended to be coupled with self.train_batch(). | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | | `epochs` | `int` | number of epochs to train | \"\"\" train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader ) if dev_loader is not None : dev_loader = train_loader acc , conf_mat = self . evaluate ( dev_loader ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info train_batch ( self , loaded_input , loaded_output ) Train the neural network for one batch. Param Type Description loaded_input torch.Tensor input tensor loaded_output torch.Tensor output tensor Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output ): \"\"\" ???+ note \"Train the neural network for one batch.\" | Param | Type | Description | | :-------------- | :------------- | :-------------------- | | `loaded_input` | `torch.Tensor` | input tensor | | `loaded_output` | `torch.Tensor` | output tensor | \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = cross_entropy_with_probs ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , ) train_epoch ( self , train_loader , * args , ** kwargs ) Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Param Type Description train_loader torch.utils.data.DataLoader train set *args arguments to forward to train_batch **kwargs kwargs to forward to train_batch Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" ???+ note \"Train the neural network for one epoch.\" - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `*args` | | arguments to forward to `train_batch` | | `**kwargs` | | kwargs to forward to `train_batch` | \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs )","title":"hover.core.neural"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet","text":"Ensemble transfer learning model: multiple jointly-trained VectorNet's. Note that the VectorNets can have different vectorizers. Consequently, when training the nets, they expect multiple vectors per input. Coupled with: hover.utils.torch_helper.MultiVectorDataset","title":"MultiVectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.DEFAULT_ADJACENCY_FUNC","text":"Everyone node points at the most different member that is not itself. Triggers if accuracies are high enough. Source code in hover/core/neural.py def disagreement_priority ( info_dict , acc_bar = 0.5 ): \"\"\" Everyone node points at the most different member that is not itself. Triggers if accuracies are high enough. \"\"\" refs = [] acc_list = info_dict [ \"accuracy\" ] disagree_dict = info_dict [ \"disagreement_rate\" ] for i in range ( 0 , len ( acc_list )): top_candidates = sorted ( disagree_dict [ i ] . keys (), key = lambda j : disagree_dict [ i ][ j ], reverse = True ) candidate = top_candidates [ 0 ] if top_candidates [ 0 ] != i else top_candidates [ 1 ] if acc_list [ i ] > acc_bar and acc_list [ candidate ] > acc_bar : refs . append ([ candidate ]) else : refs . append ([ i ]) return refs","title":"DEFAULT_ADJACENCY_FUNC()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.__init__","text":"Create the VectorNet , loading parameters if available. Param Type Description vector_nets list list of VectorNet instances verbose int logging verbosity level Source code in hover/core/neural.py def __init__ ( self , vector_nets , verbose = 0 ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :-------------- | :----- | :--------------------------- | | `vector_nets` | `list` | list of VectorNet instances | | `verbose` | `int` | logging verbosity level | \"\"\" self . vector_nets = vector_nets self . _dynamic_params = dict () assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose self . _warn ( \"this class is in preview and is not sufficiently tested. Use with caution.\" )","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.adjust_optimizer_params","text":"Adjust all optimizer params. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Adjust all optimizer params.\" \"\"\" for _net , _dict in zip ( self . vector_nets , self . _dynamic_params [ \"optimizer\" ]): _net . _dynamic_params [ \"optimizer\" ] = _dict . copy () _net . adjust_optimizer_params ()","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.evaluate_ensemble","text":"Evaluate against a dev set, adding up logits from all VectorNets. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate_ensemble ( self , dev_loader ): \"\"\" ???+ note \"Evaluate against a dev set, adding up logits from all VectorNets.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" for _net in self . vector_nets : _net . nn . eval () true = [] pred = [] for loaded_input_list , loaded_output , _idx in dev_loader : net_inp_pairs = zip ( self . vector_nets , loaded_input_list ) _output_tensor = loaded_output . float () _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) _logits_sum = None for _net , _inp in net_inp_pairs : _logits = _net . nn ( _inp . float ()) . detach () if _logits_sum is None : _logits_sum = _logits . clone () else : _logits_sum = _logits_sum . add ( _logits . clone ()) _pred_batch = F . softmax ( _logits_sum , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Ensemble Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat","title":"evaluate_ensemble()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.evaluate_individual","text":"Evaluate each VectorNet against a dev set. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate_individual ( self , dev_loader ): \"\"\" ???+ note \"Evaluate each VectorNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" for _net in self . vector_nets : _net . nn . eval () true = [] pred_list = [[] for _net in self . vector_nets ] for loaded_input_list , loaded_output , _idx in dev_loader : net_inp_pairs = zip ( self . vector_nets , loaded_input_list ) _output_tensor = loaded_output . float () _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) for i , ( _net , _inp ) in enumerate ( net_inp_pairs ): _logits = _net . nn ( _inp . float ()) _probs = F . softmax ( _logits , dim = 1 ) _pred_batch = _probs . argmax ( dim = 1 ) . detach () . numpy () pred_list [ i ] . append ( _pred_batch ) true = np . concatenate ( true ) pred_list = [ np . concatenate ( _pred ) for _pred in pred_list ] accuracy_list = [ classification_accuracy ( true , _pred ) for _pred in pred_list ] conf_mat_list = [ confusion_matrix ( true , _pred ) for _pred in pred_list ] disagree_rate = prediction_disagreement ( pred_list , reduce = True ) if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"\" . join ( [ \"|M {0} : Acc {1:.3f} \" . format ( i , _acc ) for i , _acc in enumerate ( accuracy_list ) ] ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy_list , conf_mat_list , disagree_rate","title":"evaluate_individual()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.train","text":"Train multiple VectorNet's jointly. Param Type Description train_loader torch.utils.data.DataLoader train set params_per_epoch list of dict updates to dynamic params dev_loader torch.utils.data.DataLoader dev set Example for params_per_epoch: def get_params(warmup_epochs=5, coteach_epochs=10, forget_rate=0.3): for i in range(warmup_epochs): yield {\"forget_rate\": 0.0, \"optimizer\": [{\"lr\": 0.1, \"momentum\": 0.9}] * 4} for i in range(coteach_epochs): yield {\"forget_rate\": forget_rate, \"optimizer\": [{\"lr\": 0.05, \"momentum\": 0.7}] * 4} params_per_epoch = get_params() Source code in hover/core/neural.py def train ( self , train_loader , params_per_epoch , dev_loader = None ): \"\"\" ???+ note \"Train multiple VectorNet's jointly.\" | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `params_per_epoch` | `list` of `dict` | updates to dynamic params | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | Example for params_per_epoch: ``` def get_params(warmup_epochs=5, coteach_epochs=10, forget_rate=0.3): for i in range(warmup_epochs): yield {\"forget_rate\": 0.0, \"optimizer\": [{\"lr\": 0.1, \"momentum\": 0.9}] * 4} for i in range(coteach_epochs): yield {\"forget_rate\": forget_rate, \"optimizer\": [{\"lr\": 0.05, \"momentum\": 0.7}] * 4} params_per_epoch = get_params() ``` \"\"\" train_info = [] for epoch_idx , param_dict in enumerate ( params_per_epoch ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . _dynamic_params . update ( param_dict ) self . train_epoch ( train_loader ) if dev_loader is None : dev_loader = train_loader acc_list , conf_list , disagree_rate = self . evaluate_individual ( dev_loader ) adj_func = self . _dynamic_params . get ( \"adjacency_function\" , self . __class__ . DEFAULT_ADJACENCY_FUNC , ) # keep training information and re-pick model teachers info_dict = { \"accuracy\" : acc_list , \"confusion_matrix\" : conf_list , \"disagreement_rate\" : disagree_rate , } train_info . append ( info_dict ) self . _dynamic_params [ \"tail_head_teachers\" ] = adj_func ( info_dict ) return train_info","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.train_batch","text":"Train all neural networks for one batch. Param Type Description loaded_input_list list of torch.Tensor input tensors loaded_output torch.Tensor output tensor verbose int verbosity for logging Source code in hover/core/neural.py def train_batch ( self , loaded_input_list , loaded_output ): \"\"\" ???+ note \"Train all neural networks for one batch.\" | Param | Type | Description | | :------------------ | :------------- | :---------------------- | | `loaded_input_list` | `list` of `torch.Tensor` | input tensors | | `loaded_output` | `torch.Tensor` | output tensor | | `verbose` | `int` | verbosity for logging | \"\"\" forget_rate = self . _dynamic_params [ \"forget_rate\" ] tail_head_teachers = self . _dynamic_params . get ( \"tail_head_teachers\" , [[ i ] for i , _net in enumerate ( self . vector_nets )] ) frozen_indices = self . _dynamic_params . get ( \"frozen\" , []) # determine which nets to enable train mode for i , _net in enumerate ( self . vector_nets ): if i not in frozen_indices : _net . nn . train () # compute logits output_tensor = loaded_output . float () net_inp_pairs = zip ( self . vector_nets , loaded_input_list ) logits_list = [ _net . nn ( _inp . float ()) for _net , _inp in net_inp_pairs ] loss_list = loss_coteaching_graph ( logits_list , output_tensor , tail_head_teachers , forget_rate , ) for i , ( _net , _loss ) in enumerate ( zip ( self . vector_nets , loss_list )): if i not in frozen_indices : _net . nn_optimizer . zero_grad () _loss . backward () _net . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"\" . join ( [ \"|M {0} : L {1:.3f} \" . format ( i , _loss ) for i , _loss in enumerate ( loss_list ) ] ) print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , )","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet","text":"Simple transfer learning model: a user-supplied vectorizer followed by a neural net. This is a parent class whose children may use different training schemes. Coupled with: hover.utils.torch_helper.VectorDataset","title":"VectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS","text":"Implements Adam algorithm. It has been proposed in Adam: A Method for Stochastic Optimization _. Parameters: Name Type Description Default params iterable iterable of parameters to optimize or dicts defining parameter groups required lr float learning rate (default: 1e-3) required betas Tuple[float, float] coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) required eps float term added to the denominator to improve numerical stability (default: 1e-8) required weight_decay float weight decay (L2 penalty) (default: 0) required amsgrad boolean whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond _ (default: False) required .. _Adam: A Method for Stochastic Optimization: https://arxiv.org/abs/1412.6980 .. _On the Convergence of Adam and Beyond: https://openreview.net/forum?id=ryQu7f-RZ","title":"DEFAULT_OPTIM_CLS"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS.step","text":"Performs a single optimization step. Parameters: Name Type Description Default closure callable A closure that reevaluates the model and returns the loss. None Source code in hover/core/neural.py @torch . no_grad () def step ( self , closure = None ): \"\"\"Performs a single optimization step. Arguments: closure (callable, optional): A closure that reevaluates the model and returns the loss. \"\"\" loss = None if closure is not None : with torch . enable_grad (): loss = closure () for group in self . param_groups : for p in group [ 'params' ]: if p . grad is None : continue grad = p . grad if grad . is_sparse : raise RuntimeError ( 'Adam does not support sparse gradients, please consider SparseAdam instead' ) amsgrad = group [ 'amsgrad' ] state = self . state [ p ] # State initialization if len ( state ) == 0 : state [ 'step' ] = 0 # Exponential moving average of gradient values state [ 'exp_avg' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) # Exponential moving average of squared gradient values state [ 'exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) if amsgrad : # Maintains max of all exp. moving avg. of sq. grad. values state [ 'max_exp_avg_sq' ] = torch . zeros_like ( p , memory_format = torch . preserve_format ) exp_avg , exp_avg_sq = state [ 'exp_avg' ], state [ 'exp_avg_sq' ] if amsgrad : max_exp_avg_sq = state [ 'max_exp_avg_sq' ] beta1 , beta2 = group [ 'betas' ] state [ 'step' ] += 1 bias_correction1 = 1 - beta1 ** state [ 'step' ] bias_correction2 = 1 - beta2 ** state [ 'step' ] if group [ 'weight_decay' ] != 0 : grad = grad . add ( p , alpha = group [ 'weight_decay' ]) # Decay the first and second moment running average coefficient exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad , value = 1 - beta2 ) if amsgrad : # Maintains the maximum of all 2nd moment running avg. till now torch . max ( max_exp_avg_sq , exp_avg_sq , out = max_exp_avg_sq ) # Use the max. for normalizing running avg. of gradient denom = ( max_exp_avg_sq . sqrt () / math . sqrt ( bias_correction2 )) . add_ ( group [ 'eps' ]) else : denom = ( exp_avg_sq . sqrt () / math . sqrt ( bias_correction2 )) . add_ ( group [ 'eps' ]) step_size = group [ 'lr' ] / bias_correction1 p . addcdiv_ ( exp_avg , denom , value =- step_size ) return loss","title":"step()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.__init__","text":"Create the VectorNet , loading parameters if available. Param Type Description vectorizer callable the feature -> vector function architecture class a torch.nn.Module child class state_dict_path str path to a (could-be-empty) torch state dict labels list list of str classification labels backup_state_dict bool whether to backup the loaded state dict optimizer_cls subclass of torch.optim.Optimizer pytorch optimizer class optimizer_kwargs dict pytorch optimizer kwargs verbose int logging verbosity level Source code in hover/core/neural.py def __init__ ( self , vectorizer , architecture , state_dict_path , labels , backup_state_dict = True , optimizer_cls = None , optimizer_kwargs = None , verbose = 0 , ): \"\"\" ???+ note \"Create the `VectorNet`, loading parameters if available.\" | Param | Type | Description | | :---------------- | :--------- | :----------------------------------- | | `vectorizer` | `callable` | the feature -> vector function | | `architecture` | `class` | a `torch.nn.Module` child class | | `state_dict_path` | `str` | path to a (could-be-empty) `torch` state dict | | `labels` | `list` | list of `str` classification labels | | `backup_state_dict` | `bool` | whether to backup the loaded state dict | | `optimizer_cls` | `subclass of torch.optim.Optimizer` | pytorch optimizer class | | `optimizer_kwargs` | `dict` | pytorch optimizer kwargs | | `verbose` | `int` | logging verbosity level | \"\"\" assert isinstance ( verbose , int ), f \"Expected verbose as int, got { type ( verbose ) } { verbose } \" self . verbose = verbose # set up label conversion self . label_encoder = { _label : i for i , _label in enumerate ( labels )} self . label_decoder = { i : _label for i , _label in enumerate ( labels )} self . num_classes = len ( self . label_encoder ) # set up vectorizer and the neural network with appropriate dimensions self . vectorizer = vectorizer vec_dim = self . vectorizer ( \"\" ) . shape [ 0 ] self . nn = architecture ( vec_dim , self . num_classes ) # if a state dict exists, load it and create a backup copy import os if os . path . isfile ( state_dict_path ): from shutil import copyfile try : self . nn . load_state_dict ( torch . load ( state_dict_path )) except Exception as e : self . _warn ( f \"Load VectorNet state path failed with { type ( e ) } : { e } \" ) if backup_state_dict : state_dict_backup_path = ( f \" { state_dict_path } . { current_time ( '%Y%m %d %H%M%S' ) } \" ) copyfile ( state_dict_path , state_dict_backup_path ) # set a path to store updated parameters self . nn_update_path = state_dict_path # initialize an optimizer object and a dict to hold dynamic parameters optimizer_cls = optimizer_cls or self . __class__ . DEFAULT_OPTIM_CLS optimizer_kwargs = ( optimizer_kwargs or self . __class__ . DEFAULT_OPTIM_KWARGS . copy () ) self . nn_optimizer = optimizer_cls ( self . nn . parameters ()) assert isinstance ( self . nn_optimizer , torch . optim . Optimizer ), f \"Expected an optimizer, got { type ( self . nn_optimizer ) } \" self . _dynamic_params = { \"optimizer\" : optimizer_kwargs }","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.adjust_optimizer_params","text":"Dynamically change parameters of the neural net optimizer. Intended to be polymorphic in child classes and to be called per epoch. Source code in hover/core/neural.py def adjust_optimizer_params ( self ): \"\"\" ???+ note \"Dynamically change parameters of the neural net optimizer.\" - Intended to be polymorphic in child classes and to be called per epoch. \"\"\" for _group in self . nn_optimizer . param_groups : _group . update ( self . _dynamic_params [ \"optimizer\" ])","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.evaluate","text":"Evaluate the VecNet against a dev set. Param Type Description dev_loader torch.utils.data.DataLoader dev set Source code in hover/core/neural.py def evaluate ( self , dev_loader ): \"\"\" ???+ note \"Evaluate the VecNet against a dev set.\" | Param | Type | Description | | :----------- | :----------- | :------------------------- | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | \"\"\" self . nn . eval () true = [] pred = [] for loaded_input , loaded_output , _idx in dev_loader : _input_tensor = loaded_input . float () _output_tensor = loaded_output . float () _logits = self . nn ( _input_tensor ) _true_batch = _output_tensor . argmax ( dim = 1 ) . detach () . numpy () _pred_batch = F . softmax ( _logits , dim = 1 ) . argmax ( dim = 1 ) . detach () . numpy () true . append ( _true_batch ) pred . append ( _pred_batch ) true = np . concatenate ( true ) pred = np . concatenate ( pred ) accuracy = classification_accuracy ( true , pred ) conf_mat = confusion_matrix ( true , pred ) if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Acc {0:.3f} \" . format ( accuracy ) self . _info ( \" {0: <80} \" . format ( \"Eval: Epoch {epoch} {performance} \" . format ( ** log_info ) ) ) return accuracy , conf_mat","title":"evaluate()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.from_module","text":"Create a VectorNet model from a loadable module. Param Type Description model_module module or str (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable labels list list of str classification labels Source code in hover/core/neural.py @classmethod def from_module ( cls , model_module , labels ): \"\"\" ???+ note \"Create a VectorNet model from a loadable module.\" | Param | Type | Description | | :------------- | :--------- | :----------------------------------- | | `model_module` | `module` or `str` | (path to) a local Python workspace module which contains a get_vectorizer() callable, get_architecture() callable, and a get_state_dict_path() callable | | `labels` | `list` | list of `str` classification labels | \"\"\" if isinstance ( model_module , str ): from importlib import import_module model_module = import_module ( model_module ) # Load the model by retrieving the inp-to-vec function, architecture, and state dict model = cls ( model_module . get_vectorizer (), model_module . get_architecture (), model_module . get_state_dict_path (), labels , ) return model","title":"from_module()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.manifold_trajectory","text":"Compute a propagation trajectory of the dataset manifold through the neural net. vectorize inps forward propagate, keeping intermediates fit intermediates to 2D manifolds fit manifolds using Procrustes shape analysis fit shapes to trajectory splines Param Type Description inps dynamic (a list of) input features to vectorize method str reduction method: \"umap\" or \"ivis\" **kwargs kwargs to forward to dimensionality reduction Source code in hover/core/neural.py def manifold_trajectory ( self , inps , method = \"umap\" , ** kwargs ): \"\"\" ???+ note \"Compute a propagation trajectory of the dataset manifold through the neural net.\" 1. vectorize inps 2. forward propagate, keeping intermediates 3. fit intermediates to 2D manifolds 4. fit manifolds using Procrustes shape analysis 5. fit shapes to trajectory splines | Param | Type | Description | | :------- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | | `method` | `str` | reduction method: `\"umap\"` or `\"ivis\"` | | `**kwargs` | | kwargs to forward to dimensionality reduction | \"\"\" from hover.core.representation.manifold import LayerwiseManifold from hover.core.representation.trajectory import manifold_spline # step 1 & 2 vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) self . nn . eval () intermediates = self . nn . eval_per_layer ( vectors ) intermediates = [ _tensor . detach () . numpy () for _tensor in intermediates ] # step 3 & 4 LM = LayerwiseManifold ( intermediates ) LM . unfold ( method = method ) seq_arr , disparities = LM . procrustes () seq_arr = np . array ( seq_arr ) # step 5 traj_arr = manifold_spline ( np . array ( seq_arr ), ** kwargs ) return traj_arr , seq_arr , disparities","title":"manifold_trajectory()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.predict_proba","text":"End-to-end single/multi-piece prediction from inp to class probabilities. Param Type Description inps dynamic (a list of) input features to vectorize Source code in hover/core/neural.py def predict_proba ( self , inps ): \"\"\" ???+ note \"End-to-end single/multi-piece prediction from inp to class probabilities.\" | Param | Type | Description | | :----- | :------ | :----------------------------------- | | `inps` | dynamic | (a list of) input features to vectorize | \"\"\" # if the input is a single piece of inp, cast it to a list FLAG_SINGLE = not isinstance ( inps , list ) if FLAG_SINGLE : inps = [ inps ] # the actual prediction self . nn . eval () vectors = torch . Tensor ([ self . vectorizer ( _inp ) for _inp in inps ]) logits = self . nn ( vectors ) probs = F . softmax ( logits , dim =- 1 ) . detach () . numpy () # inverse-cast if applicable if FLAG_SINGLE : probs = probs [ 0 ] return probs","title":"predict_proba()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.save","text":"Save the current state dict with authorization to overwrite. Param Type Description save_path str option alternative path to state dict Source code in hover/core/neural.py def save ( self , save_path = None ): \"\"\" ???+ note \"Save the current state dict with authorization to overwrite.\" | Param | Type | Description | | :---------- | :---- | :------------------------------------ | | `save_path` | `str` | option alternative path to state dict | \"\"\" if save_path is None : save_path = self . nn_update_path torch . save ( self . nn . state_dict (), save_path )","title":"save()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train","text":"Train the neural network part of the VecNet. This method is a vanilla template and is intended to be overridden in child classes. Also intended to be coupled with self.train_batch(). Param Type Description train_loader torch.utils.data.DataLoader train set dev_loader torch.utils.data.DataLoader dev set epochs int number of epochs to train Source code in hover/core/neural.py def train ( self , train_loader , dev_loader = None , epochs = 1 ): \"\"\" ???+ note \"Train the neural network part of the VecNet.\" - This method is a vanilla template and is intended to be overridden in child classes. - Also intended to be coupled with self.train_batch(). | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `dev_loader` | `torch.utils.data.DataLoader` | dev set | | `epochs` | `int` | number of epochs to train | \"\"\" train_info = [] for epoch_idx in range ( epochs ): self . _dynamic_params [ \"epoch\" ] = epoch_idx + 1 self . train_epoch ( train_loader ) if dev_loader is not None : dev_loader = train_loader acc , conf_mat = self . evaluate ( dev_loader ) train_info . append ({ \"accuracy\" : acc , \"confusion_matrix\" : conf_mat }) return train_info","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_batch","text":"Train the neural network for one batch. Param Type Description loaded_input torch.Tensor input tensor loaded_output torch.Tensor output tensor Source code in hover/core/neural.py def train_batch ( self , loaded_input , loaded_output ): \"\"\" ???+ note \"Train the neural network for one batch.\" | Param | Type | Description | | :-------------- | :------------- | :-------------------- | | `loaded_input` | `torch.Tensor` | input tensor | | `loaded_output` | `torch.Tensor` | output tensor | \"\"\" self . nn . train () input_tensor = loaded_input . float () output_tensor = loaded_output . float () # compute logits logits = self . nn ( input_tensor ) loss = cross_entropy_with_probs ( logits , output_tensor ) self . nn_optimizer . zero_grad () loss . backward () self . nn_optimizer . step () if self . verbose > 0 : log_info = dict ( self . _dynamic_params ) log_info [ \"performance\" ] = \"Loss {0:.3f} \" . format ( loss ) self . _print ( \" {0: <80} \" . format ( \"Train: Epoch {epoch} Batch {batch} {performance} \" . format ( ** log_info ) ), end = \" \\r \" , )","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_epoch","text":"Train the neural network for one epoch. Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. Param Type Description train_loader torch.utils.data.DataLoader train set *args arguments to forward to train_batch **kwargs kwargs to forward to train_batch Source code in hover/core/neural.py def train_epoch ( self , train_loader , * args , ** kwargs ): \"\"\" ???+ note \"Train the neural network for one epoch.\" - Supports flexible args and kwargs for child classes that may implement self.train() and self.train_batch() differently. | Param | Type | Description | | :------------- | :----------- | :------------------------- | | `train_loader` | `torch.utils.data.DataLoader` | train set | | `*args` | | arguments to forward to `train_batch` | | `**kwargs` | | kwargs to forward to `train_batch` | \"\"\" self . adjust_optimizer_params () for batch_idx , ( loaded_input , loaded_output , _ ) in enumerate ( train_loader ): self . _dynamic_params [ \"batch\" ] = batch_idx + 1 self . train_batch ( loaded_input , loaded_output , * args , ** kwargs )","title":"train_epoch()"},{"location":"pages/reference/core-neural/#hover.core.neural","text":"","title":"hover.core.neural"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet","text":"","title":"MultiVectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.DEFAULT_ADJACENCY_FUNC","text":"","title":"DEFAULT_ADJACENCY_FUNC()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.adjust_optimizer_params","text":"","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.evaluate_ensemble","text":"","title":"evaluate_ensemble()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.evaluate_individual","text":"","title":"evaluate_individual()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.train","text":"","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.MultiVectorNet.train_batch","text":"","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet","text":"","title":"VectorNet"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS","text":"","title":"DEFAULT_OPTIM_CLS"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.DEFAULT_OPTIM_CLS.step","text":"","title":"step()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.adjust_optimizer_params","text":"","title":"adjust_optimizer_params()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.evaluate","text":"","title":"evaluate()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.from_module","text":"","title":"from_module()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.manifold_trajectory","text":"","title":"manifold_trajectory()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.predict_proba","text":"","title":"predict_proba()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.save","text":"","title":"save()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train","text":"","title":"train()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_batch","text":"","title":"train_batch()"},{"location":"pages/reference/core-neural/#hover.core.neural.VectorNet.train_epoch","text":"","title":"train_epoch()"},{"location":"pages/reference/core-representation/","text":"hover.core.representation.reduction Linker data structures which tie (potentially multiple) dimensionality reducers to arrays. The point is to make it clear which reduction is in reference to which array. Icing on the cake: unify the syntax across different kinds of reducers. DimensionalityReducer __init__ ( self , array ) special Link self to the shared input array for reduction methods. Param Type Description array np.ndarray the input array to fit on Source code in hover/core/representation/reduction.py def __init__ ( self , array ): \"\"\" ???+ note \"Link self to the shared input array for reduction methods.\" | Param | Type | Description | | :------ | :----------- | :---------------------------- | | `array` | `np.ndarray` | the input array to fit on | \"\"\" self . reference_array = array fit_transform ( self , method , * args , ** kwargs ) Fit and transform an array and store the reducer. Param Type Description method str \"umap\" or \"ivis\" *args forwarded to the reducer **kwargs forwarded to the reducer Source code in hover/core/representation/reduction.py def fit_transform ( self , method , * args , ** kwargs ): \"\"\" ???+ note \"Fit and transform an array and store the reducer.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | forwarded to the reducer | \"\"\" if method == \"umap\" : try : import umap reducer = umap . UMAP ( * args , ** kwargs ) except ModuleNotFoundError : raise ModuleNotFoundError ( \"Please install umap-learn via pip.\" ) elif method == \"ivis\" : try : import ivis reducer = ivis . Ivis ( * args , ** kwargs ) except ModuleNotFoundError : raise ModuleNotFoundError ( \"Please install ivis[cpu] or ivis[gpu] via pip.\" ) else : raise ValueError ( self . __class__ . METHOD_ERROR_MSG ) embedding = reducer . fit_transform ( self . reference_array ) setattr ( self , method , reducer ) return embedding transform ( self , array , method ) Transform an array with a already-fitted reducer. Param Type Description array np.ndarray the array to transform method str \"umap\" or \"ivis\" Source code in hover/core/representation/reduction.py def transform ( self , array , method ): \"\"\" ???+ note \"Transform an array with a already-fitted reducer.\" | Param | Type | Description | | :--------- | :----------- | :----------------------- | | `array` | `np.ndarray` | the array to transform | | `method` | `str` | `\"umap\"` or `\"ivis\"` | \"\"\" assert method in [ \"umap\" , \"ivis\" ], self . method_error_msg assert isinstance ( array , np . ndarray ), f \"Expected np.ndarray, got { type ( array ) } \" # edge case: array is too small if array . shape [ 0 ] < 1 : return np . array ([]) reducer = getattr ( self , method ) return reducer . transform ( array )","title":"hover.core.representation"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction","text":"Linker data structures which tie (potentially multiple) dimensionality reducers to arrays. The point is to make it clear which reduction is in reference to which array. Icing on the cake: unify the syntax across different kinds of reducers.","title":"reduction"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer","text":"","title":"DimensionalityReducer"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.__init__","text":"Link self to the shared input array for reduction methods. Param Type Description array np.ndarray the input array to fit on Source code in hover/core/representation/reduction.py def __init__ ( self , array ): \"\"\" ???+ note \"Link self to the shared input array for reduction methods.\" | Param | Type | Description | | :------ | :----------- | :---------------------------- | | `array` | `np.ndarray` | the input array to fit on | \"\"\" self . reference_array = array","title":"__init__()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.fit_transform","text":"Fit and transform an array and store the reducer. Param Type Description method str \"umap\" or \"ivis\" *args forwarded to the reducer **kwargs forwarded to the reducer Source code in hover/core/representation/reduction.py def fit_transform ( self , method , * args , ** kwargs ): \"\"\" ???+ note \"Fit and transform an array and store the reducer.\" | Param | Type | Description | | :--------- | :----- | :----------------------- | | `method` | `str` | `\"umap\"` or `\"ivis\"` | | `*args` | | forwarded to the reducer | | `**kwargs` | | forwarded to the reducer | \"\"\" if method == \"umap\" : try : import umap reducer = umap . UMAP ( * args , ** kwargs ) except ModuleNotFoundError : raise ModuleNotFoundError ( \"Please install umap-learn via pip.\" ) elif method == \"ivis\" : try : import ivis reducer = ivis . Ivis ( * args , ** kwargs ) except ModuleNotFoundError : raise ModuleNotFoundError ( \"Please install ivis[cpu] or ivis[gpu] via pip.\" ) else : raise ValueError ( self . __class__ . METHOD_ERROR_MSG ) embedding = reducer . fit_transform ( self . reference_array ) setattr ( self , method , reducer ) return embedding","title":"fit_transform()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.transform","text":"Transform an array with a already-fitted reducer. Param Type Description array np.ndarray the array to transform method str \"umap\" or \"ivis\" Source code in hover/core/representation/reduction.py def transform ( self , array , method ): \"\"\" ???+ note \"Transform an array with a already-fitted reducer.\" | Param | Type | Description | | :--------- | :----------- | :----------------------- | | `array` | `np.ndarray` | the array to transform | | `method` | `str` | `\"umap\"` or `\"ivis\"` | \"\"\" assert method in [ \"umap\" , \"ivis\" ], self . method_error_msg assert isinstance ( array , np . ndarray ), f \"Expected np.ndarray, got { type ( array ) } \" # edge case: array is too small if array . shape [ 0 ] < 1 : return np . array ([]) reducer = getattr ( self , method ) return reducer . transform ( array )","title":"transform()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction","text":"","title":"reduction"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer","text":"","title":"DimensionalityReducer"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.__init__","text":"","title":"__init__()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.fit_transform","text":"","title":"fit_transform()"},{"location":"pages/reference/core-representation/#hover.core.representation.reduction.DimensionalityReducer.transform","text":"","title":"transform()"},{"location":"pages/reference/recipes/","text":"hover.recipes hover.recipes.stable High-level functions to produce an interactive annotation interface. Stable recipes whose function signatures should almost never change in the future. linked_annotator ( dataset , ** kwargs ) Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout simple_annotator ( dataset , ** kwargs ) Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout hover.recipes.experimental High-level functions to produce an interactive annotation interface. Experimental recipes whose function signatures might change significantly in the future. Use with caution. active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ) Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vectorizer callable the feature -> vector function vecnet_callback callable the (dataset, vectorizer) -> VecNet function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search -> highlight Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vectorizer` | `callable` | the feature -> vector function | | `vecnet_callback` | `callable` | the (dataset, vectorizer) -> `VecNet` function| | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search -> highlight | \"\"\" layout , _ = _active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ) return layout snorkel_crosscheck ( dataset , lf_list , ** kwargs ) Display the dataset for annotation, cross-checking with labeling functions. Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. Param Type Description dataset SupervisableDataset the dataset to link to lf_list list a list of callables decorated by @hover.utils.snorkel_helper.labeling_function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSnorkelExplorer BokehDataAnnotator manage data subsets inspect labeling functions make annotations Source code in hover/recipes/experimental.py @servable ( title = \"Snorkel Crosscheck\" ) def snorkel_crosscheck ( dataset , lf_list , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, cross-checking with labeling functions.\" Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `lf_list` | `list` | a list of callables decorated by `@hover.utils.snorkel_helper.labeling_function` | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSnorkelExplorer | BokehDataAnnotator | | :------------------ | :------------------------- | :----------------- | | manage data subsets | inspect labeling functions | make annotations | \"\"\" layout , _ = _snorkel_crosscheck ( dataset , lf_list , ** kwargs ) return layout hover.recipes.subroutine Building blocks of high-level recipes. Includes the following: functions for creating individual standard explorers appropriate for a dataset. get_explorer_class ( task , feature ) Get the right hover.core.explorer class given a task and a feature. Can be useful for dynamically creating explorers without knowing the feature in advance. Param Type Description task str name of the task, which can be \"finder\" , \"annotator\" , \"margin\" , \"softlabel\" , or \"snorkel\" feature str name of the main feature, which can be \"text\" , \"audio\" or \"image\" Usage: # this creates an instance of BokehTextFinder explorer = get_explorer_class ( \"finder\" , \"text\" )( * args , ** kwargs ) Source code in hover/recipes/subroutine.py def get_explorer_class ( task , feature ): \"\"\" ???+ note \"Get the right `hover.core.explorer` class given a task and a feature.\" Can be useful for dynamically creating explorers without knowing the feature in advance. | Param | Type | Description | | :-------- | :---- | :----------------------------------- | | `task` | `str` | name of the task, which can be `\"finder\"`, `\"annotator\"`, `\"margin\"`, `\"softlabel\"`, or `\"snorkel\"` | | `feature` | `str` | name of the main feature, which can be `\"text\"`, `\"audio\"` or `\"image\"` | Usage: ```python # this creates an instance of BokehTextFinder explorer = get_explorer_class(\"finder\", \"text\")(*args, **kwargs) ``` \"\"\" assert task in EXPLORER_CATALOG , f \"Invalid task: { task } \" assert feature in EXPLORER_CATALOG [ task ], f \"Invalid feature: { feature } \" return EXPLORER_CATALOG [ task ][ feature ] standard_annotator ( dataset , ** kwargs ) Set up a BokehDataAnnotator for a SupervisableDataset . The annotator has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset can commit annotations through selections in the \"raw\" subset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataAnnotator Source code in hover/recipes/subroutine.py def standard_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataAnnotator` for a `SupervisableDataset`.\" The annotator has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset - can commit annotations through selections in the \"raw\" subset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataAnnotator` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"annotator\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () annotator = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Annotator: apply labels to the selected points\" , ** kwargs , ) annotator . plot () # subscribe for df updates dataset . subscribe_update_push ( annotator , { _k : _k for _k in subsets }) # annotators can commit to a dataset dataset . subscribe_data_commit ( annotator , { \"raw\" : \"raw\" }) # annotators by default link the selection for preview dataset . subscribe_selection_view ( annotator , [ \"raw\" , \"train\" , \"dev\" , \"test\" ]) return annotator standard_finder ( dataset , ** kwargs ) Set up a BokehDataFinder for a SupervisableDataset . The finder has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataFinder Source code in hover/recipes/subroutine.py def standard_finder ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataFinder` for a `SupervisableDataset`.\" The finder has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataFinder` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"finder\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () finder = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Finder: use the search widget for highlights\" , ** kwargs , ) finder . plot () # subscribe for df updates dataset . subscribe_update_push ( finder , { _k : _k for _k in subsets }) return finder standard_snorkel ( dataset , ** kwargs ) Set up a BokehSnorkelExplorer for a SupervisableDataset . The snorkel explorer has a few standard interactions with the dataset: read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" subscribe to all updates in those subsets Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehSnorkelExplorer Source code in hover/recipes/subroutine.py def standard_snorkel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSnorkelExplorer` for a `SupervisableDataset`.\" The snorkel explorer has a few standard interactions with the dataset: - read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" - subscribe to all updates in those subsets | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehSnorkelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"snorkel\" , feature ) # first \"static\" version of the plot snorkel = explorer_cls . from_dataset ( dataset , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }, title = \"Snorkel: square for correct, x for incorrect, + for missed, o for hit; click on legends to hide or show LF\" , ** kwargs , ) snorkel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( snorkel , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }) return snorkel standard_softlabel ( dataset , ** kwargs ) Set up a BokehSoftLabelExplorer for a SupervisableDataset . The soft label explorer has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehSoftLabelExplorer Source code in hover/recipes/subroutine.py def standard_softlabel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSoftLabelExplorer` for a `SupervisableDataset`.\" The soft label explorer has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehSoftLabelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"softlabel\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () softlabel = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, \"pred_label\" , \"pred_score\" , title = \"SoftLabel: inspect predictions and scores\" , ** kwargs , ) softlabel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( softlabel , { _k : _k for _k in subsets }) return softlabel","title":"hover.recipes"},{"location":"pages/reference/recipes/#hoverrecipes","text":"","title":"hover.recipes"},{"location":"pages/reference/recipes/#hover.recipes.stable","text":"High-level functions to produce an interactive annotation interface. Stable recipes whose function signatures should almost never change in the future.","title":"stable"},{"location":"pages/reference/recipes/#hover.recipes.stable.linked_annotator","text":"Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout","title":"linked_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable.simple_annotator","text":"Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout","title":"simple_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable","text":"","title":"stable"},{"location":"pages/reference/recipes/#hover.recipes.stable.linked_annotator","text":"","title":"linked_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.stable.simple_annotator","text":"","title":"simple_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.experimental","text":"High-level functions to produce an interactive annotation interface. Experimental recipes whose function signatures might change significantly in the future. Use with caution.","title":"experimental"},{"location":"pages/reference/recipes/#hover.recipes.experimental.active_learning","text":"Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vectorizer callable the feature -> vector function vecnet_callback callable the (dataset, vectorizer) -> VecNet function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search -> highlight Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vectorizer` | `callable` | the feature -> vector function | | `vecnet_callback` | `callable` | the (dataset, vectorizer) -> `VecNet` function| | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search -> highlight | \"\"\" layout , _ = _active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ) return layout","title":"active_learning()"},{"location":"pages/reference/recipes/#hover.recipes.experimental.snorkel_crosscheck","text":"Display the dataset for annotation, cross-checking with labeling functions. Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. Param Type Description dataset SupervisableDataset the dataset to link to lf_list list a list of callables decorated by @hover.utils.snorkel_helper.labeling_function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSnorkelExplorer BokehDataAnnotator manage data subsets inspect labeling functions make annotations Source code in hover/recipes/experimental.py @servable ( title = \"Snorkel Crosscheck\" ) def snorkel_crosscheck ( dataset , lf_list , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, cross-checking with labeling functions.\" Use the dev set to check labeling functions; use the labeling functions to hint at potential annotation. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `lf_list` | `list` | a list of callables decorated by `@hover.utils.snorkel_helper.labeling_function` | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSnorkelExplorer | BokehDataAnnotator | | :------------------ | :------------------------- | :----------------- | | manage data subsets | inspect labeling functions | make annotations | \"\"\" layout , _ = _snorkel_crosscheck ( dataset , lf_list , ** kwargs ) return layout","title":"snorkel_crosscheck()"},{"location":"pages/reference/recipes/#hover.recipes.experimental","text":"","title":"experimental"},{"location":"pages/reference/recipes/#hover.recipes.experimental.active_learning","text":"","title":"active_learning()"},{"location":"pages/reference/recipes/#hover.recipes.experimental.snorkel_crosscheck","text":"","title":"snorkel_crosscheck()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine","text":"Building blocks of high-level recipes. Includes the following: functions for creating individual standard explorers appropriate for a dataset.","title":"subroutine"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.get_explorer_class","text":"Get the right hover.core.explorer class given a task and a feature. Can be useful for dynamically creating explorers without knowing the feature in advance. Param Type Description task str name of the task, which can be \"finder\" , \"annotator\" , \"margin\" , \"softlabel\" , or \"snorkel\" feature str name of the main feature, which can be \"text\" , \"audio\" or \"image\" Usage: # this creates an instance of BokehTextFinder explorer = get_explorer_class ( \"finder\" , \"text\" )( * args , ** kwargs ) Source code in hover/recipes/subroutine.py def get_explorer_class ( task , feature ): \"\"\" ???+ note \"Get the right `hover.core.explorer` class given a task and a feature.\" Can be useful for dynamically creating explorers without knowing the feature in advance. | Param | Type | Description | | :-------- | :---- | :----------------------------------- | | `task` | `str` | name of the task, which can be `\"finder\"`, `\"annotator\"`, `\"margin\"`, `\"softlabel\"`, or `\"snorkel\"` | | `feature` | `str` | name of the main feature, which can be `\"text\"`, `\"audio\"` or `\"image\"` | Usage: ```python # this creates an instance of BokehTextFinder explorer = get_explorer_class(\"finder\", \"text\")(*args, **kwargs) ``` \"\"\" assert task in EXPLORER_CATALOG , f \"Invalid task: { task } \" assert feature in EXPLORER_CATALOG [ task ], f \"Invalid feature: { feature } \" return EXPLORER_CATALOG [ task ][ feature ]","title":"get_explorer_class()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_annotator","text":"Set up a BokehDataAnnotator for a SupervisableDataset . The annotator has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset can commit annotations through selections in the \"raw\" subset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataAnnotator Source code in hover/recipes/subroutine.py def standard_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataAnnotator` for a `SupervisableDataset`.\" The annotator has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset - can commit annotations through selections in the \"raw\" subset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataAnnotator` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"annotator\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () annotator = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Annotator: apply labels to the selected points\" , ** kwargs , ) annotator . plot () # subscribe for df updates dataset . subscribe_update_push ( annotator , { _k : _k for _k in subsets }) # annotators can commit to a dataset dataset . subscribe_data_commit ( annotator , { \"raw\" : \"raw\" }) # annotators by default link the selection for preview dataset . subscribe_selection_view ( annotator , [ \"raw\" , \"train\" , \"dev\" , \"test\" ]) return annotator","title":"standard_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_finder","text":"Set up a BokehDataFinder for a SupervisableDataset . The finder has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehDataFinder Source code in hover/recipes/subroutine.py def standard_finder ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehDataFinder` for a `SupervisableDataset`.\" The finder has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehDataFinder` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"finder\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () finder = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, title = \"Finder: use the search widget for highlights\" , ** kwargs , ) finder . plot () # subscribe for df updates dataset . subscribe_update_push ( finder , { _k : _k for _k in subsets }) return finder","title":"standard_finder()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_snorkel","text":"Set up a BokehSnorkelExplorer for a SupervisableDataset . The snorkel explorer has a few standard interactions with the dataset: read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" subscribe to all updates in those subsets Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehSnorkelExplorer Source code in hover/recipes/subroutine.py def standard_snorkel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSnorkelExplorer` for a `SupervisableDataset`.\" The snorkel explorer has a few standard interactions with the dataset: - read \"raw\" and \"dev\" subsets of the dataset, interpreting \"dev\" as \"labeled\" - subscribe to all updates in those subsets | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehSnorkelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"snorkel\" , feature ) # first \"static\" version of the plot snorkel = explorer_cls . from_dataset ( dataset , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }, title = \"Snorkel: square for correct, x for incorrect, + for missed, o for hit; click on legends to hide or show LF\" , ** kwargs , ) snorkel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( snorkel , { \"raw\" : \"raw\" , \"dev\" : \"labeled\" }) return snorkel","title":"standard_snorkel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_softlabel","text":"Set up a BokehSoftLabelExplorer for a SupervisableDataset . The soft label explorer has a few standard interactions with the dataset: read all subsets of the dataset subscribe to all updates in the dataset Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to the BokehSoftLabelExplorer Source code in hover/recipes/subroutine.py def standard_softlabel ( dataset , ** kwargs ): \"\"\" ???+ note \"Set up a `BokehSoftLabelExplorer` for a `SupervisableDataset`.\" The soft label explorer has a few standard interactions with the dataset: - read all subsets of the dataset - subscribe to all updates in the dataset | Param | Type | Description | | :--------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to the `BokehSoftLabelExplorer` | \"\"\" # auto-detect the (main) feature to use feature = dataset . __class__ . FEATURE_KEY explorer_cls = get_explorer_class ( \"softlabel\" , feature ) # first \"static\" version of the plot subsets = explorer_cls . SUBSET_GLYPH_KWARGS . keys () softlabel = explorer_cls . from_dataset ( dataset , { _k : _k for _k in subsets }, \"pred_label\" , \"pred_score\" , title = \"SoftLabel: inspect predictions and scores\" , ** kwargs , ) softlabel . plot () # subscribe to dataset widgets dataset . subscribe_update_push ( softlabel , { _k : _k for _k in subsets }) return softlabel","title":"standard_softlabel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine","text":"","title":"subroutine"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.get_explorer_class","text":"","title":"get_explorer_class()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_annotator","text":"","title":"standard_annotator()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_finder","text":"","title":"standard_finder()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_snorkel","text":"","title":"standard_snorkel()"},{"location":"pages/reference/recipes/#hover.recipes.subroutine.standard_softlabel","text":"","title":"standard_softlabel()"},{"location":"pages/reference/utils-bokeh_helper/","text":"Useful subroutines for working with bokeh in general. auto_label_color ( labels ) Create a label->hex color mapping dict. Source code in hover/utils/bokeh_helper.py def auto_label_color ( labels ): \"\"\" ???+ note \"Create a label->hex color mapping dict.\" \"\"\" use_labels = set ( labels ) use_labels . discard ( module_config . ABSTAIN_DECODED ) use_labels = sorted ( use_labels , reverse = False ) assert len ( use_labels ) <= 20 , \"Too many labels to support (max at 20)\" palette = Category10 [ 10 ] if len ( use_labels ) <= 10 else Category20 [ 20 ] color_dict = { module_config . ABSTAIN_DECODED : \"#dcdcdc\" , # gainsboro hex code ** { _l : _c for _l , _c in zip ( use_labels , palette )}, } return color_dict bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None ) Create a Bokeh hover tooltip from a template. param label: whether to expect and show a \"label\" field. param text: whether to expect and show a \"text\" field. param image: whether to expect and show an \"image\" (url/path) field. param audio: whether to expect and show an \"audio\" (url/path) field. param coords: whether to show xy-coordinates. param index: whether to show indices in the dataset. param custom: {display: column} mapping of additional (text) tooltips. Source code in hover/utils/bokeh_helper.py def bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None , ): \"\"\" ???+ note \"Create a Bokeh hover tooltip from a template.\" - param label: whether to expect and show a \"label\" field. - param text: whether to expect and show a \"text\" field. - param image: whether to expect and show an \"image\" (url/path) field. - param audio: whether to expect and show an \"audio\" (url/path) field. - param coords: whether to show xy-coordinates. - param index: whether to show indices in the dataset. - param custom: {display: column} mapping of additional (text) tooltips. \"\"\" # initialize mutable default value custom = custom or dict () # prepare encapsulation of a div box and an associated script divbox_prefix = \"\"\"<div class=\"out tooltip\"> \\n \"\"\" divbox_suffix = \"\"\"</div> \\n \"\"\" script_prefix = \"\"\"<script> \\n \"\"\" script_suffix = \"\"\"</script> \\n \"\"\" # dynamically add contents to the div box and the script divbox = divbox_prefix script = script_prefix if label : divbox += \"\"\" <div> <span style=\"font-size: 16px; color: #966;\"> Label: @label </span> </div> \"\"\" if text : divbox += \"\"\" <div style=\"word-wrap: break-word; width: 95%; text-overflow: ellipsis; line-height: 90%\"> <span style=\"font-size: 11px;\"> Text: @text </span> </div> \"\"\" if image : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Image: @image </span> <img src=\"@image\" height=\"60\" alt=\"@image\" width=\"60\" style=\"float: left; margin: 0px 0px 0px 0px;\" border=\"2\" ></img> </div> \"\"\" if audio : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Audio: @audio </span> <audio autoplay preload=\"auto\" src=\"@audio\"> </audio> </div> \"\"\" if coords : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #060;\"> Coordinates: ($x, $y) </span> </div> \"\"\" if index : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #066;\"> Index: [$index] </span> </div> \"\"\" for _key , _field in custom . items (): divbox += f \"\"\" <div> <span style=\"font-size: 12px; color: #606;\"> { _key } : @ { _field } </span> </div> \"\"\" divbox += divbox_suffix script += script_suffix return divbox + script remote_jupyter_proxy_url ( port , base_url ) Callable to configure Bokeh's show method when using a proxy (JupyterHub). Usage: # show(plot) show ( plot , notebook_url = remote_jupyter_proxy_url ) Source code in hover/utils/bokeh_helper.py def remote_jupyter_proxy_url ( port , base_url ): \"\"\" ???+ note \"Callable to configure Bokeh's show method when using a proxy (JupyterHub).\" Usage: ```python # show(plot) show(plot, notebook_url=remote_jupyter_proxy_url) ``` \"\"\" # find JupyterHub base (external) url, default to Binder base_url = os . environ . get ( \"JUPYTERHUB_BASE_URL\" , \"https://hub.gke2.mybinder.org/user/\" ) host = urllib . parse . urlparse ( base_url ) . netloc if port is None : return host service_url_path = os . environ . get ( \"JUPYTERHUB_SERVICE_PREFIX\" , \"\" ) proxy_url_path = \"proxy/ %d \" % port user_url = urllib . parse . urljoin ( base_url , service_url_path ) full_url = urllib . parse . urljoin ( user_url , proxy_url_path ) return full_url servable ( title = None ) Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh. Usage: First wrap a function that creates bokeh plot elements: @servable () def dummy ( * args , ** kwargs ): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator ( * args , ** kwargs ) annotator . plot () return annotator . view () Then serve the app in your preferred setting: inline # in a Jupyter cell from bokeh.io import show , output_notebook output_notebook () show ( dummy ( * args , ** kwargs )) bokeh serve # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc () dummy ( * args , ** kwargs )( doc ) embedded app # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app' : dummy ( * args , ** kwargs ), 'my-other-app' : dummy ( * args , ** kwargs ), } server = Server ( app_dict ) server . start () Source code in hover/utils/bokeh_helper.py def servable ( title = None ): \"\"\" ???+ note \"Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh.\" Usage: First wrap a function that creates bokeh plot elements: ```python @servable() def dummy(*args, **kwargs): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator(*args, **kwargs) annotator.plot() return annotator.view() ``` Then serve the app in your preferred setting: === \"inline\" ```python # in a Jupyter cell from bokeh.io import show, output_notebook output_notebook() show(dummy(*args, **kwargs)) ``` === \"bokeh serve\" ```python # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc() dummy(*args, **kwargs)(doc) ``` === \"embedded app\" ```python # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app': dummy(*args, **kwargs), 'my-other-app': dummy(*args, **kwargs), } server = Server(app_dict) server.start() ``` \"\"\" def wrapper ( func ): @wraps ( func ) def wrapped ( * args , ** kwargs ): def handle ( doc ): \"\"\" Note that the handle must create a brand new bokeh model every time it is called. Reference: https://github.com/bokeh/bokeh/issues/8579 \"\"\" spinner = PreText ( text = \"loading...\" ) layout = column ( spinner ) def progress (): spinner . text += \".\" def load (): try : bokeh_model = func ( * args , ** kwargs ) # remove spinner and its update try : doc . remove_periodic_callback ( progress ) except Exception as e : warnings . warn ( f \"@servable: trying to remove periodic callback, got { type ( e ) } : { e } \" ) layout . children . append ( bokeh_model ) layout . children . pop ( 0 ) except Exception as e : # exception handling message = PreText ( text = f \" { type ( e ) } : { e } \\n { format_exc () } \" ) layout . children . append ( message ) doc . add_root ( layout ) doc . add_periodic_callback ( progress , 5000 ) doc . add_timeout_callback ( load , 500 ) doc . title = title or func . __name__ return handle return wrapped return wrapper","title":"hover.utils.bokeh_helper"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.auto_label_color","text":"Create a label->hex color mapping dict. Source code in hover/utils/bokeh_helper.py def auto_label_color ( labels ): \"\"\" ???+ note \"Create a label->hex color mapping dict.\" \"\"\" use_labels = set ( labels ) use_labels . discard ( module_config . ABSTAIN_DECODED ) use_labels = sorted ( use_labels , reverse = False ) assert len ( use_labels ) <= 20 , \"Too many labels to support (max at 20)\" palette = Category10 [ 10 ] if len ( use_labels ) <= 10 else Category20 [ 20 ] color_dict = { module_config . ABSTAIN_DECODED : \"#dcdcdc\" , # gainsboro hex code ** { _l : _c for _l , _c in zip ( use_labels , palette )}, } return color_dict","title":"auto_label_color()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.bokeh_hover_tooltip","text":"Create a Bokeh hover tooltip from a template. param label: whether to expect and show a \"label\" field. param text: whether to expect and show a \"text\" field. param image: whether to expect and show an \"image\" (url/path) field. param audio: whether to expect and show an \"audio\" (url/path) field. param coords: whether to show xy-coordinates. param index: whether to show indices in the dataset. param custom: {display: column} mapping of additional (text) tooltips. Source code in hover/utils/bokeh_helper.py def bokeh_hover_tooltip ( label = False , text = False , image = False , audio = False , coords = True , index = True , custom = None , ): \"\"\" ???+ note \"Create a Bokeh hover tooltip from a template.\" - param label: whether to expect and show a \"label\" field. - param text: whether to expect and show a \"text\" field. - param image: whether to expect and show an \"image\" (url/path) field. - param audio: whether to expect and show an \"audio\" (url/path) field. - param coords: whether to show xy-coordinates. - param index: whether to show indices in the dataset. - param custom: {display: column} mapping of additional (text) tooltips. \"\"\" # initialize mutable default value custom = custom or dict () # prepare encapsulation of a div box and an associated script divbox_prefix = \"\"\"<div class=\"out tooltip\"> \\n \"\"\" divbox_suffix = \"\"\"</div> \\n \"\"\" script_prefix = \"\"\"<script> \\n \"\"\" script_suffix = \"\"\"</script> \\n \"\"\" # dynamically add contents to the div box and the script divbox = divbox_prefix script = script_prefix if label : divbox += \"\"\" <div> <span style=\"font-size: 16px; color: #966;\"> Label: @label </span> </div> \"\"\" if text : divbox += \"\"\" <div style=\"word-wrap: break-word; width: 95%; text-overflow: ellipsis; line-height: 90%\"> <span style=\"font-size: 11px;\"> Text: @text </span> </div> \"\"\" if image : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Image: @image </span> <img src=\"@image\" height=\"60\" alt=\"@image\" width=\"60\" style=\"float: left; margin: 0px 0px 0px 0px;\" border=\"2\" ></img> </div> \"\"\" if audio : divbox += \"\"\" <div> <span style=\"font-size: 10px;\"> Audio: @audio </span> <audio autoplay preload=\"auto\" src=\"@audio\"> </audio> </div> \"\"\" if coords : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #060;\"> Coordinates: ($x, $y) </span> </div> \"\"\" if index : divbox += \"\"\" <div> <span style=\"font-size: 12px; color: #066;\"> Index: [$index] </span> </div> \"\"\" for _key , _field in custom . items (): divbox += f \"\"\" <div> <span style=\"font-size: 12px; color: #606;\"> { _key } : @ { _field } </span> </div> \"\"\" divbox += divbox_suffix script += script_suffix return divbox + script","title":"bokeh_hover_tooltip()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.remote_jupyter_proxy_url","text":"Callable to configure Bokeh's show method when using a proxy (JupyterHub). Usage: # show(plot) show ( plot , notebook_url = remote_jupyter_proxy_url ) Source code in hover/utils/bokeh_helper.py def remote_jupyter_proxy_url ( port , base_url ): \"\"\" ???+ note \"Callable to configure Bokeh's show method when using a proxy (JupyterHub).\" Usage: ```python # show(plot) show(plot, notebook_url=remote_jupyter_proxy_url) ``` \"\"\" # find JupyterHub base (external) url, default to Binder base_url = os . environ . get ( \"JUPYTERHUB_BASE_URL\" , \"https://hub.gke2.mybinder.org/user/\" ) host = urllib . parse . urlparse ( base_url ) . netloc if port is None : return host service_url_path = os . environ . get ( \"JUPYTERHUB_SERVICE_PREFIX\" , \"\" ) proxy_url_path = \"proxy/ %d \" % port user_url = urllib . parse . urljoin ( base_url , service_url_path ) full_url = urllib . parse . urljoin ( user_url , proxy_url_path ) return full_url","title":"remote_jupyter_proxy_url()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.servable","text":"Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh. Usage: First wrap a function that creates bokeh plot elements: @servable () def dummy ( * args , ** kwargs ): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator ( * args , ** kwargs ) annotator . plot () return annotator . view () Then serve the app in your preferred setting: inline # in a Jupyter cell from bokeh.io import show , output_notebook output_notebook () show ( dummy ( * args , ** kwargs )) bokeh serve # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc () dummy ( * args , ** kwargs )( doc ) embedded app # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app' : dummy ( * args , ** kwargs ), 'my-other-app' : dummy ( * args , ** kwargs ), } server = Server ( app_dict ) server . start () Source code in hover/utils/bokeh_helper.py def servable ( title = None ): \"\"\" ???+ note \"Create a decorator which returns an app (or \"handle\" function) to be passed to bokeh.\" Usage: First wrap a function that creates bokeh plot elements: ```python @servable() def dummy(*args, **kwargs): from hover.core.explorer import BokehCorpusAnnotator annotator = BokehCorpusAnnotator(*args, **kwargs) annotator.plot() return annotator.view() ``` Then serve the app in your preferred setting: === \"inline\" ```python # in a Jupyter cell from bokeh.io import show, output_notebook output_notebook() show(dummy(*args, **kwargs)) ``` === \"bokeh serve\" ```python # in <your-bokeh-app-dir>/main.py from bokeh.io import curdoc doc = curdoc() dummy(*args, **kwargs)(doc) ``` === \"embedded app\" ```python # anywhere in your use case from bokeh.server.server import Server app_dict = { 'my-app': dummy(*args, **kwargs), 'my-other-app': dummy(*args, **kwargs), } server = Server(app_dict) server.start() ``` \"\"\" def wrapper ( func ): @wraps ( func ) def wrapped ( * args , ** kwargs ): def handle ( doc ): \"\"\" Note that the handle must create a brand new bokeh model every time it is called. Reference: https://github.com/bokeh/bokeh/issues/8579 \"\"\" spinner = PreText ( text = \"loading...\" ) layout = column ( spinner ) def progress (): spinner . text += \".\" def load (): try : bokeh_model = func ( * args , ** kwargs ) # remove spinner and its update try : doc . remove_periodic_callback ( progress ) except Exception as e : warnings . warn ( f \"@servable: trying to remove periodic callback, got { type ( e ) } : { e } \" ) layout . children . append ( bokeh_model ) layout . children . pop ( 0 ) except Exception as e : # exception handling message = PreText ( text = f \" { type ( e ) } : { e } \\n { format_exc () } \" ) layout . children . append ( message ) doc . add_root ( layout ) doc . add_periodic_callback ( progress , 5000 ) doc . add_timeout_callback ( load , 500 ) doc . title = title or func . __name__ return handle return wrapped return wrapper","title":"servable()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.auto_label_color","text":"","title":"auto_label_color()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.bokeh_hover_tooltip","text":"","title":"bokeh_hover_tooltip()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.remote_jupyter_proxy_url","text":"","title":"remote_jupyter_proxy_url()"},{"location":"pages/reference/utils-bokeh_helper/#hover.utils.bokeh_helper.servable","text":"","title":"servable()"},{"location":"pages/reference/utils-snorkel_helper/","text":"labeling_function ( targets , label_encoder = None , ** kwargs ) Hover's flavor of the Snorkel labeling_function decorator. However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. assigns a UUID for easy identification keeps track of LF targets Param Type Description targets list of str labels that the labeling function is intended to create label_encoder dict {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a .snorkel attribute **kwargs forwarded to snorkel 's labeling_function() Source code in hover/utils/snorkel_helper.py def labeling_function ( targets , label_encoder = None , ** kwargs ): \"\"\" ???+ note \"Hover's flavor of the Snorkel labeling_function decorator.\" However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. - assigns a UUID for easy identification - keeps track of LF targets | Param | Type | Description | | :-------------- | :----- | :----------------------------------- | | `targets` | `list` of `str` | labels that the labeling function is intended to create | | `label_encoder` | `dict` | {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a `.snorkel` attribute | | `**kwargs` | | forwarded to `snorkel`'s `labeling_function()` | \"\"\" # lazy import of optional dependency from snorkel.labeling import ( labeling_function as snorkel_lf , LabelingFunction as SnorkelLF , ) def wrapper ( func ): # set up kwargs for Snorkel's LF snorkel_kwargs = { \"name\" : func . __name__ } snorkel_kwargs . update ( kwargs ) # return value of hover's decorator lf = SnorkelLF ( f = func , ** snorkel_kwargs ) # additional attributes lf . uuid = uuid . uuid1 () lf . targets = targets [:] # link a snorkel-style labeling function if applicable if label_encoder : lf . label_encoder = label_encoder def snorkel_style_func ( x ): return lf . label_encoder [ func ( x )] lf . snorkel = snorkel_lf ( ** kwargs )( snorkel_style_func ) else : lf . label_encoder = None lf . snorkel = None return lf return wrapper","title":"hover.utils.snorkel_helper"},{"location":"pages/reference/utils-snorkel_helper/#hover.utils.snorkel_helper.labeling_function","text":"Hover's flavor of the Snorkel labeling_function decorator. However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. assigns a UUID for easy identification keeps track of LF targets Param Type Description targets list of str labels that the labeling function is intended to create label_encoder dict {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a .snorkel attribute **kwargs forwarded to snorkel 's labeling_function() Source code in hover/utils/snorkel_helper.py def labeling_function ( targets , label_encoder = None , ** kwargs ): \"\"\" ???+ note \"Hover's flavor of the Snorkel labeling_function decorator.\" However, due to the dynamic label encoding nature of hover, the decorated function should return the original string label, not its encoding integer. - assigns a UUID for easy identification - keeps track of LF targets | Param | Type | Description | | :-------------- | :----- | :----------------------------------- | | `targets` | `list` of `str` | labels that the labeling function is intended to create | | `label_encoder` | `dict` | {decoded_label -> encoded_label} mapping, if you also want an original snorkel-style labeling function linked as a `.snorkel` attribute | | `**kwargs` | | forwarded to `snorkel`'s `labeling_function()` | \"\"\" # lazy import of optional dependency from snorkel.labeling import ( labeling_function as snorkel_lf , LabelingFunction as SnorkelLF , ) def wrapper ( func ): # set up kwargs for Snorkel's LF snorkel_kwargs = { \"name\" : func . __name__ } snorkel_kwargs . update ( kwargs ) # return value of hover's decorator lf = SnorkelLF ( f = func , ** snorkel_kwargs ) # additional attributes lf . uuid = uuid . uuid1 () lf . targets = targets [:] # link a snorkel-style labeling function if applicable if label_encoder : lf . label_encoder = label_encoder def snorkel_style_func ( x ): return lf . label_encoder [ func ( x )] lf . snorkel = snorkel_lf ( ** kwargs )( snorkel_style_func ) else : lf . label_encoder = None lf . snorkel = None return lf return wrapper","title":"labeling_function()"},{"location":"pages/reference/utils-snorkel_helper/#hover.utils.snorkel_helper.labeling_function","text":"","title":"labeling_function()"},{"location":"pages/topics/active-learning/","text":"","title":"Active learning"},{"location":"pages/topics/distant-supervision/","text":"","title":"Distant supervision"},{"location":"pages/topics/managing-data/","text":"hover manages data through a SupervisableDataset class. Here we walk through some basic behaviors and interactions that can turn out useful. @import url(\"../../../styles/monokai.css\"); from hover.core.dataset import SupervisableTextDataset # ---- simplistic data for illustation ---- my_data = { \"raw\": [ {\"text\": \"Avocados are my favorite!\"}, {\"text\": \"Blueberries are not bad either.\"}, {\"text\": \"Citrus ... sure why not\"}, ], \"train\": [ {\"text\": \"Citrus ... sure why not\", \"label\": \"C\"}, {\"text\": \"Dragonfruits cost too much\", \"label\": \"D\"}, ], \"dev\": [ {\"text\": \"Dragonfruits cost too much\", \"label\": \"D\"}, {\"text\": \"Eggplants? Not in this scope.\", \"label\": \"E\"}, ], \"test\": [ {\"text\": \"Eggplants? Not in this scope.\", \"label\": \"E\"}, ], } # ----------------------------------- dataset = SupervisableTextDataset( raw_dictl=my_data[\"raw\"], train_dictl=my_data[\"train\"], dev_dictl=my_data[\"dev\"], test_dictl=my_data[\"test\"], # \"text\" is the default feature field for SupervisableTextDataset # \"label\" is the default label field for SupervisableDataset ) # Be aware of the automatic deduplication by feature # which keeps test > dev > train > raw dataset.dfs { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, }","title":"Managing Data (under work)"},{"location":"pages/topics/multiple-views/","text":"","title":"Multiple views"},{"location":"pages/topics/what-hover-is/","text":"Features Here we attempt a quick comparison with a few other packages that do machine teaching: Package Hover Prodigy Snorkel Core idea batch annotation with extensions scriptable active learning programmatic distant supervision Annotates per batch of just the size you find right piece predicted to be the most valuable the whole dataset as long as it fits in Supports all classification (text only atm) text & images, audio, vidio, & more text classification (for the most part) Status open-source proprietary open-source Devs indie Explosion AI Stanford / Snorkel AI Related many imports of the awesome Bokeh builds on the Thinc / SpaCy stack variants: Snorkel Drybell , MeTaL , DeepDive Vanilla usage define a vectorizer and annotate away choose a base model and annotate away define labeling functions and apply away Advanced usage combine w/ active learning & snorkel patterns / transformers / custom models transforming / slicing functions Hardcore usage exploit hover.core templates custom @prodigy.recipe the upcoming Snorkel Flow Hover claims the best deal of scale vs. precision thanks to the flexibility to use, or not use, any technique beyond annotating on a \"map\"; the speed, or coarseness, of annotation being literally at your fingertips ; the interaction between multiple \"maps\" that each serves a different but connected purpose.","title":"What hover is"},{"location":"pages/topics/what-hover-is/#features","text":"Here we attempt a quick comparison with a few other packages that do machine teaching: Package Hover Prodigy Snorkel Core idea batch annotation with extensions scriptable active learning programmatic distant supervision Annotates per batch of just the size you find right piece predicted to be the most valuable the whole dataset as long as it fits in Supports all classification (text only atm) text & images, audio, vidio, & more text classification (for the most part) Status open-source proprietary open-source Devs indie Explosion AI Stanford / Snorkel AI Related many imports of the awesome Bokeh builds on the Thinc / SpaCy stack variants: Snorkel Drybell , MeTaL , DeepDive Vanilla usage define a vectorizer and annotate away choose a base model and annotate away define labeling functions and apply away Advanced usage combine w/ active learning & snorkel patterns / transformers / custom models transforming / slicing functions Hardcore usage exploit hover.core templates custom @prodigy.recipe the upcoming Snorkel Flow Hover claims the best deal of scale vs. precision thanks to the flexibility to use, or not use, any technique beyond annotating on a \"map\"; the speed, or coarseness, of annotation being literally at your fingertips ; the interaction between multiple \"maps\" that each serves a different but connected purpose.","title":"Features"},{"location":"pages/tutorial/t0-quickstart/","text":"Welcome to the basic use case of hover ! Let's say we want to label some data and call it a day. { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Running Python right here Think of this page as a Jupyter notebook. You can edit code and press Shift+Enter to execute. Behind the scene is a Binder -hosted Python environment. Below is the status of the kernel: Ingredient 1 / 3: Raw Data Start with a spreadsheet loaded in pandas . We turn it into a SupervisableDataset designed for labeling: from hover.core.dataset import SupervisableTextDataset import pandas as pd example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" # for fast demonstration purpose, sample the data df_raw = pd.read_csv(example_csv_path).sample(2000) # data is divided into 4 subsets: \"raw\" / \"train\" / \"dev\" / \"test\" # this example assumes no labeled data available., i.e. only \"raw\" df_raw[\"SUBSET\"] = \"raw\" # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df_raw, feature_key=\"text\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) FAQ What if I have multiple features? feature_key refers to the field that will be vectorized later on, which can be a JSON that encloses multiple features. For example, suppose our data entries look like this: { \"f1\" : \"foo\" , \"f2\" : \"bar\" , \"non_feature\" : \"abc\" } We can put f1 and f2 in a JSON and convert the entries like this: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Can I use audio or image data? In the not-too-far future, yes! Some mechanisms can get tricky with audios/images, but we are working on it: display tooltips: as of 0.4.0, tooltips are supported in the low-level APIs . search and highlight : pending (and open to contributions!) high-level API like SupervisableImageDataset : pending Ingredient 2 / 3: Embedding A pre-trained embedding lets us group data points semantically. In particular, let's define a data -> embedding vector function. import spacy import re # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") Tips Caching dataset by itself stores the original features but not the corresponding vectors. To avoid vectorizing the same feature again and again, we could simply do: from functools import cache @cache def vectorizer ( feature ): # put code here If you'd like to limit the size of the cache, something like @lru_cache(maxsize=10000) could help. Check out functools for more options. Vectorizing multiple features Suppose we have multiple features enclosed in a JSON: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Also, suppose we have individual vectorizers likes this: def vectorizer_1 ( feature_1 ): # put code here def vectorizer_2 ( feature_2 ): # put code here Then we can define a composite vectorizer: import json import numpy as np def vectorizer ( feature_json ): data_dict = json . loads ( feature_json ) vectors = [] for field , func in [ ( \"f1\" , vectorizer_1 ), ( \"f2\" , vectorizer_2 ), ]: vectors . append ( func ( data_dict [ field ])) return np . concatenate ( vectors ) Ingredient 3 / 3: 2D Embedding We compute a 2D version of the pre-trained embedding to visualize the whole dataset. Hover has built-in methods for calling umap or ivis . Dependencies (when in your own environment) The libraries for this step are not directly required by hover : for umap: pip install umap-learn for ivis: pip install ivis[cpu] or pip install ivis[gpu] umap-learn is installed in this demo environment. # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html dataset.compute_2d_embedding(vectorizer, \"umap\") # What we did adds 'x' and 'y' columns to the DataFrames in dataset.dfs # One could alternatively pre-compute these columns using any approach dataset.dfs[\"raw\"].head(5) Apply Labels We are ready for the annotation interface! In-browser limitation If running the code in your browser: The annotation interface here is for demo only. Due to event listener limitations of this page, it cannot trigger certain callbacks. For a truly interactive example, please visit Binder app or Binder repo . In the real Jupyter Lab/Notebook, this will be fully functional. from hover.recipes import simple_annotator from hover.recipes.stable import _simple_annotator from bokeh.io import show, output_notebook output_notebook() # ---------- DEMO CODE: for this documentation page ---------- # because this demo is remotely hosted, we need to handle proxy def remote_jupyter_proxy_url(port): \"\"\" Callable to configure Bokeh's show method when using a proxy (JupyterHub). \"\"\" import os import urllib base_url = 'https://hub.gke2.mybinder.org/user/' host = urllib.parse.urlparse(base_url).netloc if port is None: return host service_url_path = os.environ['JUPYTERHUB_SERVICE_PREFIX'] proxy_url_path = 'proxy/%d' % port user_url = urllib.parse.urljoin(base_url, service_url_path) full_url = urllib.parse.urljoin(user_url, proxy_url_path) return full_url # static plot for demonstating the annotation interface static_plot, plot_objects = _simple_annotator(dataset) show(static_plot, notebook_url=remote_jupyter_proxy_url) # ---------- REAL CODE: for your actual Jupyter environment --------- # the real annotation interface enables Python callbacks # interactive_plot = simple_annotator(dataset) # show(interactive_plot, notebook_url='https://localhost:port') Tips: annotation interface basics Video guide Text guide There should be a SupervisableDataset board on the left and an BokehDataAnnotator on the right, each with a few buttons. SupervisableDataset push : push Dataset updates to the bokeh plots. commit : add data entries selected in the Annotator to a specified subset. dedup : deduplicate across subsets by feature (last in gets kept). BokehDataAnnotator raw / train / dev / test : choose which subsets to display or hide. apply : apply the label input to the selected points in the raw subset only. export : save your data (all subsets) in a specified format. We've essentially put the data into neighborboods based on the vectorizer, but the quality (homogeneity of labels) of such neighborhoods can vary. hover over any data point to see its tooltip. take advantage of different selection tools to apply labels at appropriate scales. the search widget might turn out useful. note that it does not select points but highlights them. @import url(\"../../../styles/monokai.css\");","title":"Quickstart"},{"location":"pages/tutorial/t0-quickstart/#ingredient-1-3-raw-data","text":"Start with a spreadsheet loaded in pandas . We turn it into a SupervisableDataset designed for labeling: from hover.core.dataset import SupervisableTextDataset import pandas as pd example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" # for fast demonstration purpose, sample the data df_raw = pd.read_csv(example_csv_path).sample(2000) # data is divided into 4 subsets: \"raw\" / \"train\" / \"dev\" / \"test\" # this example assumes no labeled data available., i.e. only \"raw\" df_raw[\"SUBSET\"] = \"raw\" # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df_raw, feature_key=\"text\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) FAQ What if I have multiple features? feature_key refers to the field that will be vectorized later on, which can be a JSON that encloses multiple features. For example, suppose our data entries look like this: { \"f1\" : \"foo\" , \"f2\" : \"bar\" , \"non_feature\" : \"abc\" } We can put f1 and f2 in a JSON and convert the entries like this: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Can I use audio or image data? In the not-too-far future, yes! Some mechanisms can get tricky with audios/images, but we are working on it: display tooltips: as of 0.4.0, tooltips are supported in the low-level APIs . search and highlight : pending (and open to contributions!) high-level API like SupervisableImageDataset : pending","title":"Ingredient 1 / 3: Raw Data"},{"location":"pages/tutorial/t0-quickstart/#ingredient-2-3-embedding","text":"A pre-trained embedding lets us group data points semantically. In particular, let's define a data -> embedding vector function. import spacy import re # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") Tips Caching dataset by itself stores the original features but not the corresponding vectors. To avoid vectorizing the same feature again and again, we could simply do: from functools import cache @cache def vectorizer ( feature ): # put code here If you'd like to limit the size of the cache, something like @lru_cache(maxsize=10000) could help. Check out functools for more options. Vectorizing multiple features Suppose we have multiple features enclosed in a JSON: # could also keep f1 and f2 around { 'feature' : '{\"f1\": \"foo\", \"f2\": \"bar\"}' , 'non_feature' : 'abc' } Also, suppose we have individual vectorizers likes this: def vectorizer_1 ( feature_1 ): # put code here def vectorizer_2 ( feature_2 ): # put code here Then we can define a composite vectorizer: import json import numpy as np def vectorizer ( feature_json ): data_dict = json . loads ( feature_json ) vectors = [] for field , func in [ ( \"f1\" , vectorizer_1 ), ( \"f2\" , vectorizer_2 ), ]: vectors . append ( func ( data_dict [ field ])) return np . concatenate ( vectors )","title":"Ingredient 2 / 3: Embedding"},{"location":"pages/tutorial/t0-quickstart/#ingredient-3-3-2d-embedding","text":"We compute a 2D version of the pre-trained embedding to visualize the whole dataset. Hover has built-in methods for calling umap or ivis . Dependencies (when in your own environment) The libraries for this step are not directly required by hover : for umap: pip install umap-learn for ivis: pip install ivis[cpu] or pip install ivis[gpu] umap-learn is installed in this demo environment. # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html dataset.compute_2d_embedding(vectorizer, \"umap\") # What we did adds 'x' and 'y' columns to the DataFrames in dataset.dfs # One could alternatively pre-compute these columns using any approach dataset.dfs[\"raw\"].head(5)","title":"Ingredient 3 / 3: 2D Embedding"},{"location":"pages/tutorial/t0-quickstart/#apply-labels","text":"We are ready for the annotation interface! In-browser limitation If running the code in your browser: The annotation interface here is for demo only. Due to event listener limitations of this page, it cannot trigger certain callbacks. For a truly interactive example, please visit Binder app or Binder repo . In the real Jupyter Lab/Notebook, this will be fully functional. from hover.recipes import simple_annotator from hover.recipes.stable import _simple_annotator from bokeh.io import show, output_notebook output_notebook() # ---------- DEMO CODE: for this documentation page ---------- # because this demo is remotely hosted, we need to handle proxy def remote_jupyter_proxy_url(port): \"\"\" Callable to configure Bokeh's show method when using a proxy (JupyterHub). \"\"\" import os import urllib base_url = 'https://hub.gke2.mybinder.org/user/' host = urllib.parse.urlparse(base_url).netloc if port is None: return host service_url_path = os.environ['JUPYTERHUB_SERVICE_PREFIX'] proxy_url_path = 'proxy/%d' % port user_url = urllib.parse.urljoin(base_url, service_url_path) full_url = urllib.parse.urljoin(user_url, proxy_url_path) return full_url # static plot for demonstating the annotation interface static_plot, plot_objects = _simple_annotator(dataset) show(static_plot, notebook_url=remote_jupyter_proxy_url) # ---------- REAL CODE: for your actual Jupyter environment --------- # the real annotation interface enables Python callbacks # interactive_plot = simple_annotator(dataset) # show(interactive_plot, notebook_url='https://localhost:port') Tips: annotation interface basics Video guide Text guide There should be a SupervisableDataset board on the left and an BokehDataAnnotator on the right, each with a few buttons. SupervisableDataset push : push Dataset updates to the bokeh plots. commit : add data entries selected in the Annotator to a specified subset. dedup : deduplicate across subsets by feature (last in gets kept). BokehDataAnnotator raw / train / dev / test : choose which subsets to display or hide. apply : apply the label input to the selected points in the raw subset only. export : save your data (all subsets) in a specified format. We've essentially put the data into neighborboods based on the vectorizer, but the quality (homogeneity of labels) of such neighborhoods can vary. hover over any data point to see its tooltip. take advantage of different selection tools to apply labels at appropriate scales. the search widget might turn out useful. note that it does not select points but highlights them. @import url(\"../../../styles/monokai.css\");","title":"Apply Labels"},{"location":"pages/tutorial/t1-active-learning/","text":"The most common usage of hover is through built-in recipe s like in the quickstart. Let's explore another recipe -- an active learning example. @import url(\"../../../styles/monokai.css\"); { bootstrap: true, mountStatusWidget: true, mountActivateWidget: false, mountRunButton: false, mountRestartButton: false, mountRestartallButton: false, requestKernel: true, binderOptions: { repo: \"phurwicz/hover-binder\", codeMirrorConfig: { theme: \"monokai\", indentUnit: 4, } }, } Ingredient 1 ~ 3 / 4: Data, Vectorizer, Reduction This is exactly the same as in the quickstart : from hover.core.dataset import SupervisableTextDataset import pandas as pd example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" # for fast demonstration purpose, sample the data df_raw = pd.read_csv(example_csv_path).sample(2000) # data is divided into 4 subsets: \"raw\" / \"train\" / \"dev\" / \"test\" # this example assumes no labeled data available., i.e. only \"raw\" df_raw[\"SUBSET\"] = \"raw\" # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df_raw, feature_key=\"text\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) import spacy import re # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html dataset.compute_2d_embedding(vectorizer, \"umap\") # What we did adds 'x' and 'y' columns to the DataFrames in dataset.dfs # One could alternatively pre-compute these columns using any approach dataset.dfs[\"raw\"].head(5) Ingredient 4 / 4: Model Callback To utilize active learning, we need to specify how to get a model in the loop. hover considers the vectorizer as a \"frozen\" embedding and follows up with a neural network, which infers its own dimensionality from the vectorizer and the output classes. This architecture named VectorNet is the (default) basis of active learning in hover . Custom models It is possible to use a model other than VectorNet or its subclass. Simply implement the following methods with the same signatures as VectorNet : train save predict_proba from hover.core.neural import VectorNet from hover.utils.common_nn import LogisticRegression def vecnet_callback(dataset, vectorizer): \"\"\" Create a model with vectorizer-NN architecture. \"\"\" # model.pt will point to a PyTorch state dict (to be created) # which gets cumulatively updated when we train the model vecnet = VectorNet(vectorizer, LogisticRegression, \"model.pt\", dataset.classes) return vecnet vecnet = vecnet_callback(dataset, vectorizer) # predict_proba accepts individual strings or list # text -> vector -> class probabilities print(vecnet.predict_proba(text)) print(vecnet.predict_proba([text])) Note how the callback dynamically takes dataset.classes , which means the model architecture will adapt when we add classes during annotation. Recipe Time Now we invoke the active_learning recipe. Tips: how recipes work programmatically In general, a recipe is a function taking a SupervisableDataset and other arguments based on its functionality. Here are a few common recipes: active_learning Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vectorizer callable the feature -> vector function vecnet_callback callable the (dataset, vectorizer) -> VecNet function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search -> highlight Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vectorizer` | `callable` | the feature -> vector function | | `vecnet_callback` | `callable` | the (dataset, vectorizer) -> `VecNet` function| | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search -> highlight | \"\"\" layout , _ = _active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ) return layout simple_annotator Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout linked_annotator Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout The recipe returns a handle function which bokeh can use to visualize an annotation interface in multiple settings. In-browser limitation If running the code in your browser: The annotation interface here is for demo only. Due to event listener limitations of this page, it cannot trigger certain callbacks. For a truly interactive example, please visit Binder app or Binder repo . In the real Jupyter Lab/Notebook, this will be fully functional. from hover.recipes.experimental import active_learning from bokeh.io import show, output_notebook handle = active_learning(dataset, vectorizer, vecnet_callback) output_notebook() show(handle) Tips: annotation interface with multiple plots Video guide: leveraging linked selection Video guide: active learning Text guide: active learning Inspecting model predictions allows us to get an idea of how the current set of annotations will likely teach the model. locate the most valuable samples for further annotation.","title":"Using Recipes"},{"location":"pages/tutorial/t1-active-learning/#ingredient-1-3-4-data-vectorizer-reduction","text":"This is exactly the same as in the quickstart : from hover.core.dataset import SupervisableTextDataset import pandas as pd example_csv_path = \"https://raw.githubusercontent.com/phurwicz/hover-gallery/main/0.5.0/20_newsgroups_raw.csv\" # for fast demonstration purpose, sample the data df_raw = pd.read_csv(example_csv_path).sample(2000) # data is divided into 4 subsets: \"raw\" / \"train\" / \"dev\" / \"test\" # this example assumes no labeled data available., i.e. only \"raw\" df_raw[\"SUBSET\"] = \"raw\" # this class stores the dataset throught the labeling process dataset = SupervisableTextDataset.from_pandas(df_raw, feature_key=\"text\", label_key=\"label\") # each subset can be accessed as its own DataFrame dataset.dfs[\"raw\"].head(5) import spacy import re # use your preferred embedding for the task nlp = spacy.load(\"en_core_web_md\") # raw data (str in this case) -> np.array def vectorizer(text): clean_text = re.sub(r\"[\\s]+\", r\" \", str(text)) return nlp(clean_text, disable=nlp.pipe_names).vector text = dataset.dfs[\"raw\"].loc[0, \"text\"] vec = vectorizer(text) print(f\"Text: {text}\") print(f\"Vector shape: {vec.shape}\") # any kwargs will be passed onto the corresponding reduction # for umap: https://umap-learn.readthedocs.io/en/latest/parameters.html # for ivis: https://bering-ivis.readthedocs.io/en/latest/api.html dataset.compute_2d_embedding(vectorizer, \"umap\") # What we did adds 'x' and 'y' columns to the DataFrames in dataset.dfs # One could alternatively pre-compute these columns using any approach dataset.dfs[\"raw\"].head(5)","title":"Ingredient 1 ~ 3 / 4: Data, Vectorizer, Reduction"},{"location":"pages/tutorial/t1-active-learning/#ingredient-4-4-model-callback","text":"To utilize active learning, we need to specify how to get a model in the loop. hover considers the vectorizer as a \"frozen\" embedding and follows up with a neural network, which infers its own dimensionality from the vectorizer and the output classes. This architecture named VectorNet is the (default) basis of active learning in hover . Custom models It is possible to use a model other than VectorNet or its subclass. Simply implement the following methods with the same signatures as VectorNet : train save predict_proba from hover.core.neural import VectorNet from hover.utils.common_nn import LogisticRegression def vecnet_callback(dataset, vectorizer): \"\"\" Create a model with vectorizer-NN architecture. \"\"\" # model.pt will point to a PyTorch state dict (to be created) # which gets cumulatively updated when we train the model vecnet = VectorNet(vectorizer, LogisticRegression, \"model.pt\", dataset.classes) return vecnet vecnet = vecnet_callback(dataset, vectorizer) # predict_proba accepts individual strings or list # text -> vector -> class probabilities print(vecnet.predict_proba(text)) print(vecnet.predict_proba([text])) Note how the callback dynamically takes dataset.classes , which means the model architecture will adapt when we add classes during annotation.","title":"Ingredient 4 / 4: Model Callback"},{"location":"pages/tutorial/t1-active-learning/#recipe-time","text":"Now we invoke the active_learning recipe. Tips: how recipes work programmatically In general, a recipe is a function taking a SupervisableDataset and other arguments based on its functionality. Here are a few common recipes: active_learning Display the dataset for annotation, putting a classification model in the loop. Currently works most smoothly with VectorNet . Param Type Description dataset SupervisableDataset the dataset to link to vectorizer callable the feature -> vector function vecnet_callback callable the (dataset, vectorizer) -> VecNet function **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehSoftLabelExplorer BokehDataAnnotator BokehDataFinder manage data subsets inspect model predictions make annotations search -> highlight Source code in hover/recipes/experimental.py @servable ( title = \"Active Learning\" ) def active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ): \"\"\" ???+ note \"Display the dataset for annotation, putting a classification model in the loop.\" Currently works most smoothly with `VectorNet`. | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `vectorizer` | `callable` | the feature -> vector function | | `vecnet_callback` | `callable` | the (dataset, vectorizer) -> `VecNet` function| | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehSoftLabelExplorer | BokehDataAnnotator | BokehDataFinder | | :------------------ | :------------------------ | :----------------- | :------------------ | | manage data subsets | inspect model predictions | make annotations | search -> highlight | \"\"\" layout , _ = _active_learning ( dataset , vectorizer , vecnet_callback , ** kwargs ) return layout simple_annotator Display the dataset with on a 2D map for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataAnnotator manage data subsets make annotations Source code in hover/recipes/stable.py @servable ( title = \"Simple Annotator\" ) def simple_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset with on a 2D map for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataAnnotator | | :------------------ | :----------------- | | manage data subsets | make annotations | \"\"\" layout , _ = _simple_annotator ( dataset , ** kwargs ) return layout linked_annotator Display the dataset on a 2D map in two views, one for search and one for annotation. Param Type Description dataset SupervisableDataset the dataset to link to **kwargs kwargs to forward to each Bokeh figure Expected visual layout: SupervisableDataset BokehDataFinder BokehDataAnnotator manage data subsets search -> highlight make annotations Source code in hover/recipes/stable.py @servable ( title = \"Linked Annotator\" ) def linked_annotator ( dataset , ** kwargs ): \"\"\" ???+ note \"Display the dataset on a 2D map in two views, one for search and one for annotation.\" | Param | Type | Description | | :-------- | :------- | :----------------------------------- | | `dataset` | `SupervisableDataset` | the dataset to link to | | `**kwargs` | | kwargs to forward to each Bokeh figure | Expected visual layout: | SupervisableDataset | BokehDataFinder | BokehDataAnnotator | | :------------------ | :------------------ | :----------------- | | manage data subsets | search -> highlight | make annotations | \"\"\" layout , _ = _linked_annotator ( dataset , ** kwargs ) return layout The recipe returns a handle function which bokeh can use to visualize an annotation interface in multiple settings. In-browser limitation If running the code in your browser: The annotation interface here is for demo only. Due to event listener limitations of this page, it cannot trigger certain callbacks. For a truly interactive example, please visit Binder app or Binder repo . In the real Jupyter Lab/Notebook, this will be fully functional. from hover.recipes.experimental import active_learning from bokeh.io import show, output_notebook handle = active_learning(dataset, vectorizer, vecnet_callback) output_notebook() show(handle) Tips: annotation interface with multiple plots Video guide: leveraging linked selection Video guide: active learning Text guide: active learning Inspecting model predictions allows us to get an idea of how the current set of annotations will likely teach the model. locate the most valuable samples for further annotation.","title":"Recipe Time"},{"location":"pages/tutorial/t2-bokeh-app/","text":"hover creates a bokeh server app to deliver its annotation interface. This app can be served flexibly based on your needs. @import url(\"../../../styles/monokai.css\"); Prerequisites Suppose that we've already used a recipe to create a handle function like in the quickstart . Recap from the tutorials before the handle is a function which renders plot elements on a bokeh document . Option 1: Jupyter We are probably familiar with this now: from bokeh.io import show , output_notebook output_notebook () show ( handle ) # notebook_url='http://localhost:8888' Pros & Cons This inline Jupyter mode can integrate particularly well with your notebook workflow. For example, when your are (tentatively) done with annotation, the SupervisableDataset can be accessed directly in the notebook, rather than exported to a file and loaded back. The inline mode is highly recommended for local usage. However, with a remote Jupyter server, it has trouble loading JS libraries or accessing implicit bokeh server ports. Option 2: Command Line bokeh serve starts an explicit tornado server from the command line: bokeh serve my-app.py # my-app.py # handle = ... from bokeh.io import curdoc doc = curdoc () handle ( doc ) Pros & Cons This is the \"classic\" approach to run a bokeh server. Remote access is simple through parameters specified here . The bokeh plot tools are mobile-friendly too -- this means you can host a server, e.g. an http-enabled cloud virtual machine, and annotate from a tablet. The command line mode is less interactive, since Python objects in the script cannot be accessed on the fly. Option 3: Anywhere in Python It is possible to embed the app in regular Python: from bokeh.server.server import Server server = Server ({ '/my-app' : handle }) server . start () Pros & Cons This embedded mode is a go-to for serving within a greater application. Also note that each command line argument for bokeh serve has a corresponding keyword argument to Server() . For instance, bokeh serve <args> --allow-websocket-origin=* in the command line mirrors Server(*args, allow_websocket_origin='*') in Python. The embedded mode gives you the most control of your server.","title":"Server Options"},{"location":"pages/tutorial/t2-bokeh-app/#prerequisites","text":"Suppose that we've already used a recipe to create a handle function like in the quickstart . Recap from the tutorials before the handle is a function which renders plot elements on a bokeh document .","title":"Prerequisites"},{"location":"pages/tutorial/t2-bokeh-app/#option-1-jupyter","text":"We are probably familiar with this now: from bokeh.io import show , output_notebook output_notebook () show ( handle ) # notebook_url='http://localhost:8888' Pros & Cons This inline Jupyter mode can integrate particularly well with your notebook workflow. For example, when your are (tentatively) done with annotation, the SupervisableDataset can be accessed directly in the notebook, rather than exported to a file and loaded back. The inline mode is highly recommended for local usage. However, with a remote Jupyter server, it has trouble loading JS libraries or accessing implicit bokeh server ports.","title":"Option 1: Jupyter"},{"location":"pages/tutorial/t2-bokeh-app/#option-2-command-line","text":"bokeh serve starts an explicit tornado server from the command line: bokeh serve my-app.py # my-app.py # handle = ... from bokeh.io import curdoc doc = curdoc () handle ( doc ) Pros & Cons This is the \"classic\" approach to run a bokeh server. Remote access is simple through parameters specified here . The bokeh plot tools are mobile-friendly too -- this means you can host a server, e.g. an http-enabled cloud virtual machine, and annotate from a tablet. The command line mode is less interactive, since Python objects in the script cannot be accessed on the fly.","title":"Option 2: Command Line"},{"location":"pages/tutorial/t2-bokeh-app/#option-3-anywhere-in-python","text":"It is possible to embed the app in regular Python: from bokeh.server.server import Server server = Server ({ '/my-app' : handle }) server . start () Pros & Cons This embedded mode is a go-to for serving within a greater application. Also note that each command line argument for bokeh serve has a corresponding keyword argument to Server() . For instance, bokeh serve <args> --allow-websocket-origin=* in the command line mirrors Server(*args, allow_websocket_origin='*') in Python. The embedded mode gives you the most control of your server.","title":"Option 3: Anywhere in Python"},{"location":"snippets/markdown/jupyterlab-js-issue/","text":"In-browser limitation If running the code in your browser: The annotation interface here is for demo only. Due to event listener limitations of this page, it cannot trigger certain callbacks. For a truly interactive example, please visit Binder app or Binder repo . In the real Jupyter Lab/Notebook, this will be fully functional.","title":"Jupyterlab js issue"}]}